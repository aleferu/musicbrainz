{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e9c01d7",
   "metadata": {},
   "source": [
    "**Disclaimer**: Directories don't match the repo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10c5329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/27 13:57:57 WARN Utils: Your hostname, aleferu-PC resolves to a loopback address: 127.0.1.1; using 192.168.1.46 instead (on interface eno1)\n",
      "25/04/27 13:57:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/27 13:57:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark: SparkSession = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"foo\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"12g\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"50\")  # shuffle overhead\n",
    "    .config(\"spark.memory.fraction\", \"0.4\")  # memory cache\n",
    "    .config(\"spark.memory.storageFraction\", \"0.3\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc: SparkContext = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd3bff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,6M\topt_data/artist_tags_clean.csv\n",
      "297M\topt_data/mlp2019110.5x_test.pt\n",
      "730M\topt_data/mlp2019110.5x_train.pt\n",
      "3,2M\topt_data/mlp2019110.5y_test.pt\n",
      "7,8M\topt_data/mlp2019110.5y_train.pt\n",
      "199M\topt_data/mlp2019110.75x_test.pt\n",
      "462M\topt_data/mlp2019110.75x_train.pt\n",
      "2,2M\topt_data/mlp2019110.75y_test.pt\n",
      "5,0M\topt_data/mlp2019110.75y_train.pt\n",
      "102M\topt_data/mlp2019110.9x_test.pt\n",
      "227M\topt_data/mlp2019110.9x_train.pt\n",
      "1,1M\topt_data/mlp2019110.9y_test.pt\n",
      "2,5M\topt_data/mlp2019110.9y_train.pt\n",
      "501M\topt_data/mlp2019110x_test.pt\n",
      "1,3G\topt_data/mlp2019110x_train.pt\n",
      "5,4M\topt_data/mlp2019110y_test.pt\n",
      "14M\topt_data/mlp2019110y_train.pt\n",
      "168M\topt_data/mlp2021110.5x_test.pt\n",
      "859M\topt_data/mlp2021110.5x_train.pt\n",
      "1,8M\topt_data/mlp2021110.5y_test.pt\n",
      "9,2M\topt_data/mlp2021110.5y_train.pt\n",
      "113M\topt_data/mlp2021110.75x_test.pt\n",
      "548M\topt_data/mlp2021110.75x_train.pt\n",
      "1,2M\topt_data/mlp2021110.75y_test.pt\n",
      "5,9M\topt_data/mlp2021110.75y_train.pt\n",
      "58M\topt_data/mlp2021110.9x_test.pt\n",
      "270M\topt_data/mlp2021110.9x_train.pt\n",
      "632K\topt_data/mlp2021110.9y_test.pt\n",
      "2,9M\topt_data/mlp2021110.9y_train.pt\n",
      "279M\topt_data/mlp2021110x_test.pt\n",
      "1,5G\topt_data/mlp2021110x_train.pt\n",
      "3,0M\topt_data/mlp2021110y_test.pt\n",
      "16M\topt_data/mlp2021110y_train.pt\n",
      "53M\topt_data/mlp2023110.5x_test.pt\n",
      "973M\topt_data/mlp2023110.5x_train.pt\n",
      "576K\topt_data/mlp2023110.5y_test.pt\n",
      "11M\topt_data/mlp2023110.5y_train.pt\n",
      "36M\topt_data/mlp2023110.75x_test.pt\n",
      "625M\topt_data/mlp2023110.75x_train.pt\n",
      "384K\topt_data/mlp2023110.75y_test.pt\n",
      "6,7M\topt_data/mlp2023110.75y_train.pt\n",
      "19M\topt_data/mlp2023110.9x_test.pt\n",
      "310M\topt_data/mlp2023110.9x_train.pt\n",
      "200K\topt_data/mlp2023110.9y_test.pt\n",
      "3,3M\topt_data/mlp2023110.9y_train.pt\n",
      "95M\topt_data/mlp2023110x_test.pt\n",
      "1,7G\topt_data/mlp2023110x_train.pt\n",
      "1,1M\topt_data/mlp2023110y_test.pt\n",
      "18M\topt_data/mlp2023110y_train.pt\n",
      "1,9G\topt_data/tracks_no_va_merged_id_clean.csv\n",
      "13G\topt_data/\n"
     ]
    }
   ],
   "source": [
    "data_dir: str = \"opt_data\"\n",
    "\n",
    "!du -h $data_dir/*\n",
    "!du -hs $data_dir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7908d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    142936 opt_data/artist_tags_clean.csv\n",
      "  24324101 opt_data/tracks_no_va_merged_id_clean.csv\n",
      "  24467037 total\n"
     ]
    }
   ],
   "source": [
    "!wc -l $data_dir/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d54b5c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> opt_data/artist_tags_clean.csv <==\n",
      "artist,tags\n",
      "4,\"2, 1, 16, 3\"\n",
      "6,\"3, 16, 2, 9\"\n",
      "21685,16\n",
      "9,\"3, 2, 9\"\n",
      "10,16\n",
      "11,\"16, 9\"\n",
      "12,\"2, 16, 8, 5\"\n",
      "15,\"17, 6\"\n",
      "16,16\n",
      "\n",
      "==> opt_data/tracks_no_va_merged_id_clean.csv <==\n",
      "name,date,year,month,artist_count,a0_id,a0_name,tags,a1_id,a1_name,a2_id,a2_name,a3_id,a3_name,a4_id,a4_name,id\n",
      " *~ƒint_vœr!~* ,201612,2016,12,1,2808021,Julius Androide,,,,,,,,,,0\n",
      "roots rock reggae,200006,2000,6,1,637799,Baba Dread,,,,,,,,,,1\n",
      "roots rock reggae,201304,2013,4,1,625247,Jah Sun,,,,,,,,,,2\n",
      "roots rock reggae,199713,1997,13,1,442806,The Wailers Band,,,,,,,,,,3\n",
      "roots rock reggae,201009,2010,9,1,426971,Dean Fraser,,,,,,,,,,4\n",
      "roots rock reggae,200013,2000,13,1,248826,Bob Marley,9,,,,,,,,,5\n",
      "roots rock reggae,200813,2008,13,1,248660,Tony Roots,,,,,,,,,,6\n",
      "roots rock reggae,197310,1973,10,1,232732,Bob Marley & The Wailers,,,,,,,,,,7\n",
      "roots rock reggae,201001,2010,1,1,693259,Solo Banton,,,,,,,,,,8\n"
     ]
    }
   ],
   "source": [
    "!head $data_dir/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5976a83e",
   "metadata": {},
   "source": [
    "Update artist_tags_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b880e349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han cargado 142935 entradas del fichero 'artist_tags_clean.csv'.\n",
      "+------+-----------+\n",
      "|artist|       tags|\n",
      "+------+-----------+\n",
      "|     4|2, 1, 16, 3|\n",
      "|     6|3, 16, 2, 9|\n",
      "| 21685|         16|\n",
      "|     9|    3, 2, 9|\n",
      "|    10|         16|\n",
      "+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "at_schema: StructType = StructType([\n",
    "    StructField(\"artist\", IntegerType(), False),  # important: int\n",
    "    StructField(\"tags\", StringType(), False)\n",
    "])\n",
    "\n",
    "artist_tags: DataFrame = spark.read.csv(\n",
    "    f\"{data_dir}/artist_tags_clean.csv\",\n",
    "    header=True,\n",
    "    schema=at_schema\n",
    ").filter(\n",
    "    F.col(\"tags\").isNotNull()\n",
    ")\n",
    "print(\"Se han cargado \" + str(artist_tags.count()) + \" entradas del fichero 'artist_tags_clean.csv'.\")\n",
    "\n",
    "artist_tags.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43403ba",
   "metadata": {},
   "source": [
    "list of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74159079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+\n",
      "|artist|            tags|\n",
      "+------+----------------+\n",
      "|     4|   [2, 1, 16, 3]|\n",
      "|    11|         [16, 9]|\n",
      "|    20|      [3, 0, 16]|\n",
      "|    23|[0, 2, 16, 9, 7]|\n",
      "|    28|          [2, 0]|\n",
      "+------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def tags_comma_to_list(df: DataFrame) -> DataFrame:\n",
    "    return (df\n",
    "        .withColumn(\"tags\", F.split(\"tags\", \", \"))\n",
    "        .groupBy(\"artist\")\n",
    "        .agg(F.collect_list(\"tags\").alias(\"tags\"))\n",
    "        .withColumn(\"tags\", F.flatten(\"tags\"))\n",
    "    )\n",
    "\n",
    "artist_tags = tags_comma_to_list(artist_tags)\n",
    "\n",
    "artist_tags.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118217c8",
   "metadata": {},
   "source": [
    "Other CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c95d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=================================================>       (13 + 2) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han cargado 3534032 entradas del fichero 'tracks_no_va_merged_id_clean.csv'.\n",
      "+-------+-----+-----+-----+-----+----+\n",
      "|  a0_id|a1_id|a2_id|a3_id|a4_id|tags|\n",
      "+-------+-----+-----+-----+-----+----+\n",
      "| 248826| NULL| NULL| NULL| NULL|   9|\n",
      "|  96651| NULL| NULL| NULL| NULL|   9|\n",
      "| 763291| NULL| NULL| NULL| NULL|   9|\n",
      "|2158618| NULL| NULL| NULL| NULL|   9|\n",
      "| 654641| NULL| NULL| NULL| NULL|   9|\n",
      "+-------+-----+-----+-----+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "track_tags: DataFrame = spark.read.csv(\n",
    "    f\"{data_dir}/tracks_no_va_merged_id_clean.csv\",\n",
    "    header=True,\n",
    ").select(\n",
    "    # all integers for easy joins\n",
    "    *[F.col(f\"a{i}_id\").cast(\"integer\") for i in range(5)],  # important: int\n",
    "    F.col(\"tags\").cast(\"string\")\n",
    ").filter(\n",
    "    # ease up computations\n",
    "    F.col(\"tags\").isNotNull()\n",
    ")\n",
    "print(\"Se han cargado \" + str(track_tags.count()) + \" entradas del fichero 'tracks_no_va_merged_id_clean.csv'.\")\n",
    "\n",
    "track_tags.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264e427b",
   "metadata": {},
   "source": [
    "map *artist* - *tags*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d23bd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "| artist|tags|\n",
      "+-------+----+\n",
      "| 248826|   9|\n",
      "|  96651|   9|\n",
      "| 763291|   9|\n",
      "|2158618|   9|\n",
      "| 654641|   9|\n",
      "+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "track_tags = (track_tags\n",
    "    .withColumn(\"artist\", F.explode(F.array([f\"a{i}_id\" for i in range(5)])))\n",
    "    .select(\"artist\", \"tags\")\n",
    "    .filter(F.col(\"artist\").isNotNull())  # DO NOT REMOVE\n",
    ")\n",
    "\n",
    "track_tags.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d5758e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/27 13:58:13 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "[Stage 13:====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|artist|                tags|\n",
      "+------+--------------------+\n",
      "|     4|[3, 3, 9, 3, 3, 1...|\n",
      "|    20|[2, 6, 6, 3, 0, 6...|\n",
      "|    31|[2, 2, 2, 2, 2, 7...|\n",
      "|    32|[10, 10, 10, 7, 1...|\n",
      "|    40|[2, 0, 2, 0, 2, 0...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# tags_comma_to_list transforms `3, 3, 9` to a list and merge results from different rows with the same artist\n",
    "track_tags = tags_comma_to_list(track_tags)\n",
    "\n",
    "track_tags.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8528c5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|artist|                tags|\n",
      "+------+--------------------+\n",
      "|     4|[3, 3, 9, 3, 3, 1...|\n",
      "|    20|[2, 6, 6, 3, 0, 6...|\n",
      "|    31|[2, 2, 2, 2, 2, 7...|\n",
      "|    32|[10, 10, 10, 7, 1...|\n",
      "|    40|[2, 0, 2, 0, 2, 0...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# alias (withColumnRenamed) to avoid ambiguity\n",
    "# simple join + concat with coalesce\n",
    "complete_df = track_tags.withColumnRenamed(\"tags\", \"t0\").join(\n",
    "    artist_tags.withColumnRenamed(\"tags\", \"t1\"),\n",
    "    on=\"artist\",\n",
    "    how=\"outer\"\n",
    ").withColumn(\n",
    "    \"tags\",\n",
    "    F.concat(\n",
    "        F.coalesce(\"t0\", F.array()),\n",
    "        F.coalesce(\"t1\", F.array())\n",
    "    )\n",
    ").select(\"artist\", \"tags\")\n",
    "\n",
    "complete_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be1ed69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/27 13:58:26 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 58:=================================================>      (44 + 6) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found MB tags for 413749 artists\n",
      "+-------+------------------+-----+-------------------+-------------------+------------------+-----+-------------------+-----+-----+-----+------+------+------+------+------+--------------------+--------------------+------------------+------+------+------+--------------------+-------------------+\n",
      "| artist|             tag_0|tag_1|              tag_2|              tag_3|             tag_4|tag_5|              tag_6|tag_7|tag_8|tag_9|tag_10|tag_11|tag_12|tag_13|tag_14|              tag_15|              tag_16|            tag_17|tag_18|tag_19|tag_20|              tag_21|             tag_22|\n",
      "+-------+------------------+-----+-------------------+-------------------+------------------+-----+-------------------+-----+-----+-----+------+------+------+------+------+--------------------+--------------------+------------------+------+------+------+--------------------+-------------------+\n",
      "|  53001|              NULL| NULL|                0.2|                0.2|              NULL|  0.2|                0.2| NULL| NULL| NULL|  NULL|  NULL|  NULL|  NULL|  NULL|                NULL|                 0.2|              NULL|  NULL|   0.0|  NULL|                NULL|               NULL|\n",
      "| 463475|              NULL| NULL| 0.3333333333333333| 0.1388888888888889|0.2222222222222222| NULL|               NULL| NULL| NULL| NULL|  NULL|  NULL|  NULL|  NULL|  NULL|0.027777777777777776|0.027777777777777776|0.2222222222222222|  NULL|   0.0|  NULL|0.027777777777777776|               NULL|\n",
      "|1651520|              NULL| NULL|0.03571428571428571|0.32142857142857145|              NULL| NULL|0.32142857142857145| NULL| NULL| NULL|  NULL|  NULL|  NULL|  NULL|  NULL|                NULL|                NULL|              NULL|  NULL|   0.0|  NULL|                NULL|0.32142857142857145|\n",
      "|2132222|              NULL| NULL|              0.375|               NULL|              NULL| NULL|               NULL| NULL| NULL|0.375| 0.125|  NULL|  NULL|  NULL|  NULL|                NULL|               0.125|              NULL|  NULL|   0.0|  NULL|                NULL|               NULL|\n",
      "|2253745|0.6666666666666666| NULL|               NULL| 0.3333333333333333|              NULL| NULL|               NULL| NULL| NULL| NULL|  NULL|  NULL|  NULL|  NULL|  NULL|                NULL|                NULL|              NULL|  NULL|   0.0|  NULL|                NULL|               NULL|\n",
      "+-------+------------------+-----+-------------------+-------------------+------------------+-----+-------------------+-----+-----+-----+------+------+------+------+------+--------------------+--------------------+------------------+------+------+------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "\n",
    "# yield tag_* cols\n",
    "complete_df = complete_df.withColumn(\n",
    "    \"tags_length\", F.size(\"tags\")  # for div\n",
    ").withColumn(\n",
    "    \"tag\", F.explode(\"tags\")\n",
    ").groupBy(\n",
    "    \"artist\", \"tag\", \"tags_length\"  # tags_length does nothing here, repetition no longer exists\n",
    ").count().withColumn(\n",
    "    \"frequency\", F.col(\"count\") / F.col(\"tags_length\")  # eg: artist, tag, tags_length, freq -> 1234, 2, 5, 0.4\n",
    ").groupBy(\"artist\").pivot(\"tag\").agg(\n",
    "    F.first(\"frequency\")  # first so agg is faster (should only be one)\n",
    ")\n",
    "\n",
    "# If a tag is missing, fill with 0.0\n",
    "for i in range(23):\n",
    "    if str(i) not in complete_df.columns:\n",
    "        complete_df = complete_df.withColumn(str(i), F.lit(0.0))\n",
    "\n",
    "# Order tag columns and rename to tag_i\n",
    "complete_df = complete_df.select(\n",
    "    \"artist\",\n",
    "    *[F.col(str(i)).alias(f\"tag_{i}\") for i in range(23)]\n",
    ")\n",
    "\n",
    "# Cache on disk\n",
    "complete_df.persist(StorageLevel.DISK_ONLY)\n",
    "print(f\"Found MB tags for {complete_df.count()} artists\")\n",
    "\n",
    "complete_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bed8d21",
   "metadata": {},
   "source": [
    "Schemas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8016f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType, IntegerType\n",
    "\n",
    "# Schema of tensors, no _row_index\n",
    "pt_schema: StructType = StructType([\n",
    "    StructField(\"id0\", FloatType(), False),\n",
    "    StructField(\"hbd0\", FloatType(), False),\n",
    "    StructField(\"bd0\", FloatType(), False),\n",
    "    StructField(\"hed0\", FloatType(), False),\n",
    "    StructField(\"ed0\", FloatType(), False),\n",
    "    StructField(\"e0\", FloatType(), False),\n",
    "    StructField(\"g10\", FloatType(), False),\n",
    "    StructField(\"g20\", FloatType(), False),\n",
    "    StructField(\"g30\", FloatType(), False),\n",
    "    StructField(\"g40\", FloatType(), False),\n",
    "    StructField(\"g50\", FloatType(), False),\n",
    "    StructField(\"ps0\", FloatType(), False),\n",
    "    StructField(\"t10\", FloatType(), False),\n",
    "    StructField(\"t20\", FloatType(), False),\n",
    "    StructField(\"t30\", FloatType(), False),\n",
    "    StructField(\"t40\", FloatType(), False),\n",
    "    StructField(\"t50\", FloatType(), False),\n",
    "    StructField(\"t60\", FloatType(), False),\n",
    "    *[StructField(f\"tag0_{i}\", FloatType(), False) for i in range(23)],\n",
    "    StructField(\"cc0\", FloatType(), False),\n",
    "    StructField(\"cp0\", FloatType(), False),\n",
    "    StructField(\"sc0\", FloatType(), False),\n",
    "    StructField(\"sp0\", FloatType(), False),\n",
    "\n",
    "    StructField(\"id1\", FloatType(), False),\n",
    "    StructField(\"hbd1\", FloatType(), False),\n",
    "    StructField(\"bd1\", FloatType(), False),\n",
    "    StructField(\"hed1\", FloatType(), False),\n",
    "    StructField(\"ed1\", FloatType(), False),\n",
    "    StructField(\"e1\", FloatType(), False),\n",
    "    StructField(\"g11\", FloatType(), False),\n",
    "    StructField(\"g21\", FloatType(), False),\n",
    "    StructField(\"g31\", FloatType(), False),\n",
    "    StructField(\"g41\", FloatType(), False),\n",
    "    StructField(\"g51\", FloatType(), False),\n",
    "    StructField(\"ps1\", FloatType(), False),\n",
    "    StructField(\"t11\", FloatType(), False),\n",
    "    StructField(\"t21\", FloatType(), False),\n",
    "    StructField(\"t31\", FloatType(), False),\n",
    "    StructField(\"t41\", FloatType(), False),\n",
    "    StructField(\"t51\", FloatType(), False),\n",
    "    StructField(\"t61\", FloatType(), False),\n",
    "    *[StructField(f\"tag1_{i}\", FloatType(), False) for i in range(23)],\n",
    "    StructField(\"cc1\", FloatType(), False),\n",
    "    StructField(\"cp1\", FloatType(), False),\n",
    "    StructField(\"sc1\", FloatType(), False),\n",
    "    StructField(\"sp1\", FloatType(), False),\n",
    "\n",
    "    StructField(\"lfm\", FloatType(), False),\n",
    "    StructField(\"mrt\", FloatType(), False),\n",
    "    StructField(\"prt\", FloatType(), False),\n",
    "    StructField(\"lt\", FloatType(), False),\n",
    "\n",
    "    StructField(\"_row_index\", FloatType(), False),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b1d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import Window\n",
    "from pathlib import Path\n",
    "\n",
    "def get_df_from_pt(spark: SparkSession, pt_path: str, pt_schema: StructType) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a tensor and returns a Spark DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"Loading\", pt_path)\n",
    "    np_arr: np.ndarray = torch.load(pt_path, weights_only=False).numpy()\n",
    "\n",
    "    num_rows, num_cols = np_arr.shape\n",
    "    print(f\"Loaded array of {num_rows} rows and {num_cols} columns.\")\n",
    "\n",
    "    # keep row order\n",
    "    _row_index = np.arange(num_rows)\n",
    "    np_arr = np.column_stack((np_arr, _row_index))\n",
    "\n",
    "    # Serialization for lower memory consumption\n",
    "\n",
    "    pdf = pd.DataFrame(np_arr, columns=[f.name for f in pt_schema.fields], dtype=np.float32)\n",
    "\n",
    "    temp_path = f\"temp_{Path(pt_path).stem}.parquet\"\n",
    "    pdf.to_parquet(temp_path)\n",
    "\n",
    "    df = spark.read.schema(pt_schema).parquet(temp_path)\n",
    "\n",
    "    # too expensive!\n",
    "    # could have been done, but spark complains a lot\n",
    "    # it sends all the DF into only one executor because window is specified without partitioning\n",
    "    # it can only be done this way, as executors are independent\n",
    "    # do it in the numpy array was the easy solution\n",
    "    # I let this here for possible extra credits :P\n",
    "\n",
    "    # df = df.withColumn(\"_row_index\", F.row_number().over(Window.orderBy(F.monotonically_increasing_id())))\n",
    "\n",
    "    df = df.withColumn(\"_row_index\", F.col(\"_row_index\").cast(\"integer\"))\n",
    "    df = df.withColumn(\"id0\", F.col(\"id0\").cast(\"integer\"))\n",
    "    df = df.withColumn(\"id1\", F.col(\"id1\").cast(\"integer\"))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9964ce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_z_col(train_df: DataFrame, test_df: DataFrame, col_to_norm: str | F.Column) -> DataFrame:\n",
    "    # hack because idk how I'll pass it atm\n",
    "    if isinstance(col_to_norm, str):\n",
    "        col_name = col_to_norm\n",
    "        col_obj = F.col(col_name)\n",
    "    else:\n",
    "        col_obj = col_to_norm\n",
    "        col_name = str(col_obj)\n",
    "\n",
    "    mean_std = train_df.select(\n",
    "        F.mean(col_to_norm).alias(\"mean\"),\n",
    "        F.stddev(col_to_norm).alias(\"std\"),\n",
    "    ).first()\n",
    "    mean, std = mean_std.mean, mean_std.std\n",
    "\n",
    "    normalized_col = ((col_obj - F.lit(mean)) / F.lit(std)).alias(f\"z{col_name}\")\n",
    "\n",
    "    # Error checking (not necessary)\n",
    "    original_columns = train_df.columns\n",
    "    try:\n",
    "        original_index = original_columns.index(col_name)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Column '{col_name}' not found in DataFrame.\")\n",
    "\n",
    "    # Easier computation this way\n",
    "    # Also, ordered columns\n",
    "    new_cols = []\n",
    "    for i, col in enumerate(original_columns):\n",
    "        new_cols.append(F.col(col))\n",
    "        if i == original_index:\n",
    "            new_cols.append(normalized_col)\n",
    "\n",
    "    return train_df.select(*new_cols), test_df.select(*new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc32c6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mb_tags(arr_df: DataFrame, complete_df: DataFrame) -> DataFrame:\n",
    "    tag_cols = [f\"tag_{i}\" for i in range(23)]\n",
    "\n",
    "    # Update tag0_* columns based on id0\n",
    "    final_df: DataFrame = arr_df.join(complete_df, arr_df[\"id0\"] == complete_df[\"artist\"], \"left\")\n",
    "    for i in range(23):\n",
    "        final_df = final_df.withColumn(f\"mbtag0_{i}\", F.coalesce(F.col(f\"tag_{i}\"), F.lit(0.0)))\n",
    "    final_df = final_df.drop(\"artist\", *tag_cols)\n",
    "\n",
    "    # Update tag1_* columns based on id0\n",
    "    final_df = final_df.join(complete_df, final_df[\"id1\"] == complete_df[\"artist\"], \"left\")\n",
    "    for i in range(23):\n",
    "        final_df = final_df.withColumn(f\"mbtag1_{i}\", F.coalesce(F.col(f\"tag_{i}\"), F.lit(0.0)))\n",
    "    return final_df.drop(\"artist\", *tag_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbae88c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_pt(df: DataFrame, cols: list[str], outpath: str):\n",
    "    print(f\"Saving to {outpath}...\")\n",
    "    torch.save(\n",
    "        torch.from_numpy(\n",
    "            df.select(cols).toPandas().to_numpy(dtype=\"float32\")\n",
    "        ),\n",
    "        outpath\n",
    "    )\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3c5ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vars and dir\n",
    "out_dir: str = \"opt_result\"\n",
    "!mkdir -p $out_dir\n",
    "\n",
    "# Columns used for the original tensor\n",
    "lfm_og: list[str] = [\n",
    "    \"hbd0\",\n",
    "    \"bd0\",\n",
    "    \"hed0\",\n",
    "    \"ed0\",\n",
    "    \"e0\",\n",
    "    \"g10\",\n",
    "    \"g20\",\n",
    "    \"g30\",\n",
    "    \"g40\",\n",
    "    \"g50\",\n",
    "    \"ps0\",\n",
    "    \"t10\",\n",
    "    \"t20\",\n",
    "    \"t30\",\n",
    "    \"t40\",\n",
    "    \"t50\",\n",
    "    \"t60\",\n",
    "    *[f\"tag0_{i}\" for i in range(23)],\n",
    "    \"cc0\",\n",
    "    \"cp0\",\n",
    "    \"sc0\",\n",
    "    \"sp0\",\n",
    "\n",
    "    \"hbd1\",\n",
    "    \"bd1\",\n",
    "    \"hed1\",\n",
    "    \"ed1\",\n",
    "    \"e1\",\n",
    "    \"g11\",\n",
    "    \"g21\",\n",
    "    \"g31\",\n",
    "    \"g41\",\n",
    "    \"g51\",\n",
    "    \"ps1\",\n",
    "    \"t11\",\n",
    "    \"t21\",\n",
    "    \"t31\",\n",
    "    \"t41\",\n",
    "    \"t51\",\n",
    "    \"t61\",\n",
    "    *[f\"tag1_{i}\" for i in range(23)],\n",
    "    \"cc1\",\n",
    "    \"cp1\",\n",
    "    \"sc1\",\n",
    "    \"sp1\",\n",
    "\n",
    "    \"lfm\",\n",
    "    \"mrt\",\n",
    "    \"prt\",\n",
    "    \"lt\",\n",
    "]\n",
    "\n",
    "# Columns used for the original tensor with normalized popularity\n",
    "lfm_z: list[str] = [\n",
    "    \"hbd0\",\n",
    "    \"bd0\",\n",
    "    \"hed0\",\n",
    "    \"ed0\",\n",
    "    \"e0\",\n",
    "    \"g10\",\n",
    "    \"g20\",\n",
    "    \"g30\",\n",
    "    \"g40\",\n",
    "    \"g50\",\n",
    "    \"zps0\",\n",
    "    \"t10\",\n",
    "    \"t20\",\n",
    "    \"t30\",\n",
    "    \"t40\",\n",
    "    \"t50\",\n",
    "    \"t60\",\n",
    "    *[f\"tag0_{i}\" for i in range(23)],\n",
    "    \"cc0\",\n",
    "    \"zcp0\",\n",
    "    \"sc0\",\n",
    "    \"zsp0\",\n",
    "\n",
    "    \"hbd1\",\n",
    "    \"bd1\",\n",
    "    \"hed1\",\n",
    "    \"ed1\",\n",
    "    \"e1\",\n",
    "    \"g11\",\n",
    "    \"g21\",\n",
    "    \"g31\",\n",
    "    \"g41\",\n",
    "    \"g51\",\n",
    "    \"zps1\",\n",
    "    \"t11\",\n",
    "    \"t21\",\n",
    "    \"t31\",\n",
    "    \"t41\",\n",
    "    \"t51\",\n",
    "    \"t61\",\n",
    "    *[f\"tag1_{i}\" for i in range(23)],\n",
    "    \"cc1\",\n",
    "    \"zcp1\",\n",
    "    \"sc1\",\n",
    "    \"zsp1\",\n",
    "\n",
    "    \"lfm\",\n",
    "    \"mrt\",\n",
    "    \"prt\",\n",
    "    \"lt\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Columns used for the tensor that doesn't have LFM info\n",
    "mb_og: list[str] = [\n",
    "    \"hbd0\",\n",
    "    \"bd0\",\n",
    "    \"hed0\",\n",
    "    \"ed0\",\n",
    "    \"e0\",\n",
    "    \"g10\",\n",
    "    \"g20\",\n",
    "    \"g30\",\n",
    "    \"g40\",\n",
    "    \"g50\",\n",
    "    \"t10\",\n",
    "    \"t20\",\n",
    "    \"t30\",\n",
    "    \"t40\",\n",
    "    \"t50\",\n",
    "    \"t60\",\n",
    "    *[f\"mbtag0_{i}\" for i in range(23)],\n",
    "    \"cc0\",\n",
    "    \"sc0\",\n",
    "\n",
    "    \"hbd1\",\n",
    "    \"bd1\",\n",
    "    \"hed1\",\n",
    "    \"ed1\",\n",
    "    \"e1\",\n",
    "    \"g11\",\n",
    "    \"g21\",\n",
    "    \"g31\",\n",
    "    \"g41\",\n",
    "    \"g51\",\n",
    "    \"t11\",\n",
    "    \"t21\",\n",
    "    \"t31\",\n",
    "    \"t41\",\n",
    "    \"t51\",\n",
    "    \"t61\",\n",
    "    *[f\"mbtag1_{i}\" for i in range(23)],\n",
    "    \"cc1\",\n",
    "    \"sc1\",\n",
    "\n",
    "    \"mrt\",\n",
    "    \"prt\",\n",
    "    \"lt\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb94998",
   "metadata": {},
   "source": [
    "**MAIN LOOP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d9e6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading opt_data/mlp2023110x_train.pt\n",
      "Loaded array of 4662908 rows and 94 columns.\n",
      "Loading opt_data/mlp2023110x_test.pt\n",
      "Loaded array of 263196 rows and 94 columns.\n",
      "Saving to opt_result/mlp2023110x_train.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2023110x_test.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2023110x_train_norm.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2023110x_test_norm.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2023110x_train_mb.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2023110x_test_mb.pt...\n",
      "Done!\n",
      "Loading opt_data/mlp2023110.5x_train.pt\n",
      "Loaded array of 2713188 rows and 94 columns.\n",
      "Loading opt_data/mlp2023110.5x_test.pt\n",
      "Loaded array of 146576 rows and 94 columns.\n",
      "Saving to opt_result/mlp2023110.5x_train.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2023110.5x_test.pt...\n",
      "Done!\n",
      "Saving to opt_result/mlp2023110.5x_train_norm.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2023110.5x_test_norm.pt...\n",
      "Done!\n",
      "Saving to opt_result/mlp2023110.5x_train_mb.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2023110.5x_test_mb.pt...\n",
      "Done!\n",
      "Loading opt_data/mlp2023110.75x_train.pt\n",
      "Loaded array of 1742016 rows and 94 columns.\n",
      "Loading opt_data/mlp2023110.75x_test.pt\n",
      "Loaded array of 97864 rows and 94 columns.\n",
      "Saving to opt_result/mlp2023110.75x_train.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2023110.75x_test.pt...\n",
      "Done!\n",
      "Saving to opt_result/mlp2023110.75x_train_norm.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2023110.75x_test_norm.pt...\n",
      "Done!\n",
      "Saving to opt_result/mlp2023110.75x_train_mb.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2023110.75x_test_mb.pt...\n",
      "Done!\n",
      "Loading opt_data/mlp2023110.9x_train.pt\n",
      "Loaded array of 863004 rows and 94 columns.\n",
      "Loading opt_data/mlp2023110.9x_test.pt\n",
      "Loaded array of 50736 rows and 94 columns.\n",
      "Saving to opt_result/mlp2023110.9x_train.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2023110.9x_test.pt...\n",
      "Done!\n",
      "Saving to opt_result/mlp2023110.9x_train_norm.pt...\n",
      "Done!\n",
      "Saving to opt_result/mlp2023110.9x_test_norm.pt...\n",
      "Done!\n",
      "Saving to opt_result/mlp2023110.9x_train_mb.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2023110.9x_test_mb.pt...\n",
      "Done!\n",
      "Loading opt_data/mlp2021110x_train.pt\n",
      "Loaded array of 4149268 rows and 94 columns.\n",
      "Loading opt_data/mlp2021110x_test.pt\n",
      "Loaded array of 776836 rows and 94 columns.\n",
      "Saving to opt_result/mlp2021110x_train.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2021110x_test.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2021110x_train_norm.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2021110x_test_norm.pt...\n",
      "Done!\n",
      "Saving to opt_result/mlp2021110x_train_mb.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2021110x_test_mb.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Loading opt_data/mlp2021110.5x_train.pt\n",
      "Loaded array of 2393720 rows and 94 columns.\n",
      "Loading opt_data/mlp2021110.5x_test.pt\n",
      "Loaded array of 466044 rows and 94 columns.\n",
      "Saving to opt_result/mlp2021110.5x_train.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2021110.5x_test.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2021110.5x_train_norm.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2021110.5x_test_norm.pt...\n",
      "Done!\n",
      "Saving to opt_result/mlp2021110.5x_train_mb.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2021110.5x_test_mb.pt...\n",
      "Done!\n",
      "Loading opt_data/mlp2021110.75x_train.pt\n",
      "Loaded array of 1526336 rows and 94 columns.\n",
      "Loading opt_data/mlp2021110.75x_test.pt\n",
      "Loaded array of 313544 rows and 94 columns.\n",
      "Saving to opt_result/mlp2021110.75x_train.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2021110.75x_test.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2021110.75x_train_norm.pt...\n",
      "Done!\n",
      "Saving to opt_result/mlp2021110.75x_test_norm.pt...\n",
      "Done!\n",
      "Saving to opt_result/mlp2021110.75x_train_mb.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2021110.75x_test_mb.pt...\n",
      "Done!\n",
      "Loading opt_data/mlp2021110.9x_train.pt\n",
      "Loaded array of 752744 rows and 94 columns.\n",
      "Loading opt_data/mlp2021110.9x_test.pt\n",
      "Loaded array of 160996 rows and 94 columns.\n",
      "Saving to opt_result/mlp2021110.9x_train.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2021110.9x_test.pt...\n",
      "Done!\n",
      "Saving to opt_result/mlp2021110.9x_train_norm.pt...\n",
      "Done!\n",
      "Saving to opt_result/mlp2021110.9x_test_norm.pt...\n",
      "Done!\n",
      "Saving to opt_result/mlp2021110.9x_train_mb.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2021110.9x_test_mb.pt...\n",
      "Done!\n",
      "Loading opt_data/mlp2019110x_train.pt\n",
      "Loaded array of 3530604 rows and 94 columns.\n",
      "Loading opt_data/mlp2019110x_test.pt\n",
      "Loaded array of 1395500 rows and 94 columns.\n",
      "Saving to opt_result/mlp2019110x_train.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2019110x_test.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2019110x_train_norm.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2019110x_test_norm.pt...\n",
      "Done!\n",
      "Saving to opt_result/mlp2019110x_train_mb.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2019110x_test_mb.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Loading opt_data/mlp2019110.5x_train.pt\n",
      "Loaded array of 2033564 rows and 94 columns.\n",
      "Loading opt_data/mlp2019110.5x_test.pt\n",
      "Loaded array of 826200 rows and 94 columns.\n",
      "Saving to opt_result/mlp2019110.5x_train.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2019110.5x_test.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2019110.5x_train_norm.pt...\n",
      "Done!\n",
      "Saving to opt_result/mlp2019110.5x_test_norm.pt...\n",
      "Done!\n",
      "Saving to opt_result/mlp2019110.5x_train_mb.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2019110.5x_test_mb.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Loading opt_data/mlp2019110.75x_train.pt\n",
      "Loaded array of 1286384 rows and 94 columns.\n",
      "Loading opt_data/mlp2019110.75x_test.pt\n",
      "Loaded array of 553496 rows and 94 columns.\n",
      "Saving to opt_result/mlp2019110.75x_train.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2019110.75x_test.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2019110.75x_train_norm.pt...\n",
      "Done!\n",
      "Saving to opt_result/mlp2019110.75x_test_norm.pt...\n",
      "Done!\n",
      "Saving to opt_result/mlp2019110.75x_train_mb.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2019110.75x_test_mb.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Loading opt_data/mlp2019110.9x_train.pt\n",
      "Loaded array of 631620 rows and 94 columns.\n",
      "Loading opt_data/mlp2019110.9x_test.pt\n",
      "Loaded array of 282120 rows and 94 columns.\n",
      "Saving to opt_result/mlp2019110.9x_train.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2019110.9x_test.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2019110.9x_train_norm.pt...\n",
      "Done!\n",
      "Saving to opt_result/mlp2019110.9x_test_norm.pt...\n",
      "Done!\n",
      "Saving to opt_result/mlp2019110.9x_train_mb.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Saving to opt_result/mlp2019110.9x_test_mb.pt...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# loop\n",
    "for year in [2023, 2021, 2019]:\n",
    "    for perc in [0, 0.5, 0.75, 0.9]:\n",
    "        # filename\n",
    "        train_name: str = f\"mlp{year}11{perc}x_train.pt\"\n",
    "        test_name: str = f\"mlp{year}11{perc}x_test.pt\"\n",
    "\n",
    "        # data read\n",
    "        train_df = get_df_from_pt(spark, f\"{data_dir}/{train_name}\", pt_schema)\n",
    "        test_df = get_df_from_pt(spark, f\"{data_dir}/{test_name}\", pt_schema)\n",
    "\n",
    "        # add norm cols\n",
    "        for c_name in [f\"{c}{i}\" for c in [\"ps\", \"cp\", \"sp\"] for i in [0, 1]]:\n",
    "            train_df, test_df = add_z_col(train_df, test_df, c_name)\n",
    "\n",
    "        # persist intermediate result once, after all the shared z_col work\n",
    "        train_df = train_df.orderBy(\"_row_index\").persist(StorageLevel.DISK_ONLY)\n",
    "        test_df = test_df.orderBy(\"_row_index\").persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "        # now compute the MB-tags versions + persist\n",
    "        train_df_tagged = add_mb_tags(train_df, complete_df).orderBy(\"_row_index\").persist(StorageLevel.DISK_ONLY)\n",
    "        test_df_tagged = add_mb_tags(test_df, complete_df).orderBy(\"_row_index\").persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "        # save original\n",
    "        save_to_pt(train_df, lfm_og, f\"{out_dir}/{train_name}\")\n",
    "        save_to_pt(test_df, lfm_og, f\"{out_dir}/{test_name}\")\n",
    "\n",
    "        # save normalized\n",
    "        save_to_pt(train_df, lfm_z, f\"{out_dir}/mlp{year}11{perc}x_train_norm.pt\")\n",
    "        save_to_pt(test_df, lfm_z, f\"{out_dir}/mlp{year}11{perc}x_test_norm.pt\")\n",
    "\n",
    "        # save with mb tags\n",
    "        save_to_pt(train_df_tagged, mb_og, f\"{out_dir}/mlp{year}11{perc}x_train_mb.pt\")\n",
    "        save_to_pt(test_df_tagged, mb_og, f\"{out_dir}/mlp{year}11{perc}x_test_mb.pt\")\n",
    "\n",
    "        # clean up everything\n",
    "        train_df.unpersist()\n",
    "        test_df.unpersist()\n",
    "        train_df_tagged.unpersist()\n",
    "        test_df_tagged.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7e78232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp2019110.5x_test_mb.pt      mlp2021110.9x_test_mb.pt\n",
      "mlp2019110.5x_test_norm.pt    mlp2021110.9x_test_norm.pt\n",
      "mlp2019110.5x_test.pt\t      mlp2021110.9x_test.pt\n",
      "mlp2019110.5x_train_mb.pt     mlp2021110.9x_train_mb.pt\n",
      "mlp2019110.5x_train_norm.pt   mlp2021110.9x_train_norm.pt\n",
      "mlp2019110.5x_train.pt\t      mlp2021110.9x_train.pt\n",
      "mlp2019110.75x_test_mb.pt     mlp2021110x_test_mb.pt\n",
      "mlp2019110.75x_test_norm.pt   mlp2021110x_test_norm.pt\n",
      "mlp2019110.75x_test.pt\t      mlp2021110x_test.pt\n",
      "mlp2019110.75x_train_mb.pt    mlp2021110x_train_mb.pt\n",
      "mlp2019110.75x_train_norm.pt  mlp2021110x_train_norm.pt\n",
      "mlp2019110.75x_train.pt       mlp2021110x_train.pt\n",
      "mlp2019110.9x_test_mb.pt      mlp2023110.5x_test_mb.pt\n",
      "mlp2019110.9x_test_norm.pt    mlp2023110.5x_test_norm.pt\n",
      "mlp2019110.9x_test.pt\t      mlp2023110.5x_test.pt\n",
      "mlp2019110.9x_train_mb.pt     mlp2023110.5x_train_mb.pt\n",
      "mlp2019110.9x_train_norm.pt   mlp2023110.5x_train_norm.pt\n",
      "mlp2019110.9x_train.pt\t      mlp2023110.5x_train.pt\n",
      "mlp2019110x_test_mb.pt\t      mlp2023110.75x_test_mb.pt\n",
      "mlp2019110x_test_norm.pt      mlp2023110.75x_test_norm.pt\n",
      "mlp2019110x_test.pt\t      mlp2023110.75x_test.pt\n",
      "mlp2019110x_train_mb.pt       mlp2023110.75x_train_mb.pt\n",
      "mlp2019110x_train_norm.pt     mlp2023110.75x_train_norm.pt\n",
      "mlp2019110x_train.pt\t      mlp2023110.75x_train.pt\n",
      "mlp2021110.5x_test_mb.pt      mlp2023110.9x_test_mb.pt\n",
      "mlp2021110.5x_test_norm.pt    mlp2023110.9x_test_norm.pt\n",
      "mlp2021110.5x_test.pt\t      mlp2023110.9x_test.pt\n",
      "mlp2021110.5x_train_mb.pt     mlp2023110.9x_train_mb.pt\n",
      "mlp2021110.5x_train_norm.pt   mlp2023110.9x_train_norm.pt\n",
      "mlp2021110.5x_train.pt\t      mlp2023110.9x_train.pt\n",
      "mlp2021110.75x_test_mb.pt     mlp2023110x_test_mb.pt\n",
      "mlp2021110.75x_test_norm.pt   mlp2023110x_test_norm.pt\n",
      "mlp2021110.75x_test.pt\t      mlp2023110x_test.pt\n",
      "mlp2021110.75x_train_mb.pt    mlp2023110x_train_mb.pt\n",
      "mlp2021110.75x_train_norm.pt  mlp2023110x_train_norm.pt\n",
      "mlp2021110.75x_train.pt       mlp2023110x_train.pt\n"
     ]
    }
   ],
   "source": [
    "!ls opt_result/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad76f49",
   "metadata": {},
   "source": [
    "## cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "382b6ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_mlp2019110.5x_test.parquet    temp_mlp2021110.9x_test.parquet\n",
      "temp_mlp2019110.5x_train.parquet   temp_mlp2021110.9x_train.parquet\n",
      "temp_mlp2019110.75x_test.parquet   temp_mlp2021110x_test.parquet\n",
      "temp_mlp2019110.75x_train.parquet  temp_mlp2021110x_train.parquet\n",
      "temp_mlp2019110.9x_test.parquet    temp_mlp2023110.5x_test.parquet\n",
      "temp_mlp2019110.9x_train.parquet   temp_mlp2023110.5x_train.parquet\n",
      "temp_mlp2019110x_test.parquet\t   temp_mlp2023110.75x_test.parquet\n",
      "temp_mlp2019110x_train.parquet\t   temp_mlp2023110.75x_train.parquet\n",
      "temp_mlp2021110.5x_test.parquet    temp_mlp2023110.9x_test.parquet\n",
      "temp_mlp2021110.5x_train.parquet   temp_mlp2023110.9x_train.parquet\n",
      "temp_mlp2021110.75x_test.parquet   temp_mlp2023110x_test.parquet\n",
      "temp_mlp2021110.75x_train.parquet  temp_mlp2023110x_train.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls temp_mlp*.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f7edfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm temp_mlp*.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09e51f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
