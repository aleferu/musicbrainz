{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: '{device}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65419/2425721752.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  x_train = torch.load(\"ds/mlp20200300x_train.pt\")\n",
      "/tmp/ipykernel_65419/2425721752.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  x_test = torch.load(\"ds/mlp20200300x_test.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: torch.Size([3203952, 76])\n",
      "x_test shape: torch.Size([1722152, 76])\n",
      "y_train shape: torch.Size([3203952])\n",
      "y_test shape: torch.Size([1722152])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65419/2425721752.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  y_train = torch.load(\"ds/mlp20200300y_train.pt\")\n",
      "/tmp/ipykernel_65419/2425721752.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  y_test = torch.load(\"ds/mlp20200300y_test.pt\")\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.load(\"ds/mlp20200300x_train.pt\")\n",
    "x_test = torch.load(\"ds/mlp20200300x_test.pt\")\n",
    "y_train = torch.load(\"ds/mlp20200300y_train.pt\")\n",
    "y_test = torch.load(\"ds/mlp20200300y_test.pt\")\n",
    "\n",
    "print(f\"x_train shape:\", x_train.shape)\n",
    "print(f\"x_test shape:\", x_test.shape)\n",
    "print(f\"y_train shape:\", y_train.shape)\n",
    "print(f\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "test_dataset = TensorDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * x_train.shape[0])\n",
    "val_size = x_train.shape[0] - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size: int):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs):\n",
    "    best_threshold = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        model.train()  # Set model to training mode\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in tqdm.tqdm(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(inputs).squeeze(1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()  # Backpropagate the loss\n",
    "            optimizer.step()  # Update the weights\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        print(\"Train loss:\", train_loss)\n",
    "    \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_probs = []  # Store probabilities for ROC-AUC\n",
    "        print(\"Validating...\")\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in tqdm.tqdm(val_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                outputs = model(inputs).squeeze(1)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Get predictions and probabilities (assuming binary classification with sigmoid output)\n",
    "                probs = torch.sigmoid(outputs).cpu().numpy()  # Apply sigmoid if needed\n",
    "                labels = targets.cpu().numpy()\n",
    "\n",
    "                all_labels.extend(labels)\n",
    "                all_probs.extend(probs.flatten())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Find threshold for predictions\n",
    "        print(\"Looking for threshold\")\n",
    "        best_threshold = 0\n",
    "        best_f1 = 0\n",
    "        for threshold in tqdm.tqdm(np.arange(0.2, 0.81, 0.01)):\n",
    "            preds_binary = (all_probs > threshold).astype(int)\n",
    "            cm = confusion_matrix(all_labels, preds_binary)\n",
    "            tp = cm[1, 1]\n",
    "            fp = cm[0, 1]\n",
    "            fn = cm[1, 0]\n",
    "            tn = cm[0, 0]\n",
    "            precision = 0 if tp == 0 else tp / (tp + fp)\n",
    "            recall = 0 if tp == 0 else tp / (tp + fn)\n",
    "            f1 = 0 if precision * recall == 0 else 2 * precision * recall / (precision + recall)\n",
    "            if f1 > best_f1:\n",
    "                best_threshold = threshold\n",
    "                best_f1 = f1\n",
    "        print(f\"Best threshold: {best_threshold}\")\n",
    "        all_preds = (all_probs > best_threshold).astype(int)\n",
    "\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        tp = cm[1, 1]\n",
    "        fp = cm[0, 1]\n",
    "        fn = cm[1, 0]\n",
    "        tn = cm[0, 0]\n",
    "\n",
    "        accuracy = (tp + tn) / (tp + fp + fn + tn) if (tp + fp + fn + tn) > 0 else 0.0 # Handle division by zero\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        roc_auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "        print(f\"Validation Metrics - Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"Loss      :{val_loss:.4f}\")\n",
    "        print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall:    {recall:.4f}\")\n",
    "        print(f\"F1-score:  {f1:.4f}\")\n",
    "        print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{tp} {fn}\\n{fp} {tn}\")\n",
    "    \n",
    "    return best_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device, criterion, best_threshold):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm.tqdm(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(inputs).squeeze(1)\n",
    "            loss = criterion(outputs, targets)  # Use criterion here\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy()  # Apply sigmoid if needed\n",
    "            preds = (probs > best_threshold).astype(int)  # Convert probabilities to predictions\n",
    "            labels = targets.cpu().numpy()\n",
    "\n",
    "            all_labels.extend(labels)\n",
    "            all_preds.extend(preds.flatten())\n",
    "            all_probs.extend(probs.flatten())\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    tp = cm[1, 1]\n",
    "    fp = cm[0, 1]\n",
    "    fn = cm[1, 0]\n",
    "    tn = cm[0, 0]\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn) if (tp + fp + fn + tn) > 0 else 0.0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    try:\n",
    "      roc_auc = roc_auc_score(all_labels, all_probs)\n",
    "    except ValueError:\n",
    "        roc_auc = 0.0\n",
    "\n",
    "    print(f\"Test Metrics:\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-score:  {f1:.4f}\")\n",
    "    print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{tp} {fn}\\n{fp} {tn}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\") # Print the loss as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80099/80099 [01:57<00:00, 682.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4572873471505017\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20025/20025 [00:11<00:00, 1723.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [01:25<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.4300000000000002\n",
      "Validation Metrics - Epoch 1/3:\n",
      "Loss      :0.4171\n",
      "Accuracy:  0.7771\n",
      "Precision: 0.7185\n",
      "Recall:    0.9102\n",
      "F1-score:  0.8031\n",
      "ROC-AUC:   0.8827\n",
      "Confusion Matrix:\n",
      "291208 28713\n",
      "114089 206781\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80099/80099 [01:58<00:00, 678.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.36714490110177606\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20025/20025 [00:11<00:00, 1712.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [01:25<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.38000000000000017\n",
      "Validation Metrics - Epoch 2/3:\n",
      "Loss      :0.4351\n",
      "Accuracy:  0.8351\n",
      "Precision: 0.8059\n",
      "Recall:    0.8821\n",
      "F1-score:  0.8423\n",
      "ROC-AUC:   0.9242\n",
      "Confusion Matrix:\n",
      "282206 37715\n",
      "67982 252888\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80099/80099 [01:59<00:00, 667.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.33788825660973004\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20025/20025 [00:11<00:00, 1713.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [01:24<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.4300000000000002\n",
      "Validation Metrics - Epoch 3/3:\n",
      "Loss      :0.3245\n",
      "Accuracy:  0.8565\n",
      "Precision: 0.8493\n",
      "Recall:    0.8663\n",
      "F1-score:  0.8577\n",
      "ROC-AUC:   0.9345\n",
      "Confusion Matrix:\n",
      "277144 42777\n",
      "49161 271709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4300000000000002"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP(input_size).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "best_threshold = train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device,\n",
    "    3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53818/53818 [00:32<00:00, 1672.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics:\n",
      "Accuracy:  0.7078\n",
      "Precision: 0.6996\n",
      "Recall:    0.7282\n",
      "F1-score:  0.7136\n",
      "ROC-AUC:   0.7581\n",
      "Confusion Matrix:\n",
      "626994 234082\n",
      "269216 591860\n",
      "Test Loss: 7.1332\n"
     ]
    }
   ],
   "source": [
    "test(\n",
    "    model,\n",
    "    test_loader,\n",
    "    device,\n",
    "    criterion,\n",
    "    best_threshold\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicbrainz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
