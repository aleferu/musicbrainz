{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31d33376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear, ReLU, Sequential\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.nn import HeteroConv, GATConv, SAGEConv, Linear\n",
    "from torch_geometric.nn.aggr import Aggregation, MultiAggregation\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.typing import OptPairTensor, Adj, Size\n",
    "import torch_geometric.transforms as T\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "import os.path as path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import tqdm\n",
    "import copy\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e12322ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"ds/\"\n",
    "model_name = \"main_mb_test\"\n",
    "year = 2019\n",
    "month = 11\n",
    "perc = 0.9\n",
    "latest_epoch = 0\n",
    "train_hd = f\"train_hdmb_{year}_{month}_{perc}.pt\"\n",
    "# train_hd = f\"train_hd_{year}_{month}_{perc}.pt\"\n",
    "# train_hd = f\"train_hd_nomatch_{year}_{month}_{perc}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdb5b1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8173/2815972150.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(path.join(data_folder, train_hd))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.load(path.join(data_folder, train_hd))\n",
    "\n",
    "data.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd6e6398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artist channels: 17\n",
      "Track channels: 5\n",
      "Tag channels: 24\n"
     ]
    }
   ],
   "source": [
    "artist_channels = data[\"artist\"].x.size(1)\n",
    "track_channels = data[\"track\"].x.size(1)\n",
    "tag_channels = data[\"tag\"].x.size(1)\n",
    "\n",
    "print(f\"Artist channels: {artist_channels}\")\n",
    "print(f\"Track channels: {track_channels}\")\n",
    "print(f\"Tag channels: {tag_channels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82faa4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data[\"artist\", \"collab_with\", \"artist\"].edge_index = data[\"artist\", \"collab_with\", \"artist\"].edge_index.contiguous()\n",
    "\n",
    "print(f\"Device: '{device}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "956bf7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train_loader...\n",
      "Creating val loader...\n",
      "Number of train batches: 22067\n",
      "Number of validation batches: 5517\n"
     ]
    }
   ],
   "source": [
    "compt_tree_size = [25, 20]\n",
    "\n",
    "edge_indices = torch.arange(data[\"artist\", \"collab_with\", \"artist\"].edge_index.shape[1])\n",
    "\n",
    "# Shuffle and split\n",
    "num_edges = len(edge_indices)\n",
    "perm = torch.randperm(num_edges)\n",
    "split_idx = int(0.8 * num_edges)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(perm[:split_idx])\n",
    "val_sampler = SubsetRandomSampler(perm[split_idx:])\n",
    "\n",
    "print(\"Creating train_loader...\")\n",
    "train_loader = LinkNeighborLoader(\n",
    "    data=data,\n",
    "    num_neighbors=compt_tree_size,\n",
    "    neg_sampling_ratio=1,\n",
    "    edge_label_index=(\"artist\", \"collab_with\", \"artist\"),\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    "    sampler=train_sampler,\n",
    ")\n",
    "\n",
    "print(\"Creating val loader...\")\n",
    "val_loader = LinkNeighborLoader(\n",
    "    data=data,\n",
    "    num_neighbors=compt_tree_size,\n",
    "    neg_sampling_ratio=1,\n",
    "    edge_label_index=(\"artist\", \"collab_with\", \"artist\"),\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    "    sampler=val_sampler,\n",
    ")\n",
    "\n",
    "print(\"Number of train batches:\", len(train_loader))\n",
    "print(\"Number of validation batches:\", len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91d598ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, metadata, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.metadata = metadata\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.conv1 = HeteroConv({\n",
    "            (\"artist\", \"collab_with\", \"artist\"): GATConv((artist_channels, artist_channels), hidden_channels, heads=3, concat=False),\n",
    "            (\"artist\", \"has_tag_artists\", \"tag\"): SAGEConv((artist_channels, tag_channels), hidden_channels, normalize=True, project=True),\n",
    "            (\"artist\", \"last_fm_match\", \"artist\"): GATConv((artist_channels, artist_channels), hidden_channels, heads=3, concat=False),\n",
    "            (\"track\", \"has_tag_tracks\", \"tag\"): SAGEConv((track_channels, tag_channels), hidden_channels, normalize=True, project=True),\n",
    "            (\"artist\", \"linked_to\", \"artist\"): GATConv((artist_channels, artist_channels), hidden_channels, heads=3, concat=False),\n",
    "            (\"artist\", \"musically_related_to\", \"artist\"): GATConv((artist_channels, artist_channels), hidden_channels, heads=3, concat=False),\n",
    "            (\"artist\", \"personally_related_to\", \"artist\"): GATConv((artist_channels, artist_channels), hidden_channels, heads=3, concat=False),\n",
    "            (\"tag\", \"tags_artists\", \"artist\"): SAGEConv((tag_channels, artist_channels), hidden_channels, normalize=True, project=True),\n",
    "            (\"tag\", \"tags_tracks\", \"track\"): SAGEConv((tag_channels, track_channels), hidden_channels, normalize=True, project=True),\n",
    "            (\"track\", \"worked_by\", \"artist\"): SAGEConv((track_channels, artist_channels), hidden_channels, normalize=True, project=True),\n",
    "            (\"artist\", \"worked_in\", \"track\"): SAGEConv((artist_channels, track_channels), hidden_channels, normalize=True, project=True),\n",
    "        }, aggr=\"mean\")\n",
    "\n",
    "        self.conv2 = HeteroConv({\n",
    "            (\"artist\", \"collab_with\", \"artist\"): GATConv((hidden_channels, hidden_channels), hidden_channels, heads=3, concat=False),\n",
    "            (\"artist\", \"has_tag_artists\", \"tag\"): SAGEConv((hidden_channels, hidden_channels), hidden_channels, normalize=True, project=True),\n",
    "            (\"artist\", \"last_fm_match\", \"artist\"): GATConv((hidden_channels, hidden_channels), hidden_channels, heads=3, concat=False),\n",
    "            (\"track\", \"has_tag_tracks\", \"tag\"): SAGEConv((hidden_channels, hidden_channels), hidden_channels, normalize=True, project=True),\n",
    "            (\"artist\", \"linked_to\", \"artist\"): GATConv((hidden_channels, hidden_channels), hidden_channels, heads=3, concat=False),\n",
    "            (\"artist\", \"musically_related_to\", \"artist\"): GATConv((hidden_channels, hidden_channels), hidden_channels, heads=3, concat=False),\n",
    "            (\"artist\", \"personally_related_to\", \"artist\"): GATConv((hidden_channels, hidden_channels), hidden_channels, heads=3, concat=False),\n",
    "            (\"tag\", \"tags_artists\", \"artist\"): SAGEConv((hidden_channels, hidden_channels), hidden_channels, normalize=True, project=True),\n",
    "            (\"tag\", \"tags_tracks\", \"track\"): SAGEConv((hidden_channels, hidden_channels), hidden_channels, normalize=True, project=True),\n",
    "            (\"track\", \"worked_by\", \"artist\"): SAGEConv((hidden_channels, hidden_channels), hidden_channels, normalize=True, project=True),\n",
    "            (\"artist\", \"worked_in\", \"track\"): SAGEConv((hidden_channels, hidden_channels), hidden_channels, normalize=True, project=True),\n",
    "        }, aggr=\"mean\")\n",
    "\n",
    "        self.linear1 = Linear(hidden_channels * 2, hidden_channels * 4)\n",
    "        self.linear2 = Linear(hidden_channels * 4, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict1 = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict2 = self.conv2(x_dict1, edge_index_dict)\n",
    "\n",
    "        x_artist = torch.cat([x_dict1['artist'], x_dict2['artist']], dim=-1)\n",
    "\n",
    "        x_artist = self.linear1(x_artist)\n",
    "        x_artist = F.relu(x_artist)\n",
    "        x_artist = self.linear2(x_artist)\n",
    "\n",
    "        # Normalize the artist node features\n",
    "        x_artist = F.normalize(x_artist, p=2, dim=-1)\n",
    "\n",
    "        # Update the dictionary with the new 'artist' features, leaving other nodes unchanged\n",
    "        x_dict['artist'] = x_artist\n",
    "\n",
    "        return x_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9525bb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs, patience=5):\n",
    "    best_val_f1 = 0.0\n",
    "    best_threshold = 0\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "    train_losses = list()\n",
    "    val_losses = list()\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for sampled_data in tqdm.tqdm(train_loader):\n",
    "            # Move data to device\n",
    "            sampled_data = sampled_data.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_dict = model(sampled_data.x_dict, sampled_data.edge_index_dict)\n",
    "            \n",
    "            # Get predictions and labels for the 'collab_with' edge type\n",
    "            edge_label_index = sampled_data['artist', 'collab_with', 'artist'].edge_label_index\n",
    "            edge_label = sampled_data['artist', 'collab_with', 'artist'].edge_label\n",
    "\n",
    "            src_emb = pred_dict['artist'][edge_label_index[0]]  # Source node embeddings\n",
    "            dst_emb = pred_dict['artist'][edge_label_index[1]]  # Destination node embeddings\n",
    "            \n",
    "            # Compute the dot product between source and destination embeddings\n",
    "            preds = (src_emb * dst_emb).sum(dim=-1)  # Scalar for each edge\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(preds, edge_label.float())\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Average loss for the epoch\n",
    "        epoch_loss /= len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        print(\"Computing validation metrics\")\n",
    "        \n",
    "        # Validation metrics\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient computation for validation\n",
    "            for sampled_data in tqdm.tqdm(val_loader):\n",
    "                # Move data to device\n",
    "                sampled_data = sampled_data.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                pred_dict = model(sampled_data.x_dict, sampled_data.edge_index_dict)\n",
    "\n",
    "                # Get predictions and labels for the 'collab_with' edge type\n",
    "                edge_label_index = sampled_data['artist', 'collab_with', 'artist'].edge_label_index\n",
    "                edge_label = sampled_data['artist', 'collab_with', 'artist'].edge_label\n",
    "\n",
    "                src_emb = pred_dict['artist'][edge_label_index[0]]  # Source node embeddings\n",
    "                dst_emb = pred_dict['artist'][edge_label_index[1]]  # Destination node embeddings\n",
    "\n",
    "                # Compute the dot product between source and destination embeddings\n",
    "                preds = (src_emb * dst_emb).sum(dim=-1)  # Scalar for each edge\n",
    "\n",
    "                loss = criterion(preds, edge_label.float())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                probs = torch.sigmoid(preds)  # Convert to probabilities\n",
    "\n",
    "                # Collect predictions, probabilities, and labels\n",
    "                all_labels.append(edge_label.cpu())\n",
    "                all_probs.append(probs.cpu())\n",
    "        \n",
    "        # Concatenate all predictions and labels\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        all_probs = torch.cat(all_probs)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Find threshold for predictions\n",
    "        print(\"Looking for threshold\")\n",
    "        best_threshold_epoch = 0\n",
    "        best_f1_epoch = 0\n",
    "        for threshold in tqdm.tqdm(np.arange(0.2, 0.91, 0.01)):\n",
    "            preds_binary = (all_probs > threshold).long()\n",
    "            cm = confusion_matrix(all_labels, preds_binary)\n",
    "            tp = cm[1, 1]\n",
    "            fp = cm[0, 1]\n",
    "            fn = cm[1, 0]\n",
    "            tn = cm[0, 0]\n",
    "            precision = 0 if tp == 0 else tp / (tp + fp)\n",
    "            recall = 0 if tp == 0 else tp / (tp + fn)\n",
    "            f1 = 0 if precision * recall == 0 else 2 * precision * recall / (precision + recall)\n",
    "            if f1 > best_f1_epoch:\n",
    "                best_threshold_epoch = threshold\n",
    "                best_f1_epoch = f1\n",
    "        print(f\"Best threshold: {best_threshold_epoch}\")\n",
    "        all_preds = (all_probs > best_threshold_epoch).long()\n",
    "        \n",
    "        # Compute metrics\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        tp = cm[1, 1]\n",
    "        fp = cm[0, 1]\n",
    "        fn = cm[1, 0]\n",
    "        tn = cm[0, 0]\n",
    "        accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        roc_auc = roc_auc_score(all_labels, all_probs)\n",
    "        \n",
    "        # Print validation metrics\n",
    "        print(f\"Validation Metrics - Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"Loss:      {val_loss:.4f}\")\n",
    "        print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall:    {recall:.4f}\")\n",
    "        print(f\"F1-score:  {f1:.4f}\")\n",
    "        print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{tp} {fn}\\n{fp} {tn}\")\n",
    "\n",
    "        new_row = {\n",
    "            \"model\": model_name,\n",
    "            \"year\": year,\n",
    "            \"month\": month,\n",
    "            \"perc\": perc,\n",
    "            \"epoch\": latest_epoch + epoch + 1,\n",
    "            \"train_loss\": epoch_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"acc\": accuracy,\n",
    "            \"prec\": precision,\n",
    "            \"rec\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"auc\": roc_auc,\n",
    "            \"tp\": int(tp),\n",
    "            \"fp\": int(fp),\n",
    "            \"fn\": int(fn),\n",
    "            \"tn\": int(tn),\n",
    "            \"best_threshold\": best_threshold_epoch,\n",
    "            \"done\": False\n",
    "        }\n",
    "        url = \"http://localhost:5000/save_results\"\n",
    "        response = requests.post(url, json=new_row)\n",
    "        assert response.status_code == 200\n",
    "\n",
    "        torch.save(model.state_dict(), f\"./model_{model_name}_{year}_{month}_{perc}_{latest_epoch + epoch + 1}.pth\")\n",
    "\n",
    "        if f1 > best_val_f1:\n",
    "            best_val_f1 = f1\n",
    "            best_threshold = best_threshold_epoch\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = latest_epoch + epoch + 1\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print(f\"Early stopping!!!\")\n",
    "                print(f\"Early stopping!!!\")\n",
    "                print(f\"Early stopping!!!\")\n",
    "                print(\"Best epoch:\", best_epoch)\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "\n",
    "    return best_threshold, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71d770d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22067 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 0.0000\n",
      "Computing validation metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5517 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:00<00:00, 1108.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.5700000000000003\n",
      "Validation Metrics - Epoch 1/100:\n",
      "Loss:      0.0001\n",
      "Accuracy:  0.6562\n",
      "Precision: 0.6000\n",
      "Recall:    0.9375\n",
      "F1-score:  0.7317\n",
      "ROC-AUC:   0.6016\n",
      "Confusion Matrix:\n",
      "60 4\n",
      "40 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22067 [00:00<?, ?it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7bb8d029f560>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aleferu/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1477, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/aleferu/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1441, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/aleferu/miniforge3/envs/musicbrainz/lib/python3.12/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aleferu/miniforge3/envs/musicbrainz/lib/python3.12/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aleferu/miniforge3/envs/musicbrainz/lib/python3.12/multiprocessing/connection.py\", line 1136, in wait\n",
      "    ready = selector.select(timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aleferu/miniforge3/envs/musicbrainz/lib/python3.12/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n",
      "  0%|          | 0/22067 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, Training Loss: 0.0000\n",
      "Computing validation metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5517 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:00<00:00, 1307.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.5800000000000003\n",
      "Validation Metrics - Epoch 2/100:\n",
      "Loss:      0.0001\n",
      "Accuracy:  0.7500\n",
      "Precision: 0.6702\n",
      "Recall:    0.9844\n",
      "F1-score:  0.7975\n",
      "ROC-AUC:   0.6920\n",
      "Confusion Matrix:\n",
      "63 1\n",
      "31 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22067 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, Training Loss: 0.0000\n",
      "Computing validation metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5517 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:00<00:00, 1298.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.5700000000000003\n",
      "Validation Metrics - Epoch 3/100:\n",
      "Loss:      0.0001\n",
      "Accuracy:  0.7500\n",
      "Precision: 0.6702\n",
      "Recall:    0.9844\n",
      "F1-score:  0.7975\n",
      "ROC-AUC:   0.6699\n",
      "Confusion Matrix:\n",
      "63 1\n",
      "31 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22067 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, Training Loss: 0.0000\n",
      "Computing validation metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5517 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:00<00:00, 1278.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.5600000000000003\n",
      "Validation Metrics - Epoch 4/100:\n",
      "Loss:      0.0001\n",
      "Accuracy:  0.7734\n",
      "Precision: 0.6882\n",
      "Recall:    1.0000\n",
      "F1-score:  0.8153\n",
      "ROC-AUC:   0.7074\n",
      "Confusion Matrix:\n",
      "64 0\n",
      "29 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22067 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "model = GNN(metadata=data.metadata(), hidden_channels=64, out_channels=64).to(device)\n",
    "\n",
    "if latest_epoch > 0:\n",
    "    model.load_state_dict(torch.load(f\"./model_{model_name}_{year}_{month}_{perc}_{latest_epoch}.pth\"))\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "best_threshold, train_losses, val_losses = train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    F.binary_cross_entropy_with_logits,\n",
    "    device,\n",
    "    100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f570fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST THRESHOLD: 0.7300000000000004\n"
     ]
    }
   ],
   "source": [
    "print(\"BEST THRESHOLD:\", best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebb80aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"./model_{model_name}_{year}_{month}_{perc}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicbrainz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
