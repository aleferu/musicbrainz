{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31d33376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "import os.path as path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e12322ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"ds_float32_tagfix/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdb5b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = HeteroData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1192f48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artist tensor shape: torch.Size([1489250, 16])\n",
      "Track tensor shape: torch.Size([24324100, 4])\n",
      "Tag tensor shape: torch.Size([23, 24])\n",
      "collab_with index tensor shape: torch.Size([2, 2463052])\n",
      "collab_with attr tensor shape: torch.Size([2463052, 1])\n",
      "has_tag_artists index tensor shape: torch.Size([2, 2410207])\n",
      "has_tag_tracks index tensor shape: torch.Size([2, 4030735])\n",
      "last_fm_match index tensor shape: torch.Size([2, 154865250])\n",
      "last_fm_match attr tensor shape: torch.Size([154865250, 1])\n",
      "linked_to index tensor shape: torch.Size([2, 23128])\n",
      "linked_to attr tensor shape: torch.Size([23128, 1])\n",
      "musically_related_to index tensor shape: torch.Size([2, 373262])\n",
      "musically_related_to attr tensor shape: torch.Size([373262, 1])\n",
      "personally_related_to index tensor shape: torch.Size([2, 26720])\n",
      "personally_related_to attr tensor shape: torch.Size([26720, 1])\n",
      "tags_artists index tensor shape: torch.Size([2, 2410207])\n",
      "tags_tracks index tensor shape: torch.Size([2, 4030735])\n",
      "worked_by index tensor shape: torch.Size([2, 27661673])\n",
      "worked_in index tensor shape: torch.Size([2, 27661673])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"artist\"].x = torch.load(path.join(data_folder, \"artists.pt\"), weights_only=True)\n",
    "print(\"Artist tensor shape:\", data[\"artist\"].x.shape)\n",
    "\n",
    "data[\"track\"].x = torch.load(path.join(data_folder, \"tracks.pt\"), weights_only=True)\n",
    "print(\"Track tensor shape:\", data[\"track\"].x.shape)\n",
    "\n",
    "data[\"tag\"].x = torch.load(path.join(data_folder, \"tags.pt\"), weights_only=True)\n",
    "print(\"Tag tensor shape:\", data[\"tag\"].x.shape)\n",
    "\n",
    "\n",
    "data[\"artist\", \"collab_with\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"collab_with.pt\"), weights_only=True).t().long()\n",
    "data[\"artist\", \"collab_with\", \"artist\"].edge_attr = torch.load(path.join(data_folder, \"collab_with_attr.pt\"), weights_only=True)\n",
    "print(\"collab_with index tensor shape:\", data[\"artist\", \"collab_with\", \"artist\"].edge_index.shape)\n",
    "print(\"collab_with attr tensor shape:\", data[\"artist\", \"collab_with\", \"artist\"].edge_attr.shape)\n",
    "\n",
    "data[\"artist\", \"has_tag_artists\", \"tag\"].edge_index = torch.load(path.join(data_folder, \"has_tag_artists.pt\"), weights_only=True).t().long()\n",
    "data[\"track\", \"has_tag_tracks\", \"tag\"].edge_index = torch.load(path.join(data_folder, \"has_tag_tracks.pt\"), weights_only=True).t().long()\n",
    "print(\"has_tag_artists index tensor shape:\", data[\"artist\", \"has_tag_artists\", \"tag\"].edge_index.shape)\n",
    "print(\"has_tag_tracks index tensor shape:\", data[\"track\", \"has_tag_tracks\", \"tag\"].edge_index.shape)\n",
    "\n",
    "data[\"artist\", \"last_fm_match\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"last_fm_match.pt\"), weights_only=True).t().long()\n",
    "data[\"artist\", \"last_fm_match\", \"artist\"].edge_attr = torch.load(path.join(data_folder, \"last_fm_match_attr.pt\"), weights_only=True)\n",
    "print(\"last_fm_match index tensor shape:\", data[\"artist\", \"last_fm_match\", \"artist\"].edge_index.shape)\n",
    "print(\"last_fm_match attr tensor shape:\", data[\"artist\", \"last_fm_match\", \"artist\"].edge_attr.shape)\n",
    "\n",
    "data[\"artist\", \"linked_to\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"linked_to.pt\"), weights_only=True).t().long()\n",
    "data[\"artist\", \"linked_to\", \"artist\"].edge_attr = torch.load(path.join(data_folder, \"linked_to_attr.pt\"), weights_only=True)\n",
    "print(\"linked_to index tensor shape:\", data[\"artist\", \"linked_to\", \"artist\"].edge_index.shape)\n",
    "print(\"linked_to attr tensor shape:\", data[\"artist\", \"linked_to\", \"artist\"].edge_attr.shape)\n",
    "\n",
    "data[\"artist\", \"musically_related_to\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"musically_related_to.pt\"), weights_only=True).t().long()\n",
    "data[\"artist\", \"musically_related_to\", \"artist\"].edge_attr = torch.load(path.join(data_folder, \"musically_related_to_attr.pt\"), weights_only=True)\n",
    "print(\"musically_related_to index tensor shape:\", data[\"artist\", \"musically_related_to\", \"artist\"].edge_index.shape)\n",
    "print(\"musically_related_to attr tensor shape:\", data[\"artist\", \"musically_related_to\", \"artist\"].edge_attr.shape)\n",
    "\n",
    "data[\"artist\", \"personally_related_to\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"personally_related_to.pt\"), weights_only=True).t().long()\n",
    "data[\"artist\", \"personally_related_to\", \"artist\"].edge_attr = torch.load(path.join(data_folder, \"personally_related_to_attr.pt\"), weights_only=True)\n",
    "print(\"personally_related_to index tensor shape:\", data[\"artist\", \"personally_related_to\", \"artist\"].edge_index.shape)\n",
    "print(\"personally_related_to attr tensor shape:\", data[\"artist\", \"personally_related_to\", \"artist\"].edge_attr.shape)\n",
    "\n",
    "data[\"tag\", \"tags_artists\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"tags_artists.pt\"), weights_only=True).t().long()\n",
    "data[\"tag\", \"tags_track\", \"track\"].edge_index = torch.load(path.join(data_folder, \"tags_tracks.pt\"), weights_only=True).t().long()\n",
    "print(\"tags_artists index tensor shape:\", data[\"tag\", \"tags_artists\", \"artist\"].edge_index.shape)\n",
    "print(\"tags_tracks index tensor shape:\", data[\"tag\", \"tags_track\", \"track\"].edge_index.shape)\n",
    "\n",
    "data[\"track\", \"worked_by\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"worked_by.pt\"), weights_only=True).t().long()\n",
    "data[\"artist\", \"worked_in\", \"track\"].edge_index = torch.load(path.join(data_folder, \"worked_in.pt\"), weights_only=True).t().long()\n",
    "print(\"worked_by index tensor shape:\", data[\"track\", \"worked_by\", \"artist\"].edge_index.shape)\n",
    "print(\"worked_in index tensor shape:\", data[\"artist\", \"worked_in\", \"track\"].edge_index.shape)\n",
    "\n",
    "print()\n",
    "\n",
    "data.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a33bd03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge type: ('artist', 'collab_with', 'artist'), edge_index shape: torch.Size([2, 2463052])\n",
      "Edge type: ('artist', 'has_tag_artists', 'tag'), edge_index shape: torch.Size([2, 2410207])\n",
      "Edge type: ('track', 'has_tag_tracks', 'tag'), edge_index shape: torch.Size([2, 4030735])\n",
      "Edge type: ('artist', 'last_fm_match', 'artist'), edge_index shape: torch.Size([2, 154865250])\n",
      "Edge type: ('artist', 'linked_to', 'artist'), edge_index shape: torch.Size([2, 23128])\n",
      "Edge type: ('artist', 'musically_related_to', 'artist'), edge_index shape: torch.Size([2, 373262])\n",
      "Edge type: ('artist', 'personally_related_to', 'artist'), edge_index shape: torch.Size([2, 26720])\n",
      "Edge type: ('tag', 'tags_artists', 'artist'), edge_index shape: torch.Size([2, 2410207])\n",
      "Edge type: ('tag', 'tags_track', 'track'), edge_index shape: torch.Size([2, 4030735])\n",
      "Edge type: ('track', 'worked_by', 'artist'), edge_index shape: torch.Size([2, 27661673])\n",
      "Edge type: ('artist', 'worked_in', 'track'), edge_index shape: torch.Size([2, 27661673])\n",
      "Subgraph artist tensor shape: torch.Size([1489250, 16])\n",
      "Subgraph track tensor shape: torch.Size([24324100, 4])\n",
      "Subgraph tag tensor shape: torch.Size([23, 24])\n",
      "\n",
      "\n",
      "Validation successful.\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL SUBGRAPH\n",
    "\n",
    "edge_types = [\n",
    "    (\"artist\", \"collab_with\", \"artist\"),\n",
    "    (\"artist\", \"has_tag_artists\", \"tag\"),\n",
    "    (\"track\", \"has_tag_tracks\", \"tag\"),\n",
    "    (\"artist\", \"last_fm_match\", \"artist\"),\n",
    "    (\"artist\", \"linked_to\", \"artist\"),\n",
    "    (\"artist\", \"musically_related_to\", \"artist\"),\n",
    "    (\"artist\", \"personally_related_to\", \"artist\"),\n",
    "    (\"tag\", \"tags_artists\", \"artist\"),\n",
    "    (\"tag\", \"tags_track\", \"track\"),\n",
    "    (\"track\", \"worked_by\", \"artist\"),\n",
    "    (\"artist\", \"worked_in\", \"track\")\n",
    "]\n",
    "\n",
    "if False:\n",
    "\n",
    "    # Data\n",
    "    percentile = 0.85\n",
    "    artist_popularity = data[\"artist\"].x[:, 8]\n",
    "    edge_types = [\n",
    "        (\"artist\", \"collab_with\", \"artist\"),\n",
    "        (\"artist\", \"has_tag_artists\", \"tag\"),\n",
    "        (\"track\", \"has_tag_tracks\", \"tag\"),\n",
    "        (\"artist\", \"last_fm_match\", \"artist\"),\n",
    "        (\"artist\", \"linked_to\", \"artist\"),\n",
    "        (\"artist\", \"musically_related_to\", \"artist\"),\n",
    "        (\"artist\", \"personally_related_to\", \"artist\"),\n",
    "        (\"tag\", \"tags_artists\", \"artist\"),\n",
    "        (\"tag\", \"tags_track\", \"track\"),\n",
    "        (\"track\", \"worked_by\", \"artist\"),\n",
    "        (\"artist\", \"worked_in\", \"track\")\n",
    "    ]\n",
    "\n",
    "    # Threshold obtention\n",
    "    threshold = torch.quantile(artist_popularity, percentile)\n",
    "    selected_artists = artist_popularity >= threshold\n",
    "    selected_artist_ids = torch.nonzero(selected_artists).squeeze()\n",
    "\n",
    "    # Mapping\n",
    "    old_to_new_artist_idx = torch.zeros(\n",
    "        data[\"artist\"].x.shape[0],\n",
    "        dtype=torch.long\n",
    "    )\n",
    "\n",
    "    for i, selected_artist_id in enumerate(selected_artist_ids):\n",
    "        old_to_new_artist_idx[selected_artist_id] = i\n",
    "\n",
    "    # Subgraph\n",
    "    for edge_type in edge_types:\n",
    "        print(f\"edge_type: {edge_type}\")\n",
    "        # Filter edge indices\n",
    "        edge_index = data[edge_type].edge_index\n",
    "        mask = torch.ones(edge_index.shape[1], dtype=torch.bool)\n",
    "        if edge_type[0] == \"artist\":\n",
    "            mask &= torch.isin(edge_index[0], selected_artist_ids)\n",
    "        if edge_type[2] == \"artist\":\n",
    "            mask &= torch.isin(edge_index[1], selected_artist_ids)\n",
    "\n",
    "        filtered_edge_index = edge_index[:, mask]\n",
    "\n",
    "        # Map the old indices to new ones for 'artist' nodes\n",
    "        # if edge_type[0] == \"artist\":  # Reindex source node\n",
    "        #     filtered_edge_index[0] = torch.tensor(\n",
    "        #         [old_to_new_artist_idx[idx.item()] for idx in filtered_edge_index[0]],\n",
    "        #         dtype=torch.long,\n",
    "        #     )\n",
    "        # if edge_type[2] == \"artist\":  # Reindex destination node\n",
    "        #     filtered_edge_index[1] = torch.tensor(\n",
    "        #         [old_to_new_artist_idx[idx.item()] for idx in filtered_edge_index[1]],\n",
    "        #         dtype=torch.long,\n",
    "        #     )\n",
    "        if edge_type[0] == \"artist\":  # Reindex source node\n",
    "            filtered_edge_index[0] = old_to_new_artist_idx[filtered_edge_index[0]]\n",
    "        if edge_type[2] == \"artist\":  # Reindex destination node\n",
    "            filtered_edge_index[1] = old_to_new_artist_idx[filtered_edge_index[1]]\n",
    "\n",
    "        # Assign filtered edges to subgraph\n",
    "        data[edge_type].edge_index = filtered_edge_index\n",
    "\n",
    "        # Handle edge attributes if they exist\n",
    "        if hasattr(data[edge_type], \"edge_attr\"):\n",
    "            try:\n",
    "                data[edge_type].edge_attr = data[edge_type].edge_attr[mask]\n",
    "            except IndexError as e:\n",
    "                print(f\"IndexError for {edge_type}: {e}\")\n",
    "        else:\n",
    "            print(f\"No edge_attr for {edge_type}\")\n",
    "\n",
    "    # Nodes filtering\n",
    "    data[\"artist\"].x = data[\"artist\"].x[selected_artist_ids]\n",
    "    # data[\"track\"].x = data[\"track\"].x\n",
    "    # data[\"tag\"].x = data[\"tag\"].x\n",
    "\n",
    "# Check the shape of the filtered (or not) nodes and edges\n",
    "for edge_type in edge_types:\n",
    "    print(f\"Edge type: {edge_type}, edge_index shape: {data[edge_type].edge_index.shape}\")\n",
    "\n",
    "# Check the artist features (should only have the selected artists)\n",
    "print(\"Subgraph artist tensor shape:\", data[\"artist\"].x.shape)\n",
    "print(\"Subgraph track tensor shape:\", data[\"track\"].x.shape)\n",
    "print(\"Subgraph tag tensor shape:\", data[\"tag\"].x.shape)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Validate the subgraph\n",
    "try:\n",
    "    data.validate()\n",
    "    print(\"Validation successful.\")\n",
    "except ValueError as e:\n",
    "    print(\"Validation failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b839629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "==============\n",
      "HeteroData(\n",
      "  artist={ x=[1489250, 16] },\n",
      "  track={ x=[24324100, 4] },\n",
      "  tag={ x=[23, 24] },\n",
      "  (artist, collab_with, artist)={\n",
      "    edge_index=[2, 1206897],\n",
      "    edge_attr=[1206897, 1],\n",
      "    edge_label=[517241],\n",
      "    edge_label_index=[2, 517241],\n",
      "  },\n",
      "  (artist, has_tag_artists, tag)={ edge_index=[2, 2410207] },\n",
      "  (track, has_tag_tracks, tag)={ edge_index=[2, 4030735] },\n",
      "  (artist, last_fm_match, artist)={\n",
      "    edge_index=[2, 154865250],\n",
      "    edge_attr=[154865250, 1],\n",
      "  },\n",
      "  (artist, linked_to, artist)={\n",
      "    edge_index=[2, 23128],\n",
      "    edge_attr=[23128, 1],\n",
      "  },\n",
      "  (artist, musically_related_to, artist)={\n",
      "    edge_index=[2, 373262],\n",
      "    edge_attr=[373262, 1],\n",
      "  },\n",
      "  (artist, personally_related_to, artist)={\n",
      "    edge_index=[2, 26720],\n",
      "    edge_attr=[26720, 1],\n",
      "  },\n",
      "  (tag, tags_artists, artist)={ edge_index=[2, 2410207] },\n",
      "  (tag, tags_track, track)={ edge_index=[2, 4030735] },\n",
      "  (track, worked_by, artist)={ edge_index=[2, 27661673] },\n",
      "  (artist, worked_in, track)={ edge_index=[2, 27661673] }\n",
      ")\n",
      "\n",
      "Validation data:\n",
      "================\n",
      "HeteroData(\n",
      "  artist={ x=[1489250, 16] },\n",
      "  track={ x=[24324100, 4] },\n",
      "  tag={ x=[23, 24] },\n",
      "  (artist, collab_with, artist)={\n",
      "    edge_index=[2, 1724138],\n",
      "    edge_attr=[1724138, 1],\n",
      "    edge_label=[738914],\n",
      "    edge_label_index=[2, 738914],\n",
      "  },\n",
      "  (artist, has_tag_artists, tag)={ edge_index=[2, 2410207] },\n",
      "  (track, has_tag_tracks, tag)={ edge_index=[2, 4030735] },\n",
      "  (artist, last_fm_match, artist)={\n",
      "    edge_index=[2, 154865250],\n",
      "    edge_attr=[154865250, 1],\n",
      "  },\n",
      "  (artist, linked_to, artist)={\n",
      "    edge_index=[2, 23128],\n",
      "    edge_attr=[23128, 1],\n",
      "  },\n",
      "  (artist, musically_related_to, artist)={\n",
      "    edge_index=[2, 373262],\n",
      "    edge_attr=[373262, 1],\n",
      "  },\n",
      "  (artist, personally_related_to, artist)={\n",
      "    edge_index=[2, 26720],\n",
      "    edge_attr=[26720, 1],\n",
      "  },\n",
      "  (tag, tags_artists, artist)={ edge_index=[2, 2410207] },\n",
      "  (tag, tags_track, track)={ edge_index=[2, 4030735] },\n",
      "  (track, worked_by, artist)={ edge_index=[2, 27661673] },\n",
      "  (artist, worked_in, track)={ edge_index=[2, 27661673] }\n",
      ")\n",
      "\n",
      "Test data:\n",
      "================\n",
      "HeteroData(\n",
      "  artist={ x=[1489250, 16] },\n",
      "  track={ x=[24324100, 4] },\n",
      "  tag={ x=[23, 24] },\n",
      "  (artist, collab_with, artist)={\n",
      "    edge_index=[2, 2093595],\n",
      "    edge_attr=[2093595, 1],\n",
      "    edge_label=[738914],\n",
      "    edge_label_index=[2, 738914],\n",
      "  },\n",
      "  (artist, has_tag_artists, tag)={ edge_index=[2, 2410207] },\n",
      "  (track, has_tag_tracks, tag)={ edge_index=[2, 4030735] },\n",
      "  (artist, last_fm_match, artist)={\n",
      "    edge_index=[2, 154865250],\n",
      "    edge_attr=[154865250, 1],\n",
      "  },\n",
      "  (artist, linked_to, artist)={\n",
      "    edge_index=[2, 23128],\n",
      "    edge_attr=[23128, 1],\n",
      "  },\n",
      "  (artist, musically_related_to, artist)={\n",
      "    edge_index=[2, 373262],\n",
      "    edge_attr=[373262, 1],\n",
      "  },\n",
      "  (artist, personally_related_to, artist)={\n",
      "    edge_index=[2, 26720],\n",
      "    edge_attr=[26720, 1],\n",
      "  },\n",
      "  (tag, tags_artists, artist)={ edge_index=[2, 2410207] },\n",
      "  (tag, tags_track, track)={ edge_index=[2, 4030735] },\n",
      "  (track, worked_by, artist)={ edge_index=[2, 27661673] },\n",
      "  (artist, worked_in, track)={ edge_index=[2, 27661673] }\n",
      ")\n",
      "Training edges: 1206897\n",
      "Validation edges: 1724138\n",
      "Test edges: 2093595\n",
      "Train-Val Overlap: 1206897\n",
      "Train-Test Overlap: 1206897\n",
      "Val-Test Overlap: 1724138\n"
     ]
    }
   ],
   "source": [
    "import torch_geometric.transforms as T\n",
    "\n",
    "transform = T.RandomLinkSplit(\n",
    "    num_val=0.15,\n",
    "    num_test=0.15,\n",
    "    disjoint_train_ratio=0.3,\n",
    "    neg_sampling_ratio=1,\n",
    "    add_negative_train_samples=False,\n",
    "    edge_types=(\"artist\", \"collab_with\", \"artist\")\n",
    ")\n",
    "\n",
    "train_data, val_data, test_data = transform(data)\n",
    "\n",
    "print(\"Training data:\")\n",
    "print(\"==============\")\n",
    "print(train_data)\n",
    "print()\n",
    "print(\"Validation data:\")\n",
    "print(\"================\")\n",
    "print(val_data)\n",
    "print()\n",
    "print(\"Test data:\")\n",
    "print(\"================\")\n",
    "print(test_data)\n",
    "\n",
    "print(f\"Training edges: {train_data['artist', 'collab_with', 'artist'].edge_index.shape[1]}\")\n",
    "print(f\"Validation edges: {val_data['artist', 'collab_with', 'artist'].edge_index.shape[1]}\")\n",
    "print(f\"Test edges: {test_data['artist', 'collab_with', 'artist'].edge_index.shape[1]}\")\n",
    "\n",
    "train_edges = set(map(tuple, train_data['artist', 'collab_with', 'artist'].edge_index.T.tolist()))\n",
    "val_edges = set(map(tuple, val_data['artist', 'collab_with', 'artist'].edge_index.T.tolist()))\n",
    "test_edges = set(map(tuple, test_data['artist', 'collab_with', 'artist'].edge_index.T.tolist()))\n",
    "\n",
    "print(f\"Train-Val Overlap: {len(train_edges & val_edges)}\")  # Should be 0\n",
    "print(f\"Train-Test Overlap: {len(train_edges & test_edges)}\")  # Should be 0\n",
    "print(f\"Val-Test Overlap: {len(val_edges & test_edges)}\")  # Should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d596df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get edge index\n",
    "# edge_index = data['artist', 'collab_with', 'artist'].edge_index\n",
    "\n",
    "# # Shuffle edges\n",
    "# num_edges = edge_index.shape[1]\n",
    "# perm = torch.randperm(num_edges)\n",
    "# edge_index = edge_index[:, perm]\n",
    "\n",
    "# # Define sizes\n",
    "# num_test = int(0.15 * num_edges)\n",
    "# num_val = int(0.15 * num_edges)\n",
    "# num_train = num_edges - num_val - num_test\n",
    "\n",
    "# # Split\n",
    "# train_edges = edge_index[:, :num_train]\n",
    "# val_edges = edge_index[:, num_train:num_train + num_val]\n",
    "# test_edges = edge_index[:, num_train + num_val:]\n",
    "\n",
    "# # Verify disjoint sets\n",
    "# train_set = set(map(tuple, train_edges.T.tolist()))\n",
    "# val_set = set(map(tuple, val_edges.T.tolist()))\n",
    "# test_set = set(map(tuple, test_edges.T.tolist()))\n",
    "\n",
    "# print(f\"Train-Val Overlap: {len(train_set & val_set)}\")  # Should be 0\n",
    "# print(f\"Train-Test Overlap: {len(train_set & test_set)}\")  # Should be 0\n",
    "# print(f\"Val-Test Overlap: {len(val_set & test_set)}\")  # Should be 0\n",
    "\n",
    "# # Store the new edge splits in PyG format\n",
    "# train_data = data.clone()\n",
    "# train_data['artist', 'collab_with', 'artist'].edge_index = train_edges\n",
    "\n",
    "# val_data = data.clone()\n",
    "# val_data['artist', 'collab_with', 'artist'].edge_index = val_edges\n",
    "\n",
    "# test_data = data.clone()\n",
    "# test_data['artist', 'collab_with', 'artist'].edge_index = test_edges\n",
    "\n",
    "# print(f\"Training edges: {train_data['artist', 'collab_with', 'artist'].edge_index.shape[1]}\")\n",
    "# print(f\"Validation edges: {val_data['artist', 'collab_with', 'artist'].edge_index.shape[1]}\")\n",
    "# print(f\"Test edges: {test_data['artist', 'collab_with', 'artist'].edge_index.shape[1]}\")\n",
    "\n",
    "# from torch_geometric.utils import negative_sampling\n",
    "\n",
    "# def create_edge_labels(data):\n",
    "#     edge_index = data['artist', 'collab_with', 'artist'].edge_index\n",
    "#     num_nodes = data['artist'].num_nodes\n",
    "\n",
    "#     # Generate negative edges\n",
    "#     neg_edge_index = negative_sampling(\n",
    "#         edge_index=edge_index,\n",
    "#         num_nodes=num_nodes,\n",
    "#         num_neg_samples=edge_index.shape[1]  # 1:1 ratio of positive to negative samples\n",
    "#     )\n",
    "\n",
    "#     # Concatenate positive and negative edges\n",
    "#     edge_label_index = torch.cat([edge_index, neg_edge_index], dim=1)\n",
    "#     edge_label = torch.cat([torch.ones(edge_index.shape[1]), torch.zeros(neg_edge_index.shape[1])], dim=0)\n",
    "\n",
    "#     return edge_label_index, edge_label\n",
    "\n",
    "# # Apply to train, val, test\n",
    "# train_edge_label_index, train_edge_label = create_edge_labels(train_data)\n",
    "# val_edge_label_index, val_edge_label = create_edge_labels(val_data)\n",
    "# test_edge_label_index, test_edge_label = create_edge_labels(test_data)\n",
    "\n",
    "# # Assign to datasets\n",
    "# train_data['artist', 'collab_with', 'artist'].edge_label_index = train_edge_label_index\n",
    "# train_data['artist', 'collab_with', 'artist'].edge_label = train_edge_label\n",
    "\n",
    "# val_data['artist', 'collab_with', 'artist'].edge_label_index = val_edge_label_index\n",
    "# val_data['artist', 'collab_with', 'artist'].edge_label = val_edge_label\n",
    "\n",
    "# test_data['artist', 'collab_with', 'artist'].edge_label_index = test_edge_label_index\n",
    "# test_data['artist', 'collab_with', 'artist'].edge_label = test_edge_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82faa4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: '{device}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "956bf7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train_loader...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleferu/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating val_loader...\n",
      "Sampling mini-batch...\n",
      "Sampled mini-batch:\n",
      "===================\n",
      "HeteroData(\n",
      "  artist={\n",
      "    x=[114962, 16],\n",
      "    n_id=[114962],\n",
      "  },\n",
      "  track={\n",
      "    x=[82950, 4],\n",
      "    n_id=[82950],\n",
      "  },\n",
      "  tag={\n",
      "    x=[23, 24],\n",
      "    n_id=[23],\n",
      "  },\n",
      "  (artist, collab_with, artist)={\n",
      "    edge_index=[2, 25062],\n",
      "    edge_attr=[25062, 1],\n",
      "    edge_label=[128],\n",
      "    edge_label_index=[2, 128],\n",
      "    e_id=[25062],\n",
      "    input_id=[64],\n",
      "  },\n",
      "  (artist, has_tag_artists, tag)={\n",
      "    edge_index=[2, 460],\n",
      "    e_id=[460],\n",
      "  },\n",
      "  (track, has_tag_tracks, tag)={\n",
      "    edge_index=[2, 460],\n",
      "    e_id=[460],\n",
      "  },\n",
      "  (artist, last_fm_match, artist)={\n",
      "    edge_index=[2, 126912],\n",
      "    edge_attr=[126912, 1],\n",
      "    e_id=[126912],\n",
      "  },\n",
      "  (artist, linked_to, artist)={\n",
      "    edge_index=[2, 321],\n",
      "    edge_attr=[321, 1],\n",
      "    e_id=[321],\n",
      "  },\n",
      "  (artist, musically_related_to, artist)={\n",
      "    edge_index=[2, 5293],\n",
      "    edge_attr=[5293, 1],\n",
      "    e_id=[5293],\n",
      "  },\n",
      "  (artist, personally_related_to, artist)={\n",
      "    edge_index=[2, 612],\n",
      "    edge_attr=[612, 1],\n",
      "    e_id=[612],\n",
      "  },\n",
      "  (tag, tags_artists, artist)={\n",
      "    edge_index=[2, 17921],\n",
      "    e_id=[17921],\n",
      "  },\n",
      "  (tag, tags_track, track)={\n",
      "    edge_index=[2, 584],\n",
      "    e_id=[584],\n",
      "  },\n",
      "  (track, worked_by, artist)={\n",
      "    edge_index=[2, 83318],\n",
      "    e_id=[83318],\n",
      "  },\n",
      "  (artist, worked_in, track)={\n",
      "    edge_index=[2, 5511],\n",
      "    e_id=[5511],\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "compt_tree_size = [25, 20]\n",
    "\n",
    "edge_label_index = train_data[\"artist\", \"collab_with\", \"artist\"].edge_label_index\n",
    "edge_label = train_data[\"artist\", \"collab_with\", \"artist\"].edge_label\n",
    "\n",
    "print(\"Creating train_loader...\")\n",
    "train_loader = LinkNeighborLoader(\n",
    "    data=train_data,\n",
    "    num_neighbors=compt_tree_size,\n",
    "    neg_sampling_ratio=1,\n",
    "    edge_label_index=((\"artist\", \"collab_with\", \"artist\"), edge_label_index),\n",
    "    edge_label=edge_label,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "edge_label_index = val_data[\"artist\", \"collab_with\", \"artist\"].edge_label_index\n",
    "edge_label = val_data[\"artist\", \"collab_with\", \"artist\"].edge_label\n",
    "\n",
    "print(\"Creating val_loader...\")\n",
    "val_loader = LinkNeighborLoader(\n",
    "    data=val_data,\n",
    "    num_neighbors=compt_tree_size,\n",
    "    edge_label_index=((\"artist\", \"collab_with\", \"artist\"), edge_label_index),\n",
    "    edge_label=edge_label,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(\"Sampling mini-batch...\")\n",
    "\n",
    "sampled_data = next(iter(train_loader))\n",
    "\n",
    "print(\"Sampled mini-batch:\")\n",
    "print(\"===================\")\n",
    "print(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13d3164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "if debug:\n",
    "    print(torch.unique(train_data['artist', 'collab_with', 'artist'].edge_label))\n",
    "    print(torch.unique(next(iter(train_loader))[\"artist\", \"collab_with\", \"artist\"].edge_label))\n",
    "    print(torch.unique(val_data['artist', 'collab_with', 'artist'].edge_label))\n",
    "    print(torch.unique(next(iter(val_loader))[\"artist\", \"collab_with\", \"artist\"].edge_label))\n",
    "    print(torch.unique(test_data['artist', 'collab_with', 'artist'].edge_label))\n",
    "    print(torch.unique(next(iter(test_loader))[\"artist\", \"collab_with\", \"artist\"].edge_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91d598ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import HeteroConv, GATConv, SAGEConv, Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, metadata, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.metadata = metadata\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.conv1 = HeteroConv({\n",
    "            (\"artist\", \"collab_with\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"has_tag_artists\", \"tag\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"last_fm_match\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"track\", \"has_tag_tracks\", \"tag\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"linked_to\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"musically_related_to\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"personally_related_to\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"tag\", \"tags_artists\", \"artist\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"tag\", \"tags_tracks\", \"track\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"track\", \"worked_by\", \"artist\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"worked_in\", \"track\"): SAGEConv((-1, -1), hidden_channels),\n",
    "        }, aggr=\"mean\")\n",
    "\n",
    "        self.conv2 = HeteroConv({\n",
    "            (\"artist\", \"collab_with\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"has_tag_artists\", \"tag\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"last_fm_match\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"track\", \"has_tag_tracks\", \"tag\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"linked_to\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"musically_related_to\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"personally_related_to\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"tag\", \"tags_artists\", \"artist\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"tag\", \"tags_tracks\", \"track\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"track\", \"worked_by\", \"artist\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"worked_in\", \"track\"): SAGEConv((-1, -1), hidden_channels),\n",
    "        }, aggr=\"mean\")\n",
    "\n",
    "        self.linear1 = Linear(hidden_channels * 2, hidden_channels * 4)\n",
    "        self.linear2 = Linear(hidden_channels * 4, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict1 = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict2 = self.conv2(x_dict1, edge_index_dict)\n",
    "\n",
    "        x_artist = torch.cat([x_dict1['artist'], x_dict2['artist']], dim=-1)\n",
    "\n",
    "        x_artist = self.linear1(x_artist)\n",
    "        x_artist = self.linear2(x_artist)\n",
    "\n",
    "        # Normalize the artist node features\n",
    "        x_artist = F.normalize(x_artist, p=2, dim=-1)\n",
    "\n",
    "        # Update the dictionary with the new 'artist' features, leaving other nodes unchanged\n",
    "        x_dict['artist'] = x_artist\n",
    "\n",
    "        return x_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9525bb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for sampled_data in tqdm.tqdm(train_loader):\n",
    "            # Move data to device\n",
    "            sampled_data = sampled_data.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_dict = model(sampled_data.x_dict, sampled_data.edge_index_dict)\n",
    "            \n",
    "            # Get predictions and labels for the 'collab_with' edge type\n",
    "            edge_label_index = sampled_data['artist', 'collab_with', 'artist'].edge_label_index\n",
    "            edge_label = sampled_data['artist', 'collab_with', 'artist'].edge_label\n",
    "\n",
    "            src_emb = pred_dict['artist'][edge_label_index[0]]  # Source node embeddings\n",
    "            dst_emb = pred_dict['artist'][edge_label_index[1]]  # Destination node embeddings\n",
    "            \n",
    "            # Compute the dot product between source and destination embeddings\n",
    "            preds = (src_emb * dst_emb).sum(dim=-1)  # Scalar for each edge\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(preds, edge_label.float())\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Average loss for the epoch\n",
    "        epoch_loss /= len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # Validation metrics\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient computation for validation\n",
    "            for sampled_data in tqdm.tqdm(val_loader):\n",
    "                # Move data to device\n",
    "                sampled_data = sampled_data.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                pred_dict = model(sampled_data.x_dict, sampled_data.edge_index_dict)\n",
    "                \n",
    "                # Get predictions and labels for the 'collab_with' edge type\n",
    "                edge_label_index = sampled_data['artist', 'collab_with', 'artist'].edge_label_index\n",
    "                edge_label = sampled_data['artist', 'collab_with', 'artist'].edge_label\n",
    "\n",
    "                src_emb = pred_dict['artist'][edge_label_index[0]]  # Source node embeddings\n",
    "                dst_emb = pred_dict['artist'][edge_label_index[1]]  # Destination node embeddings\n",
    "                \n",
    "                # Compute the dot product between source and destination embeddings\n",
    "                preds = (src_emb * dst_emb).sum(dim=-1)  # Scalar for each edge\n",
    "\n",
    "                probs = torch.sigmoid(preds)  # Convert to probabilities\n",
    "\n",
    "                loss = criterion(preds, edge_label.float())\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Collect predictions, probabilities, and labels\n",
    "                all_labels.append(edge_label.cpu())\n",
    "                all_probs.append(probs.cpu())\n",
    "        \n",
    "        # Concatenate all predictions and labels\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        all_probs = torch.cat(all_probs)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Find threshold for predictions\n",
    "        best_threshold = 0\n",
    "        best_f1 = 0\n",
    "        for threshold in np.arange(0.2, 0.81, 0.01):\n",
    "            preds_binary = (all_probs > threshold).long()\n",
    "            cm = confusion_matrix(all_labels, preds_binary)\n",
    "            tp = cm[1, 1]\n",
    "            fp = cm[0, 1]\n",
    "            fn = cm[1, 0]\n",
    "            tn = cm[0, 0]\n",
    "            precision = 0 if tp == 0 else tp / (tp + fp)\n",
    "            recall = 0 if tp == 0 else tp / (tp + fn)\n",
    "            f1 = 0 if precision * recall == 0 else 2 * precision * recall / (precision + recall)\n",
    "            if f1 > best_f1:\n",
    "                best_threshold = threshold\n",
    "                best_f1 = f1\n",
    "        print(f\"Best threshold: {best_threshold}\")\n",
    "        all_preds = (all_probs > best_threshold).long()\n",
    "        \n",
    "        # Compute metrics\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        tp = cm[1, 1]\n",
    "        fp = cm[0, 1]\n",
    "        fn = cm[1, 0]\n",
    "        tn = cm[0, 0]\n",
    "        accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        roc_auc = roc_auc_score(all_labels, all_probs)\n",
    "        \n",
    "        # Print validation metrics\n",
    "        print(f\"Validation Metrics - Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"Loss:      {val_loss:.4f}\")\n",
    "        print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall:    {recall:.4f}\")\n",
    "        print(f\"F1-score:  {f1:.4f}\")\n",
    "        print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{tp} {fn}\\n{fp} {tn}\")\n",
    "\n",
    "    return best_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13d6278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion, device, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for sampled_data in tqdm.tqdm(test_loader):\n",
    "            # Move data to the device\n",
    "            sampled_data = sampled_data.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            pred_dict = model(sampled_data.x_dict, sampled_data.edge_index_dict)\n",
    "\n",
    "            # Get predictions and labels for the 'collab_with' edge type\n",
    "            edge_label_index = sampled_data['artist', 'collab_with', 'artist'].edge_label_index\n",
    "            edge_label = sampled_data['artist', 'collab_with', 'artist'].edge_label\n",
    "\n",
    "            src_emb = pred_dict['artist'][edge_label_index[0]]  # Source node embeddings\n",
    "            dst_emb = pred_dict['artist'][edge_label_index[1]]  # Destination node embeddings\n",
    "            \n",
    "            # Compute the dot product between source and destination embeddings\n",
    "            preds = (src_emb * dst_emb).sum(dim=-1)  # Scalar for each edge\n",
    "            probs = torch.sigmoid(preds)  # Convert logits to probabilities\n",
    "            preds_binary = (probs > threshold).long()  # Convert probabilities to binary predictions\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(preds, edge_label.float())\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Collect predictions and labels\n",
    "            all_preds.append(preds_binary.cpu())\n",
    "            all_labels.append(edge_label.cpu())\n",
    "            all_probs.append(probs.cpu())\n",
    "\n",
    "    # Concatenate all predictions and labels\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    all_probs = torch.cat(all_probs)\n",
    "\n",
    "    # Compute metrics\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    tp = cm[1, 1]\n",
    "    fp = cm[0, 1]\n",
    "    fn = cm[1, 0]\n",
    "    tn = cm[0, 0]\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    roc_auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    # Average test loss\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    print(\"Test Results:\")\n",
    "    print(f\"Loss:      {test_loss:.4f}\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-score:  {f1:.4f}\")\n",
    "    print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{tp} {fn}\\n{fp} {tn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c71d770d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8082/8082 [11:01<00:00, 12.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 0.5765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11546/11546 [06:21<00:00, 30.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.6400000000000003\n",
      "Validation Metrics - Epoch 1/20:\n",
      "Loss:      0.5699\n",
      "Accuracy:  0.8134\n",
      "Precision: 0.7931\n",
      "Recall:    0.8480\n",
      "F1-score:  0.8196\n",
      "ROC-AUC:   0.8393\n",
      "Confusion Matrix:\n",
      "313310 56147\n",
      "81736 287721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 908/8082 [01:14<09:45, 12.25it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m GNN(metadata\u001b[38;5;241m=\u001b[39mtrain_data\u001b[38;5;241m.\u001b[39mmetadata(), hidden_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m best_threshold \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m sampled_data \u001b[38;5;241m=\u001b[39m sampled_data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m pred_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampled_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampled_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Get predictions and labels for the 'collab_with' edge type\u001b[39;00m\n\u001b[1;32m     18\u001b[0m edge_label_index \u001b[38;5;241m=\u001b[39m sampled_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124martist\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcollab_with\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124martist\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39medge_label_index\n",
      "File \u001b[0;32m~/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 43\u001b[0m, in \u001b[0;36mGNN.forward\u001b[0;34m(self, x_dict, edge_index_dict)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_dict, edge_index_dict):\n\u001b[1;32m     42\u001b[0m     x_dict1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x_dict, edge_index_dict)\n\u001b[0;32m---> 43\u001b[0m     x_dict2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_dict1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     x_artist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x_dict1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124martist\u001b[39m\u001b[38;5;124m'\u001b[39m], x_dict2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124martist\u001b[39m\u001b[38;5;124m'\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     47\u001b[0m     x_artist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(x_artist)\n",
      "File \u001b[0;32m~/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch_geometric/nn/conv/hetero_conv.py:158\u001b[0m, in \u001b[0;36mHeteroConv.forward\u001b[0;34m(self, *args_dict, **kwargs_dict)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_edge_level_arg:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dst \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m out_dict:\n\u001b[1;32m    161\u001b[0m     out_dict[dst] \u001b[38;5;241m=\u001b[39m [out]\n",
      "File \u001b[0;32m~/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch_geometric/nn/conv/gat_conv.py:362\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    357\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe usage of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_attr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd_self_loops\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    358\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimultaneously is currently not yet supported for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    359\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseTensor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m form\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    361\u001b[0m \u001b[38;5;66;03m# edge_updater_type: (alpha: OptPairTensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 362\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_updater\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m                          \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor, alpha: Tensor)\u001b[39;00m\n\u001b[1;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39mx, alpha\u001b[38;5;241m=\u001b[39malpha, size\u001b[38;5;241m=\u001b[39msize)\n",
      "File \u001b[0;32m/tmp/torch_geometric.nn.conv.gat_conv_GATConv_edge_updater_epp_014o.py:176\u001b[0m, in \u001b[0;36medge_updater\u001b[0;34m(self, edge_index, alpha, edge_attr, size)\u001b[0m\n\u001b[1;32m    166\u001b[0m             kwargs \u001b[38;5;241m=\u001b[39m CollectArgs(\n\u001b[1;32m    167\u001b[0m                 alpha_j\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha_j\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    168\u001b[0m                 alpha_i\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha_i\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 dim_size\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    173\u001b[0m             )\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# End Edge Update Forward Pre Hook #########################################\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha_j\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha_j\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha_i\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# Begin Edge Update Forward Hook ###########################################\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[0;32m~/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch_geometric/nn/conv/gat_conv.py:409\u001b[0m, in \u001b[0;36mGATConv.edge_update\u001b[0;34m(self, alpha_j, alpha_i, edge_attr, index, ptr, dim_size)\u001b[0m\n\u001b[1;32m    406\u001b[0m     alpha \u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m+\u001b[39m alpha_edge\n\u001b[1;32m    408\u001b[0m alpha \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(alpha, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnegative_slope)\n\u001b[0;32m--> 409\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m alpha \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(alpha, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m alpha\n",
      "File \u001b[0;32m~/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch_geometric/utils/_softmax.py:77\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(src, index, ptr, num_nodes, dim)\u001b[0m\n\u001b[1;32m     75\u001b[0m src_max \u001b[38;5;241m=\u001b[39m scatter(src\u001b[38;5;241m.\u001b[39mdetach(), index, dim, dim_size\u001b[38;5;241m=\u001b[39mN, reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     76\u001b[0m out \u001b[38;5;241m=\u001b[39m src \u001b[38;5;241m-\u001b[39m src_max\u001b[38;5;241m.\u001b[39mindex_select(dim, index)\n\u001b[0;32m---> 77\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m out_sum \u001b[38;5;241m=\u001b[39m scatter(out, index, dim, dim_size\u001b[38;5;241m=\u001b[39mN, reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-16\u001b[39m\n\u001b[1;32m     79\u001b[0m out_sum \u001b[38;5;241m=\u001b[39m out_sum\u001b[38;5;241m.\u001b[39mindex_select(dim, index)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = GNN(metadata=train_data.metadata(), hidden_channels=64, out_channels=64).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "best_threshold = train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    F.binary_cross_entropy_with_logits,\n",
    "    device,\n",
    "    20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab218838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_threshold = train(\n",
    "#     model,\n",
    "#     train_loader,\n",
    "#     val_loader,\n",
    "#     optimizer,\n",
    "#     F.binary_cross_entropy_with_logits,\n",
    "#     device,\n",
    "#     20\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4561c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:28<00:00, 21.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Loss:      0.5740\n",
      "Accuracy:  0.8027\n",
      "Precision: 0.7512\n",
      "Recall:    0.9052\n",
      "F1-score:  0.8210\n",
      "ROC-AUC:   0.8481\n",
      "Confusion Matrix:\n",
      "142413 14922\n",
      "47159 110176\n"
     ]
    }
   ],
   "source": [
    "test_model(\n",
    "    model,\n",
    "    test_loader,\n",
    "    F.binary_cross_entropy_with_logits,\n",
    "    device,\n",
    "    best_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ebb80aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./normal-nolfm16.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b44cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_label_index = test_data[\"artist\", \"collab_with\", \"artist\"].edge_label_index\n",
    "edge_label = test_data[\"artist\", \"collab_with\", \"artist\"].edge_label\n",
    "\n",
    "print(\"Creating test_loader...\")\n",
    "test_loader = LinkNeighborLoader(\n",
    "    data=test_data,\n",
    "    num_neighbors=compt_tree_size,\n",
    "    edge_label_index=((\"artist\", \"collab_with\", \"artist\"), edge_label_index),\n",
    "    edge_label=edge_label,\n",
    "    batch_size=512,\n",
    "    shuffle=False,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d13ec911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108036/3795534198.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test.load_state_dict(torch.load(\"./normal-nolfm.pth\"))\n",
      "100%|██████████| 615/615 [00:29<00:00, 21.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Loss:      0.5740\n",
      "Accuracy:  0.8026\n",
      "Precision: 0.7512\n",
      "Recall:    0.9051\n",
      "F1-score:  0.8210\n",
      "ROC-AUC:   0.8483\n",
      "Confusion Matrix:\n",
      "142398 14937\n",
      "47169 110166\n"
     ]
    }
   ],
   "source": [
    "test = GNN(metadata=train_data.metadata(), out_channels=64).to(device)\n",
    "test.load_state_dict(torch.load(\"./normal-nolfm16.pth\"))\n",
    "test_model(\n",
    "    test,\n",
    "    test_loader,\n",
    "    F.binary_cross_entropy_with_logits,\n",
    "    device,\n",
    "    best_threshold\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicbrainz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
