{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31d33376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "import os.path as path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e12322ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"ds/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdb5b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = HeteroData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1192f48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artist tensor shape: torch.Size([1489250, 16])\n",
      "Track tensor shape: torch.Size([24324100, 4])\n",
      "Tag tensor shape: torch.Size([23, 24])\n",
      "collab_with index tensor shape: torch.Size([2, 2463052])\n",
      "collab_with attr tensor shape: torch.Size([2463052, 1])\n",
      "has_tag_artists index tensor shape: torch.Size([2, 2410207])\n",
      "has_tag_tracks index tensor shape: torch.Size([2, 4030735])\n",
      "last_fm_match index tensor shape: torch.Size([2, 154865250])\n",
      "last_fm_match attr tensor shape: torch.Size([154865250, 1])\n",
      "linked_to index tensor shape: torch.Size([2, 23128])\n",
      "linked_to attr tensor shape: torch.Size([23128, 1])\n",
      "musically_related_to index tensor shape: torch.Size([2, 373262])\n",
      "musically_related_to attr tensor shape: torch.Size([373262, 1])\n",
      "personally_related_to index tensor shape: torch.Size([2, 26720])\n",
      "personally_related_to attr tensor shape: torch.Size([26720, 1])\n",
      "tags_artists index tensor shape: torch.Size([2, 2410207])\n",
      "tags_tracks index tensor shape: torch.Size([2, 4030735])\n",
      "worked_by index tensor shape: torch.Size([2, 27661673])\n",
      "worked_in index tensor shape: torch.Size([2, 27661673])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"artist\"].x = torch.load(path.join(data_folder, \"artists.pt\"), weights_only=True)\n",
    "print(\"Artist tensor shape:\", data[\"artist\"].x.shape)\n",
    "\n",
    "data[\"track\"].x = torch.load(path.join(data_folder, \"tracks.pt\"), weights_only=True)\n",
    "print(\"Track tensor shape:\", data[\"track\"].x.shape)\n",
    "\n",
    "data[\"tag\"].x = torch.load(path.join(data_folder, \"tags.pt\"), weights_only=True)\n",
    "print(\"Tag tensor shape:\", data[\"tag\"].x.shape)\n",
    "\n",
    "\n",
    "data[\"artist\", \"collab_with\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"collab_with.pt\"), weights_only=True)\n",
    "data[\"artist\", \"collab_with\", \"artist\"].edge_attr = torch.load(path.join(data_folder, \"collab_with_attr.pt\"), weights_only=True)\n",
    "print(\"collab_with index tensor shape:\", data[\"artist\", \"collab_with\", \"artist\"].edge_index.shape)\n",
    "print(\"collab_with attr tensor shape:\", data[\"artist\", \"collab_with\", \"artist\"].edge_attr.shape)\n",
    "\n",
    "data[\"artist\", \"has_tag_artists\", \"tag\"].edge_index = torch.load(path.join(data_folder, \"has_tag_artists.pt\"), weights_only=True)\n",
    "data[\"track\", \"has_tag_tracks\", \"tag\"].edge_index = torch.load(path.join(data_folder, \"has_tag_tracks.pt\"), weights_only=True)\n",
    "print(\"has_tag_artists index tensor shape:\", data[\"artist\", \"has_tag_artists\", \"tag\"].edge_index.shape)\n",
    "print(\"has_tag_tracks index tensor shape:\", data[\"track\", \"has_tag_tracks\", \"tag\"].edge_index.shape)\n",
    "\n",
    "data[\"artist\", \"last_fm_match\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"last_fm_match.pt\"), weights_only=True)\n",
    "data[\"artist\", \"last_fm_match\", \"artist\"].edge_attr = torch.load(path.join(data_folder, \"last_fm_match_attr.pt\"), weights_only=True)\n",
    "print(\"last_fm_match index tensor shape:\", data[\"artist\", \"last_fm_match\", \"artist\"].edge_index.shape)\n",
    "print(\"last_fm_match attr tensor shape:\", data[\"artist\", \"last_fm_match\", \"artist\"].edge_attr.shape)\n",
    "\n",
    "data[\"artist\", \"linked_to\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"linked_to.pt\"), weights_only=True)\n",
    "data[\"artist\", \"linked_to\", \"artist\"].edge_attr = torch.load(path.join(data_folder, \"linked_to_attr.pt\"), weights_only=True)\n",
    "print(\"linked_to index tensor shape:\", data[\"artist\", \"linked_to\", \"artist\"].edge_index.shape)\n",
    "print(\"linked_to attr tensor shape:\", data[\"artist\", \"linked_to\", \"artist\"].edge_attr.shape)\n",
    "\n",
    "data[\"artist\", \"musically_related_to\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"musically_related_to.pt\"), weights_only=True)\n",
    "data[\"artist\", \"musically_related_to\", \"artist\"].edge_attr = torch.load(path.join(data_folder, \"musically_related_to_attr.pt\"), weights_only=True)\n",
    "print(\"musically_related_to index tensor shape:\", data[\"artist\", \"musically_related_to\", \"artist\"].edge_index.shape)\n",
    "print(\"musically_related_to attr tensor shape:\", data[\"artist\", \"musically_related_to\", \"artist\"].edge_attr.shape)\n",
    "\n",
    "data[\"artist\", \"personally_related_to\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"personally_related_to.pt\"), weights_only=True)\n",
    "data[\"artist\", \"personally_related_to\", \"artist\"].edge_attr = torch.load(path.join(data_folder, \"personally_related_to_attr.pt\"), weights_only=True)\n",
    "print(\"personally_related_to index tensor shape:\", data[\"artist\", \"personally_related_to\", \"artist\"].edge_index.shape)\n",
    "print(\"personally_related_to attr tensor shape:\", data[\"artist\", \"personally_related_to\", \"artist\"].edge_attr.shape)\n",
    "\n",
    "data[\"tag\", \"tags_artists\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"tags_artists.pt\"), weights_only=True)\n",
    "data[\"tag\", \"tags_track\", \"track\"].edge_index = torch.load(path.join(data_folder, \"tags_tracks.pt\"), weights_only=True)\n",
    "print(\"tags_artists index tensor shape:\", data[\"tag\", \"tags_artists\", \"artist\"].edge_index.shape)\n",
    "print(\"tags_tracks index tensor shape:\", data[\"tag\", \"tags_track\", \"track\"].edge_index.shape)\n",
    "\n",
    "data[\"track\", \"worked_by\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"worked_by.pt\"), weights_only=True)\n",
    "data[\"artist\", \"worked_in\", \"track\"].edge_index = torch.load(path.join(data_folder, \"worked_in.pt\"), weights_only=True)\n",
    "print(\"worked_by index tensor shape:\", data[\"track\", \"worked_by\", \"artist\"].edge_index.shape)\n",
    "print(\"worked_in index tensor shape:\", data[\"artist\", \"worked_in\", \"track\"].edge_index.shape)\n",
    "\n",
    "print()\n",
    "\n",
    "data.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a33bd03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge type: ('artist', 'collab_with', 'artist'), edge_index shape: torch.Size([2, 2463052])\n",
      "Edge type: ('artist', 'has_tag_artists', 'tag'), edge_index shape: torch.Size([2, 2410207])\n",
      "Edge type: ('track', 'has_tag_tracks', 'tag'), edge_index shape: torch.Size([2, 4030735])\n",
      "Edge type: ('artist', 'last_fm_match', 'artist'), edge_index shape: torch.Size([2, 154865250])\n",
      "Edge type: ('artist', 'linked_to', 'artist'), edge_index shape: torch.Size([2, 23128])\n",
      "Edge type: ('artist', 'musically_related_to', 'artist'), edge_index shape: torch.Size([2, 373262])\n",
      "Edge type: ('artist', 'personally_related_to', 'artist'), edge_index shape: torch.Size([2, 26720])\n",
      "Edge type: ('tag', 'tags_artists', 'artist'), edge_index shape: torch.Size([2, 2410207])\n",
      "Edge type: ('tag', 'tags_track', 'track'), edge_index shape: torch.Size([2, 4030735])\n",
      "Edge type: ('track', 'worked_by', 'artist'), edge_index shape: torch.Size([2, 27661673])\n",
      "Edge type: ('artist', 'worked_in', 'track'), edge_index shape: torch.Size([2, 27661673])\n",
      "Subgraph artist tensor shape: torch.Size([1489250, 16])\n",
      "Subgraph track tensor shape: torch.Size([24324100, 4])\n",
      "Subgraph tag tensor shape: torch.Size([23, 24])\n",
      "\n",
      "\n",
      "Validation successful.\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL SUBGRAPH\n",
    "\n",
    "edge_types = [\n",
    "    (\"artist\", \"collab_with\", \"artist\"),\n",
    "    (\"artist\", \"has_tag_artists\", \"tag\"),\n",
    "    (\"track\", \"has_tag_tracks\", \"tag\"),\n",
    "    (\"artist\", \"last_fm_match\", \"artist\"),\n",
    "    (\"artist\", \"linked_to\", \"artist\"),\n",
    "    (\"artist\", \"musically_related_to\", \"artist\"),\n",
    "    (\"artist\", \"personally_related_to\", \"artist\"),\n",
    "    (\"tag\", \"tags_artists\", \"artist\"),\n",
    "    (\"tag\", \"tags_track\", \"track\"),\n",
    "    (\"track\", \"worked_by\", \"artist\"),\n",
    "    (\"artist\", \"worked_in\", \"track\")\n",
    "]\n",
    "\n",
    "if False:\n",
    "\n",
    "    # Data\n",
    "    percentile = 0.85\n",
    "    artist_popularity = data[\"artist\"].x[:, 8]\n",
    "\n",
    "    # Threshold obtention\n",
    "    threshold = torch.quantile(artist_popularity, percentile)\n",
    "    selected_artists = artist_popularity >= threshold\n",
    "    selected_artist_ids = torch.nonzero(selected_artists).squeeze()\n",
    "\n",
    "    # Mapping\n",
    "    old_to_new_artist_idx = torch.zeros(\n",
    "        data[\"artist\"].x.shape[0],\n",
    "        dtype=torch.long\n",
    "    )\n",
    "\n",
    "    for i, selected_artist_id in enumerate(selected_artist_ids):\n",
    "        old_to_new_artist_idx[selected_artist_id] = i\n",
    "\n",
    "    # Subgraph\n",
    "    for edge_type in edge_types:\n",
    "        print(f\"edge_type: {edge_type}\")\n",
    "        # Filter edge indices\n",
    "        edge_index = data[edge_type].edge_index\n",
    "        mask = torch.ones(edge_index.shape[1], dtype=torch.bool)\n",
    "        if edge_type[0] == \"artist\":\n",
    "            mask &= torch.isin(edge_index[0], selected_artist_ids)\n",
    "        if edge_type[2] == \"artist\":\n",
    "            mask &= torch.isin(edge_index[1], selected_artist_ids)\n",
    "\n",
    "        filtered_edge_index = edge_index[:, mask]\n",
    "\n",
    "        # Map the old indices to new ones for 'artist' nodes\n",
    "        # if edge_type[0] == \"artist\":  # Reindex source node\n",
    "        #     filtered_edge_index[0] = torch.tensor(\n",
    "        #         [old_to_new_artist_idx[idx.item()] for idx in filtered_edge_index[0]],\n",
    "        #         dtype=torch.long,\n",
    "        #     )\n",
    "        # if edge_type[2] == \"artist\":  # Reindex destination node\n",
    "        #     filtered_edge_index[1] = torch.tensor(\n",
    "        #         [old_to_new_artist_idx[idx.item()] for idx in filtered_edge_index[1]],\n",
    "        #         dtype=torch.long,\n",
    "        #     )\n",
    "        if edge_type[0] == \"artist\":  # Reindex source node\n",
    "            filtered_edge_index[0] = old_to_new_artist_idx[filtered_edge_index[0]]\n",
    "        if edge_type[2] == \"artist\":  # Reindex destination node\n",
    "            filtered_edge_index[1] = old_to_new_artist_idx[filtered_edge_index[1]]\n",
    "\n",
    "        # Assign filtered edges to subgraph\n",
    "        data[edge_type].edge_index = filtered_edge_index\n",
    "\n",
    "        # Handle edge attributes if they exist\n",
    "        if hasattr(data[edge_type], \"edge_attr\"):\n",
    "            try:\n",
    "                data[edge_type].edge_attr = data[edge_type].edge_attr[mask]\n",
    "            except IndexError as e:\n",
    "                print(f\"IndexError for {edge_type}: {e}\")\n",
    "        else:\n",
    "            print(f\"No edge_attr for {edge_type}\")\n",
    "\n",
    "    # Nodes filtering\n",
    "    data[\"artist\"].x = data[\"artist\"].x[selected_artist_ids]\n",
    "    # data[\"track\"].x = data[\"track\"].x\n",
    "    # data[\"tag\"].x = data[\"tag\"].x\n",
    "\n",
    "# Check the shape of the filtered (or not) nodes and edges\n",
    "for edge_type in edge_types:\n",
    "    print(f\"Edge type: {edge_type}, edge_index shape: {data[edge_type].edge_index.shape}\")\n",
    "\n",
    "# Check the artist features (should only have the selected artists)\n",
    "print(\"Subgraph artist tensor shape:\", data[\"artist\"].x.shape)\n",
    "print(\"Subgraph track tensor shape:\", data[\"track\"].x.shape)\n",
    "print(\"Subgraph tag tensor shape:\", data[\"tag\"].x.shape)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Validate the subgraph\n",
    "try:\n",
    "    data.validate()\n",
    "    print(\"Validation successful.\")\n",
    "except ValueError as e:\n",
    "    print(\"Validation failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c957d60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>track_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>[0, 195, 350, 366, 458, 749, 1014, 1352, 1552,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>6</td>\n",
       "      <td>[1, 4005, 4028, 4935, 9400, 9504, 9717, 12368,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>[2, 199, 551, 670, 1136, 1300, 1519, 2253, 242...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997</td>\n",
       "      <td>13</td>\n",
       "      <td>[3, 42, 68, 136, 418, 438, 541, 543, 619, 662,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010</td>\n",
       "      <td>9</td>\n",
       "      <td>[4, 348, 353, 578, 1532, 2345, 2358, 2479, 252...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month                                          track_ids\n",
       "0  2016     12  [0, 195, 350, 366, 458, 749, 1014, 1352, 1552,...\n",
       "1  2000      6  [1, 4005, 4028, 4935, 9400, 9504, 9717, 12368,...\n",
       "2  2013      4  [2, 199, 551, 670, 1136, 1300, 1519, 2253, 242...\n",
       "3  1997     13  [3, 42, 68, 136, 418, 438, 541, 543, 619, 662,...\n",
       "4  2010      9  [4, 348, 353, 578, 1532, 2345, 2358, 2479, 252..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Track list\n",
    "cut_year = 2020\n",
    "cut_month = 3\n",
    "\n",
    "df = pd.read_csv(\"../data/year_month_track.csv\")\n",
    "df[\"track_ids\"] = df.track_ids.apply(eval)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "258c0f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df[\"year\"] < cut_year) | ((df[\"year\"] == cut_year) & (df[\"month\"] < cut_month))\n",
    "train_tracks_neo4j = df[mask][\"track_ids\"].explode().unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab9577bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(path.join(data_folder, \"track_map.pkl\"), \"rb\") as in_file:\n",
    "    track_map = pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce1061bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tracks_pyg = [track_map[track_id] for track_id in train_tracks_neo4j]\n",
    "train_tracks_pyg_t = torch.tensor(train_tracks_pyg)\n",
    "train_artists_pyg = data[\"artist\", \"worked_in\", \"track\"].edge_index[0, :][\n",
    "    torch.isin(data[\"artist\", \"worked_in\", \"track\"].edge_index[1, :], train_tracks_pyg_t)\n",
    "]\n",
    "set_tracks_pyg = list(\n",
    "    set(range(data[\"track\"].x.shape[0])) - set(train_tracks_pyg)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75e8cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.subgraph({\n",
    "    \"track\": train_tracks_pyg_t,\n",
    "    \"artist\": train_artists_pyg\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27c63d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing unique artists in collab_with\n",
      "Building dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/477152 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'EdgeStorage' object has no attribute 'get_neighbors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# worked_in_edge_index[1, worked_in_edge_index[0, :] == artist]\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m artist \u001b[38;5;129;01min\u001b[39;00m tqdm(unique_artists):\n\u001b[0;32m---> 14\u001b[0m     artist_tracks_dict[artist\u001b[38;5;241m.\u001b[39mitem()] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43martist\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mworked_in\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrack\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_neighbors\u001b[49m(artist)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Collect the new collaboration edges\u001b[39;00m\n\u001b[1;32m     17\u001b[0m new_collab_with_edge_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n",
      "File \u001b[0;32m~/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch_geometric/data/storage.py:96\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EdgeStorage' object has no attribute 'get_neighbors'"
     ]
    }
   ],
   "source": [
    "collab_with_edge_index = train_data[\"artist\", \"collab_with\", \"artist\"].edge_index[:, ::2]\n",
    "collab_with_edge_attr = train_data[\"artist\", \"collab_with\", \"artist\"].edge_attr[::2]\n",
    "n_collabs = collab_with_edge_attr.shape[0]\n",
    "worked_in_edge_index = train_data[\"artist\", \"worked_in\", \"track\"].edge_index\n",
    "\n",
    "# Prepare a dictionary to quickly map artists to their tracks\n",
    "print(\"Computing unique artists in collab_with\")\n",
    "unique_artists = torch.unique(torch.cat((collab_with_edge_index[0, :], collab_with_edge_index[1, :])))\n",
    "\n",
    "print(\"Building dict...\")\n",
    "artist_tracks_dict = {}\n",
    "# TODO: OPTIMIZE THIS\n",
    "# FIXME\n",
    "for artist in tqdm(unique_artists):\n",
    "    artist_tracks_dict[artist.item()] = worked_in_edge_index[1, worked_in_edge_index[0, :] == artist]\n",
    "\n",
    "# Collect the new collaboration edges\n",
    "new_collab_with_edge_index = list()\n",
    "new_collab_with_edge_attr = list()\n",
    "print(\"Building lists...\")\n",
    "for i, (a0, a1) in enumerate(zip(collab_with_edge_index[0, :], collab_with_edge_index[1, :])):\n",
    "    print(f\"{i + 1} out of {n_collabs}\")\n",
    "    a0_item = a0.item()\n",
    "    a1_item = a1.item()\n",
    "    intersection_len = len(np.intersect1d(artist_tracks_dict[a0_item], artist_tracks_dict[a1_item]))\n",
    "    if intersection_len > 0:\n",
    "        new_collab_with_edge_index.append((a0_item, a1_item))\n",
    "        new_collab_with_edge_index.append((a1_item, a0_item))\n",
    "        new_collab_with_edge_attr.extend([intersection_len, intersection_len])\n",
    "\n",
    "train_data[\"artist\", \"collab_with\", \"artist\"].edge_index = new_collab_with_edge_index\n",
    "train_data[\"artist\", \"collab_with\", \"artist\"].edge_attr = new_collab_with_edge_attr\n",
    "\n",
    "train_data.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b839629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "==============\n",
      "HeteroData(\n",
      "  artist={ x=[223388, 16] },\n",
      "  track={ x=[24324100, 4] },\n",
      "  tag={ x=[23, 24] },\n",
      "  (artist, collab_with, artist)={\n",
      "    edge_index=[2, 308377],\n",
      "    edge_attr=[308377, 1],\n",
      "    edge_label=[132161],\n",
      "    edge_label_index=[2, 132161],\n",
      "  },\n",
      "  (artist, has_tag_artists, tag)={ edge_index=[2, 1042766] },\n",
      "  (track, has_tag_tracks, tag)={ edge_index=[2, 4030735] },\n",
      "  (artist, last_fm_match, artist)={\n",
      "    edge_index=[2, 28357816],\n",
      "    edge_attr=[28357816, 1],\n",
      "  },\n",
      "  (artist, linked_to, artist)={\n",
      "    edge_index=[2, 1438],\n",
      "    edge_attr=[1438, 1],\n",
      "  },\n",
      "  (artist, musically_related_to, artist)={\n",
      "    edge_index=[2, 41760],\n",
      "    edge_attr=[41760, 1],\n",
      "  },\n",
      "  (artist, personally_related_to, artist)={\n",
      "    edge_index=[2, 3334],\n",
      "    edge_attr=[3334, 1],\n",
      "  },\n",
      "  (tag, tags_artists, artist)={ edge_index=[2, 1042766] },\n",
      "  (tag, tags_track, track)={ edge_index=[2, 4030735] },\n",
      "  (track, worked_by, artist)={ edge_index=[2, 12509457] },\n",
      "  (artist, worked_in, track)={ edge_index=[2, 12509457] }\n",
      ")\n",
      "\n",
      "Validation data:\n",
      "================\n",
      "HeteroData(\n",
      "  artist={ x=[223388, 16] },\n",
      "  track={ x=[24324100, 4] },\n",
      "  tag={ x=[23, 24] },\n",
      "  (artist, collab_with, artist)={\n",
      "    edge_index=[2, 440538],\n",
      "    edge_attr=[440538, 1],\n",
      "    edge_label=[188802],\n",
      "    edge_label_index=[2, 188802],\n",
      "  },\n",
      "  (artist, has_tag_artists, tag)={ edge_index=[2, 1042766] },\n",
      "  (track, has_tag_tracks, tag)={ edge_index=[2, 4030735] },\n",
      "  (artist, last_fm_match, artist)={\n",
      "    edge_index=[2, 28357816],\n",
      "    edge_attr=[28357816, 1],\n",
      "  },\n",
      "  (artist, linked_to, artist)={\n",
      "    edge_index=[2, 1438],\n",
      "    edge_attr=[1438, 1],\n",
      "  },\n",
      "  (artist, musically_related_to, artist)={\n",
      "    edge_index=[2, 41760],\n",
      "    edge_attr=[41760, 1],\n",
      "  },\n",
      "  (artist, personally_related_to, artist)={\n",
      "    edge_index=[2, 3334],\n",
      "    edge_attr=[3334, 1],\n",
      "  },\n",
      "  (tag, tags_artists, artist)={ edge_index=[2, 1042766] },\n",
      "  (tag, tags_track, track)={ edge_index=[2, 4030735] },\n",
      "  (track, worked_by, artist)={ edge_index=[2, 12509457] },\n",
      "  (artist, worked_in, track)={ edge_index=[2, 12509457] }\n",
      ")\n",
      "\n",
      "Test data:\n",
      "================\n",
      "HeteroData(\n",
      "  artist={ x=[223388, 16] },\n",
      "  track={ x=[24324100, 4] },\n",
      "  tag={ x=[23, 24] },\n",
      "  (artist, collab_with, artist)={\n",
      "    edge_index=[2, 534939],\n",
      "    edge_attr=[534939, 1],\n",
      "    edge_label=[188802],\n",
      "    edge_label_index=[2, 188802],\n",
      "  },\n",
      "  (artist, has_tag_artists, tag)={ edge_index=[2, 1042766] },\n",
      "  (track, has_tag_tracks, tag)={ edge_index=[2, 4030735] },\n",
      "  (artist, last_fm_match, artist)={\n",
      "    edge_index=[2, 28357816],\n",
      "    edge_attr=[28357816, 1],\n",
      "  },\n",
      "  (artist, linked_to, artist)={\n",
      "    edge_index=[2, 1438],\n",
      "    edge_attr=[1438, 1],\n",
      "  },\n",
      "  (artist, musically_related_to, artist)={\n",
      "    edge_index=[2, 41760],\n",
      "    edge_attr=[41760, 1],\n",
      "  },\n",
      "  (artist, personally_related_to, artist)={\n",
      "    edge_index=[2, 3334],\n",
      "    edge_attr=[3334, 1],\n",
      "  },\n",
      "  (tag, tags_artists, artist)={ edge_index=[2, 1042766] },\n",
      "  (tag, tags_track, track)={ edge_index=[2, 4030735] },\n",
      "  (track, worked_by, artist)={ edge_index=[2, 12509457] },\n",
      "  (artist, worked_in, track)={ edge_index=[2, 12509457] }\n",
      ")\n",
      "Training edges: 308377\n",
      "Validation edges: 440538\n",
      "Test edges: 534939\n",
      "Train-Val Overlap: 308377\n",
      "Train-Test Overlap: 308377\n",
      "Val-Test Overlap: 440538\n"
     ]
    }
   ],
   "source": [
    "import torch_geometric.transforms as T\n",
    "\n",
    "transform = T.RandomLinkSplit(\n",
    "    num_val=0.15,\n",
    "    num_test=0.15,\n",
    "    disjoint_train_ratio=0.3,\n",
    "    neg_sampling_ratio=1,\n",
    "    add_negative_train_samples=False,\n",
    "    edge_types=(\"artist\", \"collab_with\", \"artist\")\n",
    ")\n",
    "\n",
    "train_data, val_data, test_data = transform(data)\n",
    "\n",
    "print(\"Training data:\")\n",
    "print(\"==============\")\n",
    "print(train_data)\n",
    "print()\n",
    "print(\"Validation data:\")\n",
    "print(\"================\")\n",
    "print(val_data)\n",
    "print()\n",
    "print(\"Test data:\")\n",
    "print(\"================\")\n",
    "print(test_data)\n",
    "\n",
    "print(f\"Training edges: {train_data['artist', 'collab_with', 'artist'].edge_index.shape[1]}\")\n",
    "print(f\"Validation edges: {val_data['artist', 'collab_with', 'artist'].edge_index.shape[1]}\")\n",
    "print(f\"Test edges: {test_data['artist', 'collab_with', 'artist'].edge_index.shape[1]}\")\n",
    "\n",
    "train_edges = set(map(tuple, train_data['artist', 'collab_with', 'artist'].edge_index.T.tolist()))\n",
    "val_edges = set(map(tuple, val_data['artist', 'collab_with', 'artist'].edge_index.T.tolist()))\n",
    "test_edges = set(map(tuple, test_data['artist', 'collab_with', 'artist'].edge_index.T.tolist()))\n",
    "\n",
    "print(f\"Train-Val Overlap: {len(train_edges & val_edges)}\")  # Should be 0\n",
    "print(f\"Train-Test Overlap: {len(train_edges & test_edges)}\")  # Should be 0\n",
    "print(f\"Val-Test Overlap: {len(val_edges & test_edges)}\")  # Should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d596df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get edge index\n",
    "# edge_index = data['artist', 'collab_with', 'artist'].edge_index\n",
    "\n",
    "# # Shuffle edges\n",
    "# num_edges = edge_index.shape[1]\n",
    "# perm = torch.randperm(num_edges)\n",
    "# edge_index = edge_index[:, perm]\n",
    "\n",
    "# # Define sizes\n",
    "# num_test = int(0.15 * num_edges)\n",
    "# num_val = int(0.15 * num_edges)\n",
    "# num_train = num_edges - num_val - num_test\n",
    "\n",
    "# # Split\n",
    "# train_edges = edge_index[:, :num_train]\n",
    "# val_edges = edge_index[:, num_train:num_train + num_val]\n",
    "# test_edges = edge_index[:, num_train + num_val:]\n",
    "\n",
    "# # Verify disjoint sets\n",
    "# train_set = set(map(tuple, train_edges.T.tolist()))\n",
    "# val_set = set(map(tuple, val_edges.T.tolist()))\n",
    "# test_set = set(map(tuple, test_edges.T.tolist()))\n",
    "\n",
    "# print(f\"Train-Val Overlap: {len(train_set & val_set)}\")  # Should be 0\n",
    "# print(f\"Train-Test Overlap: {len(train_set & test_set)}\")  # Should be 0\n",
    "# print(f\"Val-Test Overlap: {len(val_set & test_set)}\")  # Should be 0\n",
    "\n",
    "# # Store the new edge splits in PyG format\n",
    "# train_data = data.clone()\n",
    "# train_data['artist', 'collab_with', 'artist'].edge_index = train_edges\n",
    "\n",
    "# val_data = data.clone()\n",
    "# val_data['artist', 'collab_with', 'artist'].edge_index = val_edges\n",
    "\n",
    "# test_data = data.clone()\n",
    "# test_data['artist', 'collab_with', 'artist'].edge_index = test_edges\n",
    "\n",
    "# print(f\"Training edges: {train_data['artist', 'collab_with', 'artist'].edge_index.shape[1]}\")\n",
    "# print(f\"Validation edges: {val_data['artist', 'collab_with', 'artist'].edge_index.shape[1]}\")\n",
    "# print(f\"Test edges: {test_data['artist', 'collab_with', 'artist'].edge_index.shape[1]}\")\n",
    "\n",
    "# from torch_geometric.utils import negative_sampling\n",
    "\n",
    "# def create_edge_labels(data):\n",
    "#     edge_index = data['artist', 'collab_with', 'artist'].edge_index\n",
    "#     num_nodes = data['artist'].num_nodes\n",
    "\n",
    "#     # Generate negative edges\n",
    "#     neg_edge_index = negative_sampling(\n",
    "#         edge_index=edge_index,\n",
    "#         num_nodes=num_nodes,\n",
    "#         num_neg_samples=edge_index.shape[1]  # 1:1 ratio of positive to negative samples\n",
    "#     )\n",
    "\n",
    "#     # Concatenate positive and negative edges\n",
    "#     edge_label_index = torch.cat([edge_index, neg_edge_index], dim=1)\n",
    "#     edge_label = torch.cat([torch.ones(edge_index.shape[1]), torch.zeros(neg_edge_index.shape[1])], dim=0)\n",
    "\n",
    "#     return edge_label_index, edge_label\n",
    "\n",
    "# # Apply to train, val, test\n",
    "# train_edge_label_index, train_edge_label = create_edge_labels(train_data)\n",
    "# val_edge_label_index, val_edge_label = create_edge_labels(val_data)\n",
    "# test_edge_label_index, test_edge_label = create_edge_labels(test_data)\n",
    "\n",
    "# # Assign to datasets\n",
    "# train_data['artist', 'collab_with', 'artist'].edge_label_index = train_edge_label_index\n",
    "# train_data['artist', 'collab_with', 'artist'].edge_label = train_edge_label\n",
    "\n",
    "# val_data['artist', 'collab_with', 'artist'].edge_label_index = val_edge_label_index\n",
    "# val_data['artist', 'collab_with', 'artist'].edge_label = val_edge_label\n",
    "\n",
    "# test_data['artist', 'collab_with', 'artist'].edge_label_index = test_edge_label_index\n",
    "# test_data['artist', 'collab_with', 'artist'].edge_label = test_edge_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82faa4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: '{device}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "956bf7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train_loader...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleferu/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating val_loader...\n",
      "Sampling mini-batch...\n",
      "Sampled mini-batch:\n",
      "===================\n",
      "HeteroData(\n",
      "  artist={\n",
      "    x=[90052, 16],\n",
      "    n_id=[90052],\n",
      "  },\n",
      "  track={\n",
      "    x=[119352, 4],\n",
      "    n_id=[119352],\n",
      "  },\n",
      "  tag={\n",
      "    x=[23, 24],\n",
      "    n_id=[23],\n",
      "  },\n",
      "  (artist, collab_with, artist)={\n",
      "    edge_index=[2, 35614],\n",
      "    edge_attr=[35614, 1],\n",
      "    edge_label=[128],\n",
      "    edge_label_index=[2, 128],\n",
      "    e_id=[35614],\n",
      "    input_id=[64],\n",
      "  },\n",
      "  (artist, has_tag_artists, tag)={\n",
      "    edge_index=[2, 460],\n",
      "    e_id=[460],\n",
      "  },\n",
      "  (track, has_tag_tracks, tag)={\n",
      "    edge_index=[2, 460],\n",
      "    e_id=[460],\n",
      "  },\n",
      "  (artist, last_fm_match, artist)={\n",
      "    edge_index=[2, 155658],\n",
      "    edge_attr=[155658, 1],\n",
      "    e_id=[155658],\n",
      "  },\n",
      "  (artist, linked_to, artist)={\n",
      "    edge_index=[2, 159],\n",
      "    edge_attr=[159, 1],\n",
      "    e_id=[159],\n",
      "  },\n",
      "  (artist, musically_related_to, artist)={\n",
      "    edge_index=[2, 3251],\n",
      "    edge_attr=[3251, 1],\n",
      "    e_id=[3251],\n",
      "  },\n",
      "  (artist, personally_related_to, artist)={\n",
      "    edge_index=[2, 332],\n",
      "    edge_attr=[332, 1],\n",
      "    e_id=[332],\n",
      "  },\n",
      "  (tag, tags_artists, artist)={\n",
      "    edge_index=[2, 33776],\n",
      "    e_id=[33776],\n",
      "  },\n",
      "  (tag, tags_track, track)={\n",
      "    edge_index=[2, 1515],\n",
      "    e_id=[1515],\n",
      "  },\n",
      "  (track, worked_by, artist)={\n",
      "    edge_index=[2, 119686],\n",
      "    e_id=[119686],\n",
      "  },\n",
      "  (artist, worked_in, track)={\n",
      "    edge_index=[2, 6180],\n",
      "    e_id=[6180],\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "compt_tree_size = [25, 20]\n",
    "\n",
    "edge_label_index = train_data[\"artist\", \"collab_with\", \"artist\"].edge_label_index\n",
    "edge_label = train_data[\"artist\", \"collab_with\", \"artist\"].edge_label\n",
    "\n",
    "print(\"Creating train_loader...\")\n",
    "train_loader = LinkNeighborLoader(\n",
    "    data=train_data,\n",
    "    num_neighbors=compt_tree_size,\n",
    "    neg_sampling_ratio=1,\n",
    "    edge_label_index=((\"artist\", \"collab_with\", \"artist\"), edge_label_index),\n",
    "    edge_label=edge_label,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "edge_label_index = val_data[\"artist\", \"collab_with\", \"artist\"].edge_label_index\n",
    "edge_label = val_data[\"artist\", \"collab_with\", \"artist\"].edge_label\n",
    "\n",
    "print(\"Creating val_loader...\")\n",
    "val_loader = LinkNeighborLoader(\n",
    "    data=val_data,\n",
    "    num_neighbors=compt_tree_size,\n",
    "    edge_label_index=((\"artist\", \"collab_with\", \"artist\"), edge_label_index),\n",
    "    edge_label=edge_label,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(\"Sampling mini-batch...\")\n",
    "\n",
    "sampled_data = next(iter(train_loader))\n",
    "\n",
    "print(\"Sampled mini-batch:\")\n",
    "print(\"===================\")\n",
    "print(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13d3164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "if debug:\n",
    "    print(torch.unique(train_data['artist', 'collab_with', 'artist'].edge_label))\n",
    "    print(torch.unique(next(iter(train_loader))[\"artist\", \"collab_with\", \"artist\"].edge_label))\n",
    "    print(torch.unique(val_data['artist', 'collab_with', 'artist'].edge_label))\n",
    "    print(torch.unique(next(iter(val_loader))[\"artist\", \"collab_with\", \"artist\"].edge_label))\n",
    "    print(torch.unique(test_data['artist', 'collab_with', 'artist'].edge_label))\n",
    "    print(torch.unique(next(iter(test_loader))[\"artist\", \"collab_with\", \"artist\"].edge_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91d598ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import HeteroConv, GATConv, SAGEConv, Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, metadata, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.metadata = metadata\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.conv1 = HeteroConv({\n",
    "            (\"artist\", \"collab_with\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"has_tag_artists\", \"tag\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"last_fm_match\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"track\", \"has_tag_tracks\", \"tag\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"linked_to\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"musically_related_to\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"personally_related_to\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"tag\", \"tags_artists\", \"artist\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"tag\", \"tags_tracks\", \"track\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"track\", \"worked_by\", \"artist\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"worked_in\", \"track\"): SAGEConv((-1, -1), hidden_channels),\n",
    "        }, aggr=\"mean\")\n",
    "\n",
    "        self.conv2 = HeteroConv({\n",
    "            (\"artist\", \"collab_with\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"has_tag_artists\", \"tag\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"last_fm_match\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"track\", \"has_tag_tracks\", \"tag\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"linked_to\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"musically_related_to\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"personally_related_to\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"tag\", \"tags_artists\", \"artist\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"tag\", \"tags_tracks\", \"track\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"track\", \"worked_by\", \"artist\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"worked_in\", \"track\"): SAGEConv((-1, -1), hidden_channels),\n",
    "        }, aggr=\"mean\")\n",
    "\n",
    "        self.linear1 = Linear(hidden_channels * 2, hidden_channels * 4)\n",
    "        self.linear2 = Linear(hidden_channels * 4, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict1 = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict2 = self.conv2(x_dict1, edge_index_dict)\n",
    "\n",
    "        x_artist = torch.cat([x_dict1['artist'], x_dict2['artist']], dim=-1)\n",
    "\n",
    "        x_artist = self.linear1(x_artist)\n",
    "        x_artist = self.linear2(x_artist)\n",
    "\n",
    "        # Normalize the artist node features\n",
    "        x_artist = F.normalize(x_artist, p=2, dim=-1)\n",
    "\n",
    "        # Update the dictionary with the new 'artist' features, leaving other nodes unchanged\n",
    "        x_dict['artist'] = x_artist\n",
    "\n",
    "        return x_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9525bb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for sampled_data in tqdm.tqdm(train_loader):\n",
    "            # Move data to device\n",
    "            sampled_data = sampled_data.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_dict = model(sampled_data.x_dict, sampled_data.edge_index_dict)\n",
    "            \n",
    "            # Get predictions and labels for the 'collab_with' edge type\n",
    "            edge_label_index = sampled_data['artist', 'collab_with', 'artist'].edge_label_index\n",
    "            edge_label = sampled_data['artist', 'collab_with', 'artist'].edge_label\n",
    "\n",
    "            src_emb = pred_dict['artist'][edge_label_index[0]]  # Source node embeddings\n",
    "            dst_emb = pred_dict['artist'][edge_label_index[1]]  # Destination node embeddings\n",
    "            \n",
    "            # Compute the dot product between source and destination embeddings\n",
    "            preds = (src_emb * dst_emb).sum(dim=-1)  # Scalar for each edge\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(preds, edge_label.float())\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Average loss for the epoch\n",
    "        epoch_loss /= len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # Validation metrics\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient computation for validation\n",
    "            for sampled_data in tqdm.tqdm(val_loader):\n",
    "                # Move data to device\n",
    "                sampled_data = sampled_data.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                pred_dict = model(sampled_data.x_dict, sampled_data.edge_index_dict)\n",
    "                \n",
    "                # Get predictions and labels for the 'collab_with' edge type\n",
    "                edge_label_index = sampled_data['artist', 'collab_with', 'artist'].edge_label_index\n",
    "                edge_label = sampled_data['artist', 'collab_with', 'artist'].edge_label\n",
    "\n",
    "                src_emb = pred_dict['artist'][edge_label_index[0]]  # Source node embeddings\n",
    "                dst_emb = pred_dict['artist'][edge_label_index[1]]  # Destination node embeddings\n",
    "                \n",
    "                # Compute the dot product between source and destination embeddings\n",
    "                preds = (src_emb * dst_emb).sum(dim=-1)  # Scalar for each edge\n",
    "\n",
    "                probs = torch.sigmoid(preds)  # Convert to probabilities\n",
    "\n",
    "                loss = criterion(preds, edge_label.float())\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Collect predictions, probabilities, and labels\n",
    "                all_labels.append(edge_label.cpu())\n",
    "                all_probs.append(probs.cpu())\n",
    "        \n",
    "        # Concatenate all predictions and labels\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        all_probs = torch.cat(all_probs)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Find threshold for predictions\n",
    "        best_threshold = 0\n",
    "        best_f1 = 0\n",
    "        for threshold in np.arange(0.2, 0.81, 0.01):\n",
    "            preds_binary = (all_probs > threshold).long()\n",
    "            cm = confusion_matrix(all_labels, preds_binary)\n",
    "            tp = cm[1, 1]\n",
    "            fp = cm[0, 1]\n",
    "            fn = cm[1, 0]\n",
    "            tn = cm[0, 0]\n",
    "            precision = 0 if tp == 0 else tp / (tp + fp)\n",
    "            recall = 0 if tp == 0 else tp / (tp + fn)\n",
    "            f1 = 0 if precision * recall == 0 else 2 * precision * recall / (precision + recall)\n",
    "            if f1 > best_f1:\n",
    "                best_threshold = threshold\n",
    "                best_f1 = f1\n",
    "        print(f\"Best threshold: {best_threshold}\")\n",
    "        all_preds = (all_probs > best_threshold).long()\n",
    "        \n",
    "        # Compute metrics\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        tp = cm[1, 1]\n",
    "        fp = cm[0, 1]\n",
    "        fn = cm[1, 0]\n",
    "        tn = cm[0, 0]\n",
    "        accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        roc_auc = roc_auc_score(all_labels, all_probs)\n",
    "        \n",
    "        # Print validation metrics\n",
    "        print(f\"Validation Metrics - Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"Loss:      {val_loss:.4f}\")\n",
    "        print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall:    {recall:.4f}\")\n",
    "        print(f\"F1-score:  {f1:.4f}\")\n",
    "        print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{tp} {fn}\\n{fp} {tn}\")\n",
    "\n",
    "    return best_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13d6278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion, device, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for sampled_data in tqdm.tqdm(test_loader):\n",
    "            # Move data to the device\n",
    "            sampled_data = sampled_data.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            pred_dict = model(sampled_data.x_dict, sampled_data.edge_index_dict)\n",
    "\n",
    "            # Get predictions and labels for the 'collab_with' edge type\n",
    "            edge_label_index = sampled_data['artist', 'collab_with', 'artist'].edge_label_index\n",
    "            edge_label = sampled_data['artist', 'collab_with', 'artist'].edge_label\n",
    "\n",
    "            src_emb = pred_dict['artist'][edge_label_index[0]]  # Source node embeddings\n",
    "            dst_emb = pred_dict['artist'][edge_label_index[1]]  # Destination node embeddings\n",
    "            \n",
    "            # Compute the dot product between source and destination embeddings\n",
    "            preds = (src_emb * dst_emb).sum(dim=-1)  # Scalar for each edge\n",
    "            probs = torch.sigmoid(preds)  # Convert logits to probabilities\n",
    "            preds_binary = (probs > threshold).long()  # Convert probabilities to binary predictions\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(preds, edge_label.float())\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Collect predictions and labels\n",
    "            all_preds.append(preds_binary.cpu())\n",
    "            all_labels.append(edge_label.cpu())\n",
    "            all_probs.append(probs.cpu())\n",
    "\n",
    "    # Concatenate all predictions and labels\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    all_probs = torch.cat(all_probs)\n",
    "\n",
    "    # Compute metrics\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    tp = cm[1, 1]\n",
    "    fp = cm[0, 1]\n",
    "    fn = cm[1, 0]\n",
    "    tn = cm[0, 0]\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    roc_auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    # Average test loss\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    print(\"Test Results:\")\n",
    "    print(f\"Loss:      {test_loss:.4f}\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-score:  {f1:.4f}\")\n",
    "    print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{tp} {fn}\\n{fp} {tn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c71d770d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2066/2066 [02:47<00:00, 12.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 0.5898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2951/2951 [02:18<00:00, 21.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.6800000000000004\n",
      "Validation Metrics - Epoch 1/20:\n",
      "Loss:      0.5768\n",
      "Accuracy:  0.7943\n",
      "Precision: 0.7502\n",
      "Recall:    0.8823\n",
      "F1-score:  0.8109\n",
      "ROC-AUC:   0.8223\n",
      "Confusion Matrix:\n",
      "83289 11112\n",
      "27734 66667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2066/2066 [03:00<00:00, 11.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Training Loss: 0.5672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2951/2951 [02:20<00:00, 20.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.6600000000000004\n",
      "Validation Metrics - Epoch 2/20:\n",
      "Loss:      0.5621\n",
      "Accuracy:  0.8310\n",
      "Precision: 0.7851\n",
      "Recall:    0.9116\n",
      "F1-score:  0.8436\n",
      "ROC-AUC:   0.8867\n",
      "Confusion Matrix:\n",
      "86053 8348\n",
      "23559 70842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2066/2066 [03:00<00:00, 11.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Training Loss: 0.5611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2951/2951 [02:21<00:00, 20.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.7200000000000004\n",
      "Validation Metrics - Epoch 3/20:\n",
      "Loss:      0.5676\n",
      "Accuracy:  0.8532\n",
      "Precision: 0.8438\n",
      "Recall:    0.8669\n",
      "F1-score:  0.8552\n",
      "ROC-AUC:   0.9029\n",
      "Confusion Matrix:\n",
      "81837 12564\n",
      "15149 79252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 211/2066 [00:22<03:13,  9.58it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m GNN(metadata\u001b[38;5;241m=\u001b[39mtrain_data\u001b[38;5;241m.\u001b[39mmetadata(), hidden_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m best_threshold \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sampled_data \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(train_loader):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Move data to device\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     sampled_data \u001b[38;5;241m=\u001b[39m \u001b[43msampled_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     pred_dict \u001b[38;5;241m=\u001b[39m model(sampled_data\u001b[38;5;241m.\u001b[39mx_dict, sampled_data\u001b[38;5;241m.\u001b[39medge_index_dict)\n",
      "File \u001b[0;32m~/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch_geometric/data/data.py:362\u001b[0m, in \u001b[0;36mBaseData.to\u001b[0;34m(self, device, non_blocking, *args)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, device: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m], \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    358\u001b[0m        non_blocking: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Performs tensor device conversion, either for all attributes or\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03m    only the ones given in :obj:`*args`.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch_geometric/data/data.py:342\u001b[0m, in \u001b[0;36mBaseData.apply\u001b[0;34m(self, func, *args)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies the function :obj:`func`, either to all attributes or only\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03mthe ones given in :obj:`*args`.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m store \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstores:\n\u001b[0;32m--> 342\u001b[0m     \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch_geometric/data/storage.py:201\u001b[0m, in \u001b[0;36mBaseStorage.apply\u001b[0;34m(self, func, *args)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies the function :obj:`func`, either to all attributes or only\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03mthe ones given in :obj:`*args`.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems(\u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28mself\u001b[39m[key] \u001b[38;5;241m=\u001b[39m \u001b[43mrecursive_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch_geometric/data/storage.py:897\u001b[0m, in \u001b[0;36mrecursive_apply\u001b[0;34m(data, func)\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecursive_apply\u001b[39m(data: Any, func: Callable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Tensor):\n\u001b[0;32m--> 897\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mPackedSequence):\n\u001b[1;32m    899\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(data)\n",
      "File \u001b[0;32m~/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch_geometric/data/data.py:363\u001b[0m, in \u001b[0;36mBaseData.to.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, device: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m], \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    358\u001b[0m        non_blocking: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Performs tensor device conversion, either for all attributes or\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03m    only the ones given in :obj:`*args`.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m--> 363\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = GNN(metadata=train_data.metadata(), hidden_channels=64, out_channels=64).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "best_threshold = train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    F.binary_cross_entropy_with_logits,\n",
    "    device,\n",
    "    20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab218838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_threshold = train(\n",
    "#     model,\n",
    "#     train_loader,\n",
    "#     val_loader,\n",
    "#     optimizer,\n",
    "#     F.binary_cross_entropy_with_logits,\n",
    "#     device,\n",
    "#     20\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04b44cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test_loader...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleferu/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    }
   ],
   "source": [
    "edge_label_index = test_data[\"artist\", \"collab_with\", \"artist\"].edge_label_index\n",
    "edge_label = test_data[\"artist\", \"collab_with\", \"artist\"].edge_label\n",
    "\n",
    "print(\"Creating test_loader...\")\n",
    "test_loader = LinkNeighborLoader(\n",
    "    data=test_data,\n",
    "    num_neighbors=compt_tree_size,\n",
    "    edge_label_index=((\"artist\", \"collab_with\", \"artist\"), edge_label_index),\n",
    "    edge_label=edge_label,\n",
    "    batch_size=512,\n",
    "    shuffle=False,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4561c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [01:43<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Loss:      0.5527\n",
      "Accuracy:  0.8295\n",
      "Precision: 0.8379\n",
      "Recall:    0.8171\n",
      "F1-score:  0.8274\n",
      "ROC-AUC:   0.8862\n",
      "Confusion Matrix:\n",
      "77136 17265\n",
      "14925 79476\n"
     ]
    }
   ],
   "source": [
    "test_model(\n",
    "    model,\n",
    "    test_loader,\n",
    "    F.binary_cross_entropy_with_logits,\n",
    "    device,\n",
    "    best_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ebb80aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./normal-newdata.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d13ec911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108036/3795534198.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test.load_state_dict(torch.load(\"./normal-nolfm.pth\"))\n",
      "100%|| 615/615 [00:29<00:00, 21.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Loss:      0.5740\n",
      "Accuracy:  0.8026\n",
      "Precision: 0.7512\n",
      "Recall:    0.9051\n",
      "F1-score:  0.8210\n",
      "ROC-AUC:   0.8483\n",
      "Confusion Matrix:\n",
      "142398 14937\n",
      "47169 110166\n"
     ]
    }
   ],
   "source": [
    "test = GNN(metadata=train_data.metadata(), out_channels=64).to(device)\n",
    "test.load_state_dict(torch.load(\"./normal-newdata.pth\"))\n",
    "test_model(\n",
    "    test,\n",
    "    test_loader,\n",
    "    F.binary_cross_entropy_with_logits,\n",
    "    device,\n",
    "    best_threshold\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicbrainz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
