{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31d33376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "import os.path as path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e12322ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"pyg_experiments/ds_float32/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdb5b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = HeteroData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1192f48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artist tensor shape: torch.Size([1489250, 16])\n",
      "Track tensor shape: torch.Size([24324100, 4])\n",
      "Tag tensor shape: torch.Size([23, 1])\n",
      "collab_with index tensor shape: torch.Size([2, 2463052])\n",
      "collab_with attr tensor shape: torch.Size([2463052, 1])\n",
      "has_tag_artists index tensor shape: torch.Size([2, 2410207])\n",
      "has_tag_tracks index tensor shape: torch.Size([2, 4030735])\n",
      "linked_to index tensor shape: torch.Size([2, 23128])\n",
      "linked_to attr tensor shape: torch.Size([23128, 1])\n",
      "musically_related_to index tensor shape: torch.Size([2, 373262])\n",
      "musically_related_to attr tensor shape: torch.Size([373262, 1])\n",
      "personally_related_to index tensor shape: torch.Size([2, 26720])\n",
      "personally_related_to attr tensor shape: torch.Size([26720, 1])\n",
      "tags_artists index tensor shape: torch.Size([2, 2410207])\n",
      "tags_tracks index tensor shape: torch.Size([2, 4030735])\n",
      "worked_by index tensor shape: torch.Size([2, 27661673])\n",
      "worked_in index tensor shape: torch.Size([2, 27661673])\n"
     ]
    }
   ],
   "source": [
    "data[\"artist\"].x = torch.load(path.join(data_folder, \"artists.pt\"), weights_only=True)\n",
    "print(\"Artist tensor shape:\", data[\"artist\"].x.shape)\n",
    "\n",
    "data[\"track\"].x = torch.load(path.join(data_folder, \"tracks.pt\"), weights_only=True)\n",
    "print(\"Track tensor shape:\", data[\"track\"].x.shape)\n",
    "\n",
    "data[\"tag\"].x = torch.load(path.join(data_folder, \"tags.pt\"), weights_only=True)\n",
    "print(\"Tag tensor shape:\", data[\"tag\"].x.shape)\n",
    "\n",
    "\n",
    "data[\"artist\", \"collab_with\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"collab_with.pt\"), weights_only=True).t().long()\n",
    "data[\"artist\", \"collab_with\", \"artist\"].edge_attr = torch.load(path.join(data_folder, \"collab_with_attr.pt\"), weights_only=True)\n",
    "print(\"collab_with index tensor shape:\", data[\"artist\", \"collab_with\", \"artist\"].edge_index.shape)\n",
    "print(\"collab_with attr tensor shape:\", data[\"artist\", \"collab_with\", \"artist\"].edge_attr.shape)\n",
    "\n",
    "data[\"artist\", \"has_tag_artists\", \"tag\"].edge_index = torch.load(path.join(data_folder, \"has_tag_artists.pt\"), weights_only=True).t().long()\n",
    "data[\"track\", \"has_tag_tracks\", \"tag\"].edge_index = torch.load(path.join(data_folder, \"has_tag_tracks.pt\"), weights_only=True).t().long()\n",
    "print(\"has_tag_artists index tensor shape:\", data[\"artist\", \"has_tag_artists\", \"tag\"].edge_index.shape)\n",
    "print(\"has_tag_tracks index tensor shape:\", data[\"track\", \"has_tag_tracks\", \"tag\"].edge_index.shape)\n",
    "\n",
    "# data[\"artist\", \"last_fm_match\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"last_fm_match.pt\"), weights_only=True).t().long()\n",
    "# data[\"artist\", \"last_fm_match\", \"artist\"].edge_attr = torch.load(path.join(data_folder, \"last_fm_match_attr.pt\"), weights_only=True)\n",
    "# print(\"last_fm_match index tensor shape:\", data[\"artist\", \"last_fm_match\", \"artist\"].edge_index.shape)\n",
    "# print(\"last_fm_match attr tensor shape:\", data[\"artist\", \"last_fm_match\", \"artist\"].edge_attr.shape)\n",
    "\n",
    "data[\"artist\", \"linked_to\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"linked_to.pt\"), weights_only=True).t().long()\n",
    "data[\"artist\", \"linked_to\", \"artist\"].edge_attr = torch.load(path.join(data_folder, \"linked_to_attr.pt\"), weights_only=True)\n",
    "print(\"linked_to index tensor shape:\", data[\"artist\", \"linked_to\", \"artist\"].edge_index.shape)\n",
    "print(\"linked_to attr tensor shape:\", data[\"artist\", \"linked_to\", \"artist\"].edge_attr.shape)\n",
    "\n",
    "data[\"artist\", \"musically_related_to\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"musically_related_to.pt\"), weights_only=True).t().long()\n",
    "data[\"artist\", \"musically_related_to\", \"artist\"].edge_attr = torch.load(path.join(data_folder, \"musically_related_to_attr.pt\"), weights_only=True)\n",
    "print(\"musically_related_to index tensor shape:\", data[\"artist\", \"musically_related_to\", \"artist\"].edge_index.shape)\n",
    "print(\"musically_related_to attr tensor shape:\", data[\"artist\", \"musically_related_to\", \"artist\"].edge_attr.shape)\n",
    "\n",
    "data[\"artist\", \"personally_related_to\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"personally_related_to.pt\"), weights_only=True).t().long()\n",
    "data[\"artist\", \"personally_related_to\", \"artist\"].edge_attr = torch.load(path.join(data_folder, \"personally_related_to_attr.pt\"), weights_only=True)\n",
    "print(\"personally_related_to index tensor shape:\", data[\"artist\", \"personally_related_to\", \"artist\"].edge_index.shape)\n",
    "print(\"personally_related_to attr tensor shape:\", data[\"artist\", \"personally_related_to\", \"artist\"].edge_attr.shape)\n",
    "\n",
    "data[\"tag\", \"tags_artists\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"tags_artists.pt\"), weights_only=True).t().long()\n",
    "data[\"tag\", \"tags_track\", \"track\"].edge_index = torch.load(path.join(data_folder, \"tags_tracks.pt\"), weights_only=True).t().long()\n",
    "print(\"tags_artists index tensor shape:\", data[\"tag\", \"tags_artists\", \"artist\"].edge_index.shape)\n",
    "print(\"tags_tracks index tensor shape:\", data[\"tag\", \"tags_track\", \"track\"].edge_index.shape)\n",
    "\n",
    "data[\"track\", \"worked_by\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"worked_by.pt\"), weights_only=True).t().long()\n",
    "data[\"artist\", \"worked_in\", \"track\"].edge_index = torch.load(path.join(data_folder, \"worked_in.pt\"), weights_only=True).t().long()\n",
    "print(\"worked_by index tensor shape:\", data[\"track\", \"worked_by\", \"artist\"].edge_index.shape)\n",
    "print(\"worked_in index tensor shape:\", data[\"artist\", \"worked_in\", \"track\"].edge_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b839629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "==============\n",
      "HeteroData(\n",
      "  artist={ x=[1489250, 16] },\n",
      "  track={ x=[24324100, 4] },\n",
      "  tag={ x=[23, 1] },\n",
      "  (artist, collab_with, artist)={\n",
      "    edge_index=[2, 1034483],\n",
      "    edge_attr=[1034483, 1],\n",
      "    edge_label=[443349],\n",
      "    edge_label_index=[2, 443349],\n",
      "  },\n",
      "  (artist, has_tag_artists, tag)={ edge_index=[2, 2410207] },\n",
      "  (track, has_tag_tracks, tag)={ edge_index=[2, 4030735] },\n",
      "  (artist, linked_to, artist)={\n",
      "    edge_index=[2, 23128],\n",
      "    edge_attr=[23128, 1],\n",
      "  },\n",
      "  (artist, musically_related_to, artist)={\n",
      "    edge_index=[2, 373262],\n",
      "    edge_attr=[373262, 1],\n",
      "  },\n",
      "  (artist, personally_related_to, artist)={\n",
      "    edge_index=[2, 26720],\n",
      "    edge_attr=[26720, 1],\n",
      "  },\n",
      "  (tag, tags_artists, artist)={ edge_index=[2, 2410207] },\n",
      "  (tag, tags_track, track)={ edge_index=[2, 4030735] },\n",
      "  (track, worked_by, artist)={ edge_index=[2, 27661673] },\n",
      "  (artist, worked_in, track)={ edge_index=[2, 27661673] }\n",
      ")\n",
      "\n",
      "Validation data:\n",
      "================\n",
      "HeteroData(\n",
      "  artist={ x=[1489250, 16] },\n",
      "  track={ x=[24324100, 4] },\n",
      "  tag={ x=[23, 1] },\n",
      "  (artist, collab_with, artist)={\n",
      "    edge_index=[2, 1477832],\n",
      "    edge_attr=[1477832, 1],\n",
      "    edge_label=[1477830],\n",
      "    edge_label_index=[2, 1477830],\n",
      "  },\n",
      "  (artist, has_tag_artists, tag)={ edge_index=[2, 2410207] },\n",
      "  (track, has_tag_tracks, tag)={ edge_index=[2, 4030735] },\n",
      "  (artist, linked_to, artist)={\n",
      "    edge_index=[2, 23128],\n",
      "    edge_attr=[23128, 1],\n",
      "  },\n",
      "  (artist, musically_related_to, artist)={\n",
      "    edge_index=[2, 373262],\n",
      "    edge_attr=[373262, 1],\n",
      "  },\n",
      "  (artist, personally_related_to, artist)={\n",
      "    edge_index=[2, 26720],\n",
      "    edge_attr=[26720, 1],\n",
      "  },\n",
      "  (tag, tags_artists, artist)={ edge_index=[2, 2410207] },\n",
      "  (tag, tags_track, track)={ edge_index=[2, 4030735] },\n",
      "  (track, worked_by, artist)={ edge_index=[2, 27661673] },\n",
      "  (artist, worked_in, track)={ edge_index=[2, 27661673] }\n",
      ")\n",
      "\n",
      "Test data:\n",
      "================\n",
      "HeteroData(\n",
      "  artist={ x=[1489250, 16] },\n",
      "  track={ x=[24324100, 4] },\n",
      "  tag={ x=[23, 1] },\n",
      "  (artist, collab_with, artist)={\n",
      "    edge_index=[2, 1970442],\n",
      "    edge_attr=[1970442, 1],\n",
      "    edge_label=[1477830],\n",
      "    edge_label_index=[2, 1477830],\n",
      "  },\n",
      "  (artist, has_tag_artists, tag)={ edge_index=[2, 2410207] },\n",
      "  (track, has_tag_tracks, tag)={ edge_index=[2, 4030735] },\n",
      "  (artist, linked_to, artist)={\n",
      "    edge_index=[2, 23128],\n",
      "    edge_attr=[23128, 1],\n",
      "  },\n",
      "  (artist, musically_related_to, artist)={\n",
      "    edge_index=[2, 373262],\n",
      "    edge_attr=[373262, 1],\n",
      "  },\n",
      "  (artist, personally_related_to, artist)={\n",
      "    edge_index=[2, 26720],\n",
      "    edge_attr=[26720, 1],\n",
      "  },\n",
      "  (tag, tags_artists, artist)={ edge_index=[2, 2410207] },\n",
      "  (tag, tags_track, track)={ edge_index=[2, 4030735] },\n",
      "  (track, worked_by, artist)={ edge_index=[2, 27661673] },\n",
      "  (artist, worked_in, track)={ edge_index=[2, 27661673] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch_geometric.transforms as T\n",
    "\n",
    "transform = T.RandomLinkSplit(\n",
    "    num_val=0.2,\n",
    "    num_test=0.2,\n",
    "    disjoint_train_ratio=0.3,\n",
    "    neg_sampling_ratio=2,\n",
    "    add_negative_train_samples=False,\n",
    "    edge_types=(\"artist\", \"collab_with\", \"artist\"),\n",
    ")\n",
    "\n",
    "train_data, val_data, test_data = transform(data)\n",
    "\n",
    "print(\"Training data:\")\n",
    "print(\"==============\")\n",
    "print(train_data)\n",
    "print()\n",
    "print(\"Validation data:\")\n",
    "print(\"================\")\n",
    "print(val_data)\n",
    "print()\n",
    "print(\"Test data:\")\n",
    "print(\"================\")\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "956bf7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleferu/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled mini-batch:\n",
      "===================\n",
      "HeteroData(\n",
      "  artist={\n",
      "    x=[16366, 16],\n",
      "    n_id=[16366],\n",
      "  },\n",
      "  track={\n",
      "    x=[27523, 4],\n",
      "    n_id=[27523],\n",
      "  },\n",
      "  tag={\n",
      "    x=[23, 1],\n",
      "    n_id=[23],\n",
      "  },\n",
      "  (artist, collab_with, artist)={\n",
      "    edge_index=[2, 15612],\n",
      "    edge_attr=[15612, 1],\n",
      "    edge_label=[384],\n",
      "    edge_label_index=[2, 384],\n",
      "    e_id=[15612],\n",
      "    input_id=[128],\n",
      "  },\n",
      "  (artist, has_tag_artists, tag)={\n",
      "    edge_index=[2, 230],\n",
      "    e_id=[230],\n",
      "  },\n",
      "  (track, has_tag_tracks, tag)={\n",
      "    edge_index=[2, 230],\n",
      "    e_id=[230],\n",
      "  },\n",
      "  (artist, linked_to, artist)={\n",
      "    edge_index=[2, 216],\n",
      "    edge_attr=[216, 1],\n",
      "    e_id=[216],\n",
      "  },\n",
      "  (artist, musically_related_to, artist)={\n",
      "    edge_index=[2, 3803],\n",
      "    edge_attr=[3803, 1],\n",
      "    e_id=[3803],\n",
      "  },\n",
      "  (artist, personally_related_to, artist)={\n",
      "    edge_index=[2, 468],\n",
      "    edge_attr=[468, 1],\n",
      "    e_id=[468],\n",
      "  },\n",
      "  (tag, tags_artists, artist)={\n",
      "    edge_index=[2, 8419],\n",
      "    e_id=[8419],\n",
      "  },\n",
      "  (tag, tags_track, track)={\n",
      "    edge_index=[2, 1179],\n",
      "    e_id=[1179],\n",
      "  },\n",
      "  (track, worked_by, artist)={\n",
      "    edge_index=[2, 28109],\n",
      "    e_id=[28109],\n",
      "  },\n",
      "  (artist, worked_in, track)={\n",
      "    edge_index=[2, 11251],\n",
      "    e_id=[11251],\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "edge_label_index = train_data[\"artist\", \"collab_with\", \"artist\"].edge_label_index\n",
    "edge_label = train_data[\"artist\", \"collab_with\", \"artist\"].edge_label\n",
    "\n",
    "train_loader = LinkNeighborLoader(\n",
    "    data=train_data,\n",
    "    num_neighbors=[20, 10],\n",
    "    neg_sampling_ratio= 2,\n",
    "    edge_label_index=((\"artist\", \"collab_with\", \"artist\"), edge_label_index),\n",
    "    edge_label=edge_label,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "edge_label_index = val_data[\"artist\", \"collab_with\", \"artist\"].edge_label_index\n",
    "edge_label = val_data[\"artist\", \"collab_with\", \"artist\"].edge_label\n",
    "\n",
    "val_loader = LinkNeighborLoader(\n",
    "    data=val_data,\n",
    "    num_neighbors=[20, 10],\n",
    "    neg_sampling_ratio= 2,\n",
    "    edge_label_index=((\"artist\", \"collab_with\", \"artist\"), edge_label_index),\n",
    "    edge_label=edge_label,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "edge_label_index = test_data[\"artist\", \"collab_with\", \"artist\"].edge_label_index\n",
    "edge_label = test_data[\"artist\", \"collab_with\", \"artist\"].edge_label\n",
    "\n",
    "test_loader = LinkNeighborLoader(\n",
    "    data=test_data,\n",
    "    num_neighbors=[20, 10],\n",
    "    neg_sampling_ratio= 2,\n",
    "    edge_label_index=((\"artist\", \"collab_with\", \"artist\"), edge_label_index),\n",
    "    edge_label=edge_label,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "sampled_data = next(iter(train_loader))\n",
    "\n",
    "print(\"Sampled mini-batch:\")\n",
    "print(\"===================\")\n",
    "print(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82faa4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: '{device}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91d598ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import HeteroConv, GATConv, SAGEConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, metadata, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = HeteroConv({\n",
    "            (\"artist\", \"collab_with\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"has_tag_artists\", \"tag\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"track\", \"has_tag_tracks\", \"tag\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"linked_to\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"musically_related_to\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"personally_related_to\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"tag\", \"tags_artists\", \"artist\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"tag\", \"tags_tracks\", \"track\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"track\", \"worked_by\", \"artist\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"worked_in\", \"track\"): SAGEConv((-1, -1), hidden_channels),\n",
    "        }, aggr='sum')\n",
    "        \n",
    "        self.conv2 = HeteroConv({\n",
    "            (\"artist\", \"collab_with\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"has_tag_artists\", \"tag\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"track\", \"has_tag_tracks\", \"tag\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"linked_to\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"musically_related_to\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"personally_related_to\", \"artist\"): GATConv((-1, -1), hidden_channels),\n",
    "            (\"tag\", \"tags_artists\", \"artist\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"tag\", \"tags_tracks\", \"track\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"track\", \"worked_by\", \"artist\"): SAGEConv((-1, -1), hidden_channels),\n",
    "            (\"artist\", \"worked_in\", \"track\"): SAGEConv((-1, -1), hidden_channels),\n",
    "        }, aggr='sum')\n",
    "\n",
    "        self.lin = torch.nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
    "        x_dict = self.conv2(x_dict, edge_index_dict)\n",
    "        return {key: self.lin(x) for key, x in x_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9525bb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import tqdm\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for sampled_data in tqdm.tqdm(train_loader):\n",
    "            # Move data to device\n",
    "            sampled_data = sampled_data.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_dict = model(sampled_data.x_dict, sampled_data.edge_index_dict)\n",
    "            \n",
    "            # Get predictions and labels for the 'collab_with' edge type\n",
    "            edge_label_index = sampled_data['artist', 'collab_with', 'artist'].edge_label_index\n",
    "            edge_label = sampled_data['artist', 'collab_with', 'artist'].edge_label\n",
    "            \n",
    "            preds = pred_dict['artist'][edge_label_index[0]].squeeze()\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(preds, edge_label.float())\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Average loss for the epoch\n",
    "        epoch_loss /= len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # Validation metrics\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient computation for validation\n",
    "            for sampled_data in tqdm.tqdm(val_loader):\n",
    "                # Move data to device\n",
    "                sampled_data = sampled_data.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                pred_dict = model(sampled_data.x_dict, sampled_data.edge_index_dict)\n",
    "                \n",
    "                # Get predictions and labels for the 'collab_with' edge type\n",
    "                edge_label_index = sampled_data['artist', 'collab_with', 'artist'].edge_label_index\n",
    "                edge_label = sampled_data['artist', 'collab_with', 'artist'].edge_label\n",
    "                \n",
    "                preds = pred_dict['artist'][edge_label_index[0]].squeeze()\n",
    "                probs = torch.sigmoid(preds)  # Convert to probabilities\n",
    "                preds_binary = (probs > 0.5).long()  # Convert to binary predictions\n",
    "\n",
    "                loss = criterion(preds, edge_label.float())\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Collect predictions, probabilities, and labels\n",
    "                all_preds.append(preds_binary.cpu())\n",
    "                all_labels.append(edge_label.cpu())\n",
    "                all_probs.append(probs.cpu())\n",
    "        \n",
    "        # Concatenate all predictions and labels\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        all_probs = torch.cat(all_probs)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Compute metrics\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "        roc_auc = roc_auc_score(all_labels, all_probs)\n",
    "        \n",
    "        # Print validation metrics\n",
    "        print(f\"Validation Metrics - Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"Loss:      {val_loss:.4f}\")\n",
    "        print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall:    {recall:.4f}\")\n",
    "        print(f\"F1-score:  {f1:.4f}\")\n",
    "        print(f\"ROC-AUC:   {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13d6278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for sampled_data in tqdm.tqdm(test_loader):\n",
    "            # Move data to the device\n",
    "            sampled_data = sampled_data.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            pred_dict = model(sampled_data.x_dict, sampled_data.edge_index_dict)\n",
    "\n",
    "            # Get predictions and labels for the 'collab_with' edge type\n",
    "            edge_label_index = sampled_data['artist', 'collab_with', 'artist'].edge_label_index\n",
    "            edge_label = sampled_data['artist', 'collab_with', 'artist'].edge_label\n",
    "\n",
    "            preds = pred_dict['artist'][edge_label_index[0]].squeeze()\n",
    "            probs = torch.sigmoid(preds)  # Convert logits to probabilities\n",
    "            preds_binary = (probs > 0.5).long()  # Convert probabilities to binary predictions\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(preds, edge_label.float())\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Collect predictions and labels\n",
    "            all_preds.append(preds_binary.cpu())\n",
    "            all_labels.append(edge_label.cpu())\n",
    "            all_probs.append(probs.cpu())\n",
    "\n",
    "    # Concatenate all predictions and labels\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    all_probs = torch.cat(all_probs)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "    roc_auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    # Average test loss\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    print(\"Test Results:\")\n",
    "    print(f\"Loss:      {test_loss:.4f}\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-score:  {f1:.4f}\")\n",
    "    print(f\"ROC-AUC:   {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c71d770d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3464/3464 [04:03<00:00, 14.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Training Loss: 7.3193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1732/1732 [03:41<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics - Epoch 1/3:\n",
      "Loss:      1.6500\n",
      "Accuracy:  0.7470\n",
      "Precision: 0.6045\n",
      "Recall:    0.6972\n",
      "F1-score:  0.6475\n",
      "ROC-AUC:   0.8129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3464/3464 [04:21<00:00, 13.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Training Loss: 1.2705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1732/1732 [03:55<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics - Epoch 2/3:\n",
      "Loss:      0.6933\n",
      "Accuracy:  0.8014\n",
      "Precision: 0.7505\n",
      "Recall:    0.6053\n",
      "F1-score:  0.6701\n",
      "ROC-AUC:   0.8407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3464/3464 [04:14<00:00, 13.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Training Loss: 0.6519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1732/1732 [03:44<00:00,  7.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics - Epoch 3/3:\n",
      "Loss:      0.4708\n",
      "Accuracy:  0.7855\n",
      "Precision: 0.7761\n",
      "Recall:    0.5011\n",
      "F1-score:  0.6090\n",
      "ROC-AUC:   0.8540\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "model = GNN(metadata=train_data.metadata(), hidden_channels=64, out_channels=1).to(device)\n",
    "\n",
    "train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "    F.binary_cross_entropy_with_logits,\n",
    "    device,\n",
    "    3\n",
    ")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed50f69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3464/3464 [04:05<00:00, 14.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Training Loss: 0.5458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1732/1732 [03:44<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics - Epoch 1/3:\n",
      "Loss:      0.4591\n",
      "Accuracy:  0.7914\n",
      "Precision: 0.7928\n",
      "Recall:    0.5066\n",
      "F1-score:  0.6182\n",
      "ROC-AUC:   0.8664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3464/3464 [04:01<00:00, 14.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Training Loss: 0.4655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1732/1732 [03:42<00:00,  7.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics - Epoch 2/3:\n",
      "Loss:      0.4414\n",
      "Accuracy:  0.8009\n",
      "Precision: 0.7185\n",
      "Recall:    0.6621\n",
      "F1-score:  0.6891\n",
      "ROC-AUC:   0.8566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3464/3464 [04:00<00:00, 14.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Training Loss: 0.4513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1732/1732 [03:45<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics - Epoch 3/3:\n",
      "Loss:      0.4281\n",
      "Accuracy:  0.8034\n",
      "Precision: 0.7257\n",
      "Recall:    0.6596\n",
      "F1-score:  0.6910\n",
      "ROC-AUC:   0.8674\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "    F.binary_cross_entropy_with_logits,\n",
    "    device,\n",
    "    3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4561c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3464/3464 [04:40<00:00, 12.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Loss:      0.4233\n",
      "Accuracy:  0.8027\n",
      "Precision: 0.7056\n",
      "Recall:    0.7002\n",
      "F1-score:  0.7029\n",
      "ROC-AUC:   0.8726\n"
     ]
    }
   ],
   "source": [
    "test_model(\n",
    "    model,\n",
    "    test_loader,\n",
    "    F.binary_cross_entropy_with_logits,\n",
    "    device\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicbrainz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
