{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31d33376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "import os.path as path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e12322ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"ds_float32/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdb5b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = HeteroData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1192f48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artist tensor shape: torch.Size([1489250, 16])\n",
      "Track tensor shape: torch.Size([24324100, 4])\n",
      "Tag tensor shape: torch.Size([23, 1])\n",
      "collab_with index tensor shape: torch.Size([2, 2463052])\n",
      "collab_with attr tensor shape: torch.Size([2463052, 1])\n",
      "has_tag_artists index tensor shape: torch.Size([2, 2410207])\n",
      "has_tag_tracks index tensor shape: torch.Size([2, 4030735])\n",
      "last_fm_match index tensor shape: torch.Size([2, 154865250])\n",
      "last_fm_match attr tensor shape: torch.Size([154865250, 1])\n",
      "linked_to index tensor shape: torch.Size([2, 23128])\n",
      "linked_to attr tensor shape: torch.Size([23128, 1])\n",
      "musically_related_to index tensor shape: torch.Size([2, 373262])\n",
      "musically_related_to attr tensor shape: torch.Size([373262, 1])\n",
      "personally_related_to index tensor shape: torch.Size([2, 26720])\n",
      "personally_related_to attr tensor shape: torch.Size([26720, 1])\n",
      "tags_artists index tensor shape: torch.Size([2, 2410207])\n",
      "tags_tracks index tensor shape: torch.Size([2, 4030735])\n",
      "worked_by index tensor shape: torch.Size([2, 27661673])\n",
      "worked_in index tensor shape: torch.Size([2, 27661673])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"artist\"].x = torch.load(path.join(data_folder, \"artists.pt\"), weights_only=True)\n",
    "print(\"Artist tensor shape:\", data[\"artist\"].x.shape)\n",
    "\n",
    "data[\"track\"].x = torch.load(path.join(data_folder, \"tracks.pt\"), weights_only=True)\n",
    "print(\"Track tensor shape:\", data[\"track\"].x.shape)\n",
    "\n",
    "data[\"tag\"].x = torch.load(path.join(data_folder, \"tags.pt\"), weights_only=True)\n",
    "print(\"Tag tensor shape:\", data[\"tag\"].x.shape)\n",
    "\n",
    "\n",
    "data[\"artist\", \"collab_with\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"collab_with.pt\"), weights_only=True).t().long()\n",
    "data[\"artist\", \"collab_with\", \"artist\"].edge_attr = torch.load(path.join(data_folder, \"collab_with_attr.pt\"), weights_only=True)\n",
    "print(\"collab_with index tensor shape:\", data[\"artist\", \"collab_with\", \"artist\"].edge_index.shape)\n",
    "print(\"collab_with attr tensor shape:\", data[\"artist\", \"collab_with\", \"artist\"].edge_attr.shape)\n",
    "\n",
    "data[\"artist\", \"has_tag_artists\", \"tag\"].edge_index = torch.load(path.join(data_folder, \"has_tag_artists.pt\"), weights_only=True).t().long()\n",
    "data[\"track\", \"has_tag_tracks\", \"tag\"].edge_index = torch.load(path.join(data_folder, \"has_tag_tracks.pt\"), weights_only=True).t().long()\n",
    "print(\"has_tag_artists index tensor shape:\", data[\"artist\", \"has_tag_artists\", \"tag\"].edge_index.shape)\n",
    "print(\"has_tag_tracks index tensor shape:\", data[\"track\", \"has_tag_tracks\", \"tag\"].edge_index.shape)\n",
    "\n",
    "data[\"artist\", \"last_fm_match\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"last_fm_match.pt\"), weights_only=True).t().long()\n",
    "data[\"artist\", \"last_fm_match\", \"artist\"].edge_attr = torch.load(path.join(data_folder, \"last_fm_match_attr.pt\"), weights_only=True)\n",
    "print(\"last_fm_match index tensor shape:\", data[\"artist\", \"last_fm_match\", \"artist\"].edge_index.shape)\n",
    "print(\"last_fm_match attr tensor shape:\", data[\"artist\", \"last_fm_match\", \"artist\"].edge_attr.shape)\n",
    "\n",
    "data[\"artist\", \"linked_to\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"linked_to.pt\"), weights_only=True).t().long()\n",
    "data[\"artist\", \"linked_to\", \"artist\"].edge_attr = torch.load(path.join(data_folder, \"linked_to_attr.pt\"), weights_only=True)\n",
    "print(\"linked_to index tensor shape:\", data[\"artist\", \"linked_to\", \"artist\"].edge_index.shape)\n",
    "print(\"linked_to attr tensor shape:\", data[\"artist\", \"linked_to\", \"artist\"].edge_attr.shape)\n",
    "\n",
    "data[\"artist\", \"musically_related_to\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"musically_related_to.pt\"), weights_only=True).t().long()\n",
    "data[\"artist\", \"musically_related_to\", \"artist\"].edge_attr = torch.load(path.join(data_folder, \"musically_related_to_attr.pt\"), weights_only=True)\n",
    "print(\"musically_related_to index tensor shape:\", data[\"artist\", \"musically_related_to\", \"artist\"].edge_index.shape)\n",
    "print(\"musically_related_to attr tensor shape:\", data[\"artist\", \"musically_related_to\", \"artist\"].edge_attr.shape)\n",
    "\n",
    "data[\"artist\", \"personally_related_to\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"personally_related_to.pt\"), weights_only=True).t().long()\n",
    "data[\"artist\", \"personally_related_to\", \"artist\"].edge_attr = torch.load(path.join(data_folder, \"personally_related_to_attr.pt\"), weights_only=True)\n",
    "print(\"personally_related_to index tensor shape:\", data[\"artist\", \"personally_related_to\", \"artist\"].edge_index.shape)\n",
    "print(\"personally_related_to attr tensor shape:\", data[\"artist\", \"personally_related_to\", \"artist\"].edge_attr.shape)\n",
    "\n",
    "data[\"tag\", \"tags_artists\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"tags_artists.pt\"), weights_only=True).t().long()\n",
    "data[\"tag\", \"tags_track\", \"track\"].edge_index = torch.load(path.join(data_folder, \"tags_tracks.pt\"), weights_only=True).t().long()\n",
    "print(\"tags_artists index tensor shape:\", data[\"tag\", \"tags_artists\", \"artist\"].edge_index.shape)\n",
    "print(\"tags_tracks index tensor shape:\", data[\"tag\", \"tags_track\", \"track\"].edge_index.shape)\n",
    "\n",
    "data[\"track\", \"worked_by\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"worked_by.pt\"), weights_only=True).t().long()\n",
    "data[\"artist\", \"worked_in\", \"track\"].edge_index = torch.load(path.join(data_folder, \"worked_in.pt\"), weights_only=True).t().long()\n",
    "print(\"worked_by index tensor shape:\", data[\"track\", \"worked_by\", \"artist\"].edge_index.shape)\n",
    "print(\"worked_in index tensor shape:\", data[\"artist\", \"worked_in\", \"track\"].edge_index.shape)\n",
    "\n",
    "print()\n",
    "\n",
    "data.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a33bd03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_type: ('artist', 'collab_with', 'artist')\n",
      "edge_type: ('artist', 'has_tag_artists', 'tag')\n",
      "No edge_attr for ('artist', 'has_tag_artists', 'tag')\n",
      "edge_type: ('track', 'has_tag_tracks', 'tag')\n",
      "No edge_attr for ('track', 'has_tag_tracks', 'tag')\n",
      "edge_type: ('artist', 'last_fm_match', 'artist')\n",
      "edge_type: ('artist', 'linked_to', 'artist')\n",
      "edge_type: ('artist', 'musically_related_to', 'artist')\n",
      "edge_type: ('artist', 'personally_related_to', 'artist')\n",
      "edge_type: ('tag', 'tags_artists', 'artist')\n",
      "No edge_attr for ('tag', 'tags_artists', 'artist')\n",
      "edge_type: ('tag', 'tags_track', 'track')\n",
      "No edge_attr for ('tag', 'tags_track', 'track')\n",
      "edge_type: ('track', 'worked_by', 'artist')\n",
      "No edge_attr for ('track', 'worked_by', 'artist')\n",
      "edge_type: ('artist', 'worked_in', 'track')\n",
      "No edge_attr for ('artist', 'worked_in', 'track')\n",
      "Edge type: ('artist', 'collab_with', 'artist'), edge_index shape: torch.Size([2, 629340])\n",
      "Edge type: ('artist', 'has_tag_artists', 'tag'), edge_index shape: torch.Size([2, 1042766])\n",
      "Edge type: ('track', 'has_tag_tracks', 'tag'), edge_index shape: torch.Size([2, 4030735])\n",
      "Edge type: ('artist', 'last_fm_match', 'artist'), edge_index shape: torch.Size([2, 28357816])\n",
      "Edge type: ('artist', 'linked_to', 'artist'), edge_index shape: torch.Size([2, 1438])\n",
      "Edge type: ('artist', 'musically_related_to', 'artist'), edge_index shape: torch.Size([2, 41760])\n",
      "Edge type: ('artist', 'personally_related_to', 'artist'), edge_index shape: torch.Size([2, 3334])\n",
      "Edge type: ('tag', 'tags_artists', 'artist'), edge_index shape: torch.Size([2, 1042766])\n",
      "Edge type: ('tag', 'tags_track', 'track'), edge_index shape: torch.Size([2, 4030735])\n",
      "Edge type: ('track', 'worked_by', 'artist'), edge_index shape: torch.Size([2, 12509457])\n",
      "Edge type: ('artist', 'worked_in', 'track'), edge_index shape: torch.Size([2, 12509457])\n",
      "Subgraph artist tensor shape: torch.Size([223388, 16])\n",
      "Subgraph track tensor shape: torch.Size([24324100, 4])\n",
      "Subgraph tag tensor shape: torch.Size([23, 1])\n",
      "\n",
      "\n",
      "Validation successful.\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL SUBGRAPH\n",
    "\n",
    "if True:\n",
    "\n",
    "    # Data\n",
    "    percentile = 0.85\n",
    "    artist_popularity = data[\"artist\"].x[:, 8]\n",
    "    edge_types = [\n",
    "        (\"artist\", \"collab_with\", \"artist\"),\n",
    "        (\"artist\", \"has_tag_artists\", \"tag\"),\n",
    "        (\"track\", \"has_tag_tracks\", \"tag\"),\n",
    "        (\"artist\", \"last_fm_match\", \"artist\"),\n",
    "        (\"artist\", \"linked_to\", \"artist\"),\n",
    "        (\"artist\", \"musically_related_to\", \"artist\"),\n",
    "        (\"artist\", \"personally_related_to\", \"artist\"),\n",
    "        (\"tag\", \"tags_artists\", \"artist\"),\n",
    "        (\"tag\", \"tags_track\", \"track\"),\n",
    "        (\"track\", \"worked_by\", \"artist\"),\n",
    "        (\"artist\", \"worked_in\", \"track\")\n",
    "    ]\n",
    "\n",
    "    # Threshold obtention\n",
    "    threshold = torch.quantile(artist_popularity, percentile)\n",
    "    selected_artists = artist_popularity >= threshold\n",
    "    selected_artist_ids = torch.nonzero(selected_artists).squeeze()\n",
    "\n",
    "    # Mapping\n",
    "    old_to_new_artist_idx = {old: new for new, old in enumerate(selected_artist_ids.tolist())}\n",
    "\n",
    "    # Subgraph\n",
    "    subdata = HeteroData()\n",
    "    for edge_type in edge_types:\n",
    "        print(f\"edge_type: {edge_type}\")\n",
    "        # Filter edge indices\n",
    "        edge_index = data[edge_type].edge_index\n",
    "        mask = torch.ones(edge_index.shape[1], dtype=torch.bool)\n",
    "        if edge_type[0] == \"artist\":\n",
    "            mask &= torch.isin(edge_index[0], selected_artist_ids)\n",
    "        if edge_type[2] == \"artist\":\n",
    "            mask &= torch.isin(edge_index[1], selected_artist_ids)\n",
    "\n",
    "        filtered_edge_index = edge_index[:, mask]\n",
    "\n",
    "        # Map the old indices to new ones for 'artist' nodes\n",
    "        if edge_type[0] == \"artist\":  # Reindex source node\n",
    "            filtered_edge_index[0] = torch.tensor(\n",
    "                [old_to_new_artist_idx[idx.item()] for idx in filtered_edge_index[0]],\n",
    "                dtype=torch.long,\n",
    "            )\n",
    "        if edge_type[2] == \"artist\":  # Reindex destination node\n",
    "            filtered_edge_index[1] = torch.tensor(\n",
    "                [old_to_new_artist_idx[idx.item()] for idx in filtered_edge_index[1]],\n",
    "                dtype=torch.long,\n",
    "            )\n",
    "\n",
    "        # Assign filtered edges to subgraph\n",
    "        subdata[edge_type].edge_index = filtered_edge_index\n",
    "\n",
    "        # Handle edge attributes if they exist\n",
    "        if hasattr(data[edge_type], \"edge_attr\"):\n",
    "            try:\n",
    "                subdata[edge_type].edge_attr = data[edge_type].edge_attr[mask]\n",
    "            except IndexError as e:\n",
    "                print(f\"IndexError for {edge_type}: {e}\")\n",
    "        else:\n",
    "            print(f\"No edge_attr for {edge_type}\")\n",
    "\n",
    "    # Nodes filtering\n",
    "    subdata[\"artist\"].x = data[\"artist\"].x[selected_artist_ids]\n",
    "    subdata[\"track\"].x = data[\"track\"].x\n",
    "    subdata[\"tag\"].x = data[\"tag\"].x\n",
    "\n",
    "    # Check the shape of the filtered nodes and edges\n",
    "    for edge_type in edge_types:\n",
    "        print(f\"Edge type: {edge_type}, edge_index shape: {subdata[edge_type].edge_index.shape}\")\n",
    "\n",
    "    # Check the artist features (should only have the selected artists)\n",
    "    print(\"Subgraph artist tensor shape:\", subdata[\"artist\"].x.shape)\n",
    "    print(\"Subgraph track tensor shape:\", subdata[\"track\"].x.shape)\n",
    "    print(\"Subgraph tag tensor shape:\", subdata[\"tag\"].x.shape)\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Validate the subgraph\n",
    "    try:\n",
    "        subdata.validate()\n",
    "        print(\"Validation successful.\")\n",
    "\n",
    "        del data\n",
    "        data = subdata\n",
    "    except ValueError as e:\n",
    "        print(\"Validation failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b839629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "==============\n",
      "HeteroData(\n",
      "  artist={ x=[223388, 16] },\n",
      "  track={ x=[24324100, 4] },\n",
      "  tag={ x=[23, 1] },\n",
      "  (artist, collab_with, artist)={\n",
      "    edge_index=[2, 157335],\n",
      "    edge_attr=[157335, 1],\n",
      "    edge_label=[157335],\n",
      "    edge_label_index=[2, 157335],\n",
      "  },\n",
      "  (artist, has_tag_artists, tag)={ edge_index=[2, 1042766] },\n",
      "  (track, has_tag_tracks, tag)={ edge_index=[2, 4030735] },\n",
      "  (artist, last_fm_match, artist)={\n",
      "    edge_index=[2, 28357816],\n",
      "    edge_attr=[28357816, 1],\n",
      "  },\n",
      "  (artist, linked_to, artist)={\n",
      "    edge_index=[2, 1438],\n",
      "    edge_attr=[1438, 1],\n",
      "  },\n",
      "  (artist, musically_related_to, artist)={\n",
      "    edge_index=[2, 41760],\n",
      "    edge_attr=[41760, 1],\n",
      "  },\n",
      "  (artist, personally_related_to, artist)={\n",
      "    edge_index=[2, 3334],\n",
      "    edge_attr=[3334, 1],\n",
      "  },\n",
      "  (tag, tags_artists, artist)={ edge_index=[2, 1042766] },\n",
      "  (tag, tags_track, track)={ edge_index=[2, 4030735] },\n",
      "  (track, worked_by, artist)={ edge_index=[2, 12509457] },\n",
      "  (artist, worked_in, track)={ edge_index=[2, 12509457] }\n",
      ")\n",
      "\n",
      "Validation data:\n",
      "================\n",
      "HeteroData(\n",
      "  artist={ x=[223388, 16] },\n",
      "  track={ x=[24324100, 4] },\n",
      "  tag={ x=[23, 1] },\n",
      "  (artist, collab_with, artist)={\n",
      "    edge_index=[2, 314670],\n",
      "    edge_attr=[314670, 1],\n",
      "    edge_label=[314670],\n",
      "    edge_label_index=[2, 314670],\n",
      "  },\n",
      "  (artist, has_tag_artists, tag)={ edge_index=[2, 1042766] },\n",
      "  (track, has_tag_tracks, tag)={ edge_index=[2, 4030735] },\n",
      "  (artist, last_fm_match, artist)={\n",
      "    edge_index=[2, 28357816],\n",
      "    edge_attr=[28357816, 1],\n",
      "  },\n",
      "  (artist, linked_to, artist)={\n",
      "    edge_index=[2, 1438],\n",
      "    edge_attr=[1438, 1],\n",
      "  },\n",
      "  (artist, musically_related_to, artist)={\n",
      "    edge_index=[2, 41760],\n",
      "    edge_attr=[41760, 1],\n",
      "  },\n",
      "  (artist, personally_related_to, artist)={\n",
      "    edge_index=[2, 3334],\n",
      "    edge_attr=[3334, 1],\n",
      "  },\n",
      "  (tag, tags_artists, artist)={ edge_index=[2, 1042766] },\n",
      "  (tag, tags_track, track)={ edge_index=[2, 4030735] },\n",
      "  (track, worked_by, artist)={ edge_index=[2, 12509457] },\n",
      "  (artist, worked_in, track)={ edge_index=[2, 12509457] }\n",
      ")\n",
      "\n",
      "Test data:\n",
      "================\n",
      "HeteroData(\n",
      "  artist={ x=[223388, 16] },\n",
      "  track={ x=[24324100, 4] },\n",
      "  tag={ x=[23, 1] },\n",
      "  (artist, collab_with, artist)={\n",
      "    edge_index=[2, 472005],\n",
      "    edge_attr=[472005, 1],\n",
      "    edge_label=[314670],\n",
      "    edge_label_index=[2, 314670],\n",
      "  },\n",
      "  (artist, has_tag_artists, tag)={ edge_index=[2, 1042766] },\n",
      "  (track, has_tag_tracks, tag)={ edge_index=[2, 4030735] },\n",
      "  (artist, last_fm_match, artist)={\n",
      "    edge_index=[2, 28357816],\n",
      "    edge_attr=[28357816, 1],\n",
      "  },\n",
      "  (artist, linked_to, artist)={\n",
      "    edge_index=[2, 1438],\n",
      "    edge_attr=[1438, 1],\n",
      "  },\n",
      "  (artist, musically_related_to, artist)={\n",
      "    edge_index=[2, 41760],\n",
      "    edge_attr=[41760, 1],\n",
      "  },\n",
      "  (artist, personally_related_to, artist)={\n",
      "    edge_index=[2, 3334],\n",
      "    edge_attr=[3334, 1],\n",
      "  },\n",
      "  (tag, tags_artists, artist)={ edge_index=[2, 1042766] },\n",
      "  (tag, tags_track, track)={ edge_index=[2, 4030735] },\n",
      "  (track, worked_by, artist)={ edge_index=[2, 12509457] },\n",
      "  (artist, worked_in, track)={ edge_index=[2, 12509457] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch_geometric.transforms as T\n",
    "\n",
    "transform = T.RandomLinkSplit(\n",
    "    num_val=0.25,\n",
    "    num_test=0.25,\n",
    "    disjoint_train_ratio=0.5,\n",
    "    neg_sampling_ratio=1,\n",
    "    add_negative_train_samples=False,\n",
    "    edge_types=(\"artist\", \"collab_with\", \"artist\"),\n",
    ")\n",
    "\n",
    "train_data, val_data, test_data = transform(data)\n",
    "\n",
    "print(\"Training data:\")\n",
    "print(\"==============\")\n",
    "print(train_data)\n",
    "print()\n",
    "print(\"Validation data:\")\n",
    "print(\"================\")\n",
    "print(val_data)\n",
    "print()\n",
    "print(\"Test data:\")\n",
    "print(\"================\")\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82faa4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: '{device}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "956bf7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train_loader...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleferu/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating val_loader...\n",
      "Creating test_loader...\n",
      "Sampling mini-batch...\n",
      "Sampled mini-batch:\n",
      "===================\n",
      "HeteroData(\n",
      "  artist={\n",
      "    x=[124069, 16],\n",
      "    n_id=[124069],\n",
      "  },\n",
      "  track={\n",
      "    x=[201198, 4],\n",
      "    n_id=[201198],\n",
      "  },\n",
      "  tag={\n",
      "    x=[23, 1],\n",
      "    n_id=[23],\n",
      "  },\n",
      "  (artist, collab_with, artist)={\n",
      "    edge_index=[2, 33146],\n",
      "    edge_attr=[33146, 1],\n",
      "    edge_label=[256],\n",
      "    edge_label_index=[2, 256],\n",
      "    e_id=[33146],\n",
      "    input_id=[128],\n",
      "  },\n",
      "  (artist, has_tag_artists, tag)={\n",
      "    edge_index=[2, 460],\n",
      "    e_id=[460],\n",
      "  },\n",
      "  (track, has_tag_tracks, tag)={\n",
      "    edge_index=[2, 460],\n",
      "    e_id=[460],\n",
      "  },\n",
      "  (artist, last_fm_match, artist)={\n",
      "    edge_index=[2, 272397],\n",
      "    edge_attr=[272397, 1],\n",
      "    e_id=[272397],\n",
      "  },\n",
      "  (artist, linked_to, artist)={\n",
      "    edge_index=[2, 206],\n",
      "    edge_attr=[206, 1],\n",
      "    e_id=[206],\n",
      "  },\n",
      "  (artist, musically_related_to, artist)={\n",
      "    edge_index=[2, 5089],\n",
      "    edge_attr=[5089, 1],\n",
      "    e_id=[5089],\n",
      "  },\n",
      "  (artist, personally_related_to, artist)={\n",
      "    edge_index=[2, 524],\n",
      "    edge_attr=[524, 1],\n",
      "    e_id=[524],\n",
      "  },\n",
      "  (tag, tags_artists, artist)={\n",
      "    edge_index=[2, 57991],\n",
      "    e_id=[57991],\n",
      "  },\n",
      "  (tag, tags_track, track)={\n",
      "    edge_index=[2, 2839],\n",
      "    e_id=[2839],\n",
      "  },\n",
      "  (track, worked_by, artist)={\n",
      "    edge_index=[2, 202267],\n",
      "    e_id=[202267],\n",
      "  },\n",
      "  (artist, worked_in, track)={\n",
      "    edge_index=[2, 12821],\n",
      "    e_id=[12821],\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "edge_label_index = train_data[\"artist\", \"collab_with\", \"artist\"].edge_label_index\n",
    "edge_label = train_data[\"artist\", \"collab_with\", \"artist\"].edge_label\n",
    "\n",
    "print(\"Creating train_loader...\")\n",
    "train_loader = LinkNeighborLoader(\n",
    "    data=train_data,\n",
    "    num_neighbors=[25, 20],\n",
    "    neg_sampling_ratio=1,\n",
    "    edge_label_index=((\"artist\", \"collab_with\", \"artist\"), edge_label_index),\n",
    "    edge_label=edge_label,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "edge_label_index = val_data[\"artist\", \"collab_with\", \"artist\"].edge_label_index\n",
    "edge_label = val_data[\"artist\", \"collab_with\", \"artist\"].edge_label\n",
    "\n",
    "print(\"Creating val_loader...\")\n",
    "val_loader = LinkNeighborLoader(\n",
    "    data=val_data,\n",
    "    num_neighbors=[25, 20],\n",
    "    edge_label_index=((\"artist\", \"collab_with\", \"artist\"), edge_label_index),\n",
    "    edge_label=edge_label,\n",
    "    batch_size=512,\n",
    "    shuffle=False,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "edge_label_index = test_data[\"artist\", \"collab_with\", \"artist\"].edge_label_index\n",
    "edge_label = test_data[\"artist\", \"collab_with\", \"artist\"].edge_label\n",
    "\n",
    "print(\"Creating test_loader...\")\n",
    "test_loader = LinkNeighborLoader(\n",
    "    data=test_data,\n",
    "    num_neighbors=[25, 20],\n",
    "    edge_label_index=((\"artist\", \"collab_with\", \"artist\"), edge_label_index),\n",
    "    edge_label=edge_label,\n",
    "    batch_size=512,\n",
    "    shuffle=False,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(\"Sampling mini-batch...\")\n",
    "\n",
    "sampled_data = next(iter(train_loader))\n",
    "\n",
    "print(\"Sampled mini-batch:\")\n",
    "print(\"===================\")\n",
    "print(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d3164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "if debug:\n",
    "    print(torch.unique(train_data['artist', 'collab_with', 'artist'].edge_label))\n",
    "    print(torch.unique(next(iter(train_loader))[\"artist\", \"collab_with\", \"artist\"].edge_label))\n",
    "    print(torch.unique(val_data['artist', 'collab_with', 'artist'].edge_label))\n",
    "    print(torch.unique(next(iter(val_loader))[\"artist\", \"collab_with\", \"artist\"].edge_label))\n",
    "    print(torch.unique(test_data['artist', 'collab_with', 'artist'].edge_label))\n",
    "    print(torch.unique(next(iter(test_loader))[\"artist\", \"collab_with\", \"artist\"].edge_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91d598ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import HeteroConv, GATConv, SAGEConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, metadata, out_channels):\n",
    "        super().__init__()\n",
    "        self.metadata = metadata\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.conv1 = HeteroConv({\n",
    "            (\"artist\", \"collab_with\", \"artist\"): GATConv((-1, -1), out_channels),\n",
    "            (\"artist\", \"has_tag_artists\", \"tag\"): SAGEConv((-1, -1), out_channels),\n",
    "            (\"artist\", \"last_fm_match\", \"artist\"): GATConv((-1, -1), out_channels),\n",
    "            (\"track\", \"has_tag_tracks\", \"tag\"): SAGEConv((-1, -1), out_channels),\n",
    "            (\"artist\", \"linked_to\", \"artist\"): GATConv((-1, -1), out_channels),\n",
    "            (\"artist\", \"musically_related_to\", \"artist\"): GATConv((-1, -1), out_channels),\n",
    "            (\"artist\", \"personally_related_to\", \"artist\"): GATConv((-1, -1), out_channels),\n",
    "            (\"tag\", \"tags_artists\", \"artist\"): SAGEConv((-1, -1), out_channels),\n",
    "            (\"tag\", \"tags_tracks\", \"track\"): SAGEConv((-1, -1), out_channels),\n",
    "            (\"track\", \"worked_by\", \"artist\"): SAGEConv((-1, -1), out_channels),\n",
    "            (\"artist\", \"worked_in\", \"track\"): SAGEConv((-1, -1), out_channels),\n",
    "        }, aggr=\"mean\")\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        norm_x_dict = {\n",
    "            key: F.normalize(\n",
    "                x,\n",
    "                p=2,\n",
    "                dim=-1\n",
    "            )\n",
    "            for key, x in x_dict.items()\n",
    "        }\n",
    "        return norm_x_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9525bb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for sampled_data in tqdm.tqdm(train_loader):\n",
    "            # Move data to device\n",
    "            sampled_data = sampled_data.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_dict = model(sampled_data.x_dict, sampled_data.edge_index_dict)\n",
    "            \n",
    "            # Get predictions and labels for the 'collab_with' edge type\n",
    "            edge_label_index = sampled_data['artist', 'collab_with', 'artist'].edge_label_index\n",
    "            edge_label = sampled_data['artist', 'collab_with', 'artist'].edge_label\n",
    "\n",
    "            src_emb = pred_dict['artist'][edge_label_index[0]]  # Source node embeddings\n",
    "            dst_emb = pred_dict['artist'][edge_label_index[1]]  # Destination node embeddings\n",
    "            \n",
    "            # Compute the dot product between source and destination embeddings\n",
    "            preds = (src_emb * dst_emb).sum(dim=-1)  # Scalar for each edge\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(preds, edge_label.float())\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Average loss for the epoch\n",
    "        epoch_loss /= len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # Validation metrics\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient computation for validation\n",
    "            for sampled_data in tqdm.tqdm(val_loader):\n",
    "                # Move data to device\n",
    "                sampled_data = sampled_data.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                pred_dict = model(sampled_data.x_dict, sampled_data.edge_index_dict)\n",
    "                \n",
    "                # Get predictions and labels for the 'collab_with' edge type\n",
    "                edge_label_index = sampled_data['artist', 'collab_with', 'artist'].edge_label_index\n",
    "                edge_label = sampled_data['artist', 'collab_with', 'artist'].edge_label\n",
    "\n",
    "                src_emb = pred_dict['artist'][edge_label_index[0]]  # Source node embeddings\n",
    "                dst_emb = pred_dict['artist'][edge_label_index[1]]  # Destination node embeddings\n",
    "                \n",
    "                # Compute the dot product between source and destination embeddings\n",
    "                preds = (src_emb * dst_emb).sum(dim=-1)  # Scalar for each edge\n",
    "\n",
    "                probs = torch.sigmoid(preds)  # Convert to probabilities\n",
    "\n",
    "                loss = criterion(preds, edge_label.float())\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Collect predictions, probabilities, and labels\n",
    "                all_labels.append(edge_label.cpu())\n",
    "                all_probs.append(probs.cpu())\n",
    "        \n",
    "        # Concatenate all predictions and labels\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        all_probs = torch.cat(all_probs)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Find threshold for predictions\n",
    "        best_threshold = 0\n",
    "        best_f1 = 0\n",
    "        for threshold in np.arange(0.2, 0.81, 0.01):\n",
    "            preds_binary = (all_probs > threshold).long()\n",
    "            cm = confusion_matrix(all_labels, preds_binary)\n",
    "            tp = cm[1, 1]\n",
    "            fp = cm[0, 1]\n",
    "            fn = cm[1, 0]\n",
    "            tn = cm[0, 0]\n",
    "            precision = 0 if tp == 0 else tp / (tp + fp)\n",
    "            recall = 0 if tp == 0 else tp / (tp + fn)\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "            if f1 > best_f1:\n",
    "                best_threshold = threshold\n",
    "                best_f1 = f1\n",
    "        print(f\"Best threshold: {best_threshold}\")\n",
    "        all_preds = (all_probs > best_threshold).long()\n",
    "        \n",
    "        # Compute metrics\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        tp = cm[1, 1]\n",
    "        fp = cm[0, 1]\n",
    "        fn = cm[1, 0]\n",
    "        tn = cm[0, 0]\n",
    "        accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        roc_auc = roc_auc_score(all_labels, all_probs)\n",
    "        \n",
    "        # Print validation metrics\n",
    "        print(f\"Validation Metrics - Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"Loss:      {val_loss:.4f}\")\n",
    "        print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall:    {recall:.4f}\")\n",
    "        print(f\"F1-score:  {f1:.4f}\")\n",
    "        print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{tp} {fn}\\n{fp} {tn}\")\n",
    "\n",
    "    return best_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13d6278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion, device, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for sampled_data in tqdm.tqdm(test_loader):\n",
    "            # Move data to the device\n",
    "            sampled_data = sampled_data.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            pred_dict = model(sampled_data.x_dict, sampled_data.edge_index_dict)\n",
    "\n",
    "            # Get predictions and labels for the 'collab_with' edge type\n",
    "            edge_label_index = sampled_data['artist', 'collab_with', 'artist'].edge_label_index\n",
    "            edge_label = sampled_data['artist', 'collab_with', 'artist'].edge_label\n",
    "\n",
    "            src_emb = pred_dict['artist'][edge_label_index[0]]  # Source node embeddings\n",
    "            dst_emb = pred_dict['artist'][edge_label_index[1]]  # Destination node embeddings\n",
    "            \n",
    "            # Compute the dot product between source and destination embeddings\n",
    "            preds = (src_emb * dst_emb).sum(dim=-1)  # Scalar for each edge\n",
    "            probs = torch.sigmoid(preds)  # Convert logits to probabilities\n",
    "            preds_binary = (probs > threshold).long()  # Convert probabilities to binary predictions\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(preds, edge_label.float())\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Collect predictions and labels\n",
    "            all_preds.append(preds_binary.cpu())\n",
    "            all_labels.append(edge_label.cpu())\n",
    "            all_probs.append(probs.cpu())\n",
    "\n",
    "    # Concatenate all predictions and labels\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    all_probs = torch.cat(all_probs)\n",
    "\n",
    "    # Compute metrics\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    tp = cm[1, 1]\n",
    "    fp = cm[0, 1]\n",
    "    fn = cm[1, 0]\n",
    "    tn = cm[0, 0]\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    roc_auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    # Average test loss\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    print(\"Test Results:\")\n",
    "    print(f\"Loss:      {test_loss:.4f}\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-score:  {f1:.4f}\")\n",
    "    print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{tp} {fn}\\n{fp} {tn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71d770d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1230/1230 [02:31<00:00,  8.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 0.6736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [02:20<00:00,  4.37it/s]\n",
      "/tmp/ipykernel_72737/2017333959.py:88: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  precision = tp / (tp + fp)\n",
      "/tmp/ipykernel_72737/2017333959.py:88: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  precision = tp / (tp + fp)\n",
      "/tmp/ipykernel_72737/2017333959.py:88: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  precision = tp / (tp + fp)\n",
      "/tmp/ipykernel_72737/2017333959.py:88: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  precision = tp / (tp + fp)\n",
      "/tmp/ipykernel_72737/2017333959.py:88: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  precision = tp / (tp + fp)\n",
      "/tmp/ipykernel_72737/2017333959.py:88: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  precision = tp / (tp + fp)\n",
      "/tmp/ipykernel_72737/2017333959.py:88: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  precision = tp / (tp + fp)\n",
      "/tmp/ipykernel_72737/2017333959.py:88: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  precision = tp / (tp + fp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.46000000000000024\n",
      "Validation Metrics - Epoch 1/20:\n",
      "Loss:      0.6551\n",
      "Accuracy:  0.6701\n",
      "Precision: 0.6082\n",
      "Recall:    0.9562\n",
      "F1-score:  0.7435\n",
      "ROC-AUC:   0.6488\n",
      "Confusion Matrix:\n",
      "150446 6889\n",
      "96928 60407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 600/1230 [01:17<01:03,  9.94it/s]"
     ]
    }
   ],
   "source": [
    "model = GNN(metadata=train_data.metadata(), out_channels=64).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "best_threshold = train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    F.binary_cross_entropy_with_logits,\n",
    "    device,\n",
    "    20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab218838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_threshold = train(\n",
    "#     model,\n",
    "#     train_loader,\n",
    "#     val_loader,\n",
    "#     optimizer,\n",
    "#     F.binary_cross_entropy_with_logits,\n",
    "#     device,\n",
    "#     100\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4561c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 670/670 [02:01<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Loss:      0.6480\n",
      "Accuracy:  0.7994\n",
      "Precision: 0.6310\n",
      "Recall:    0.9590\n",
      "F1-score:  0.7612\n",
      "ROC-AUC:   0.8279\n",
      "Confusion Matrix:\n",
      "109534 4683\n",
      "64046 164388\n"
     ]
    }
   ],
   "source": [
    "test_model(\n",
    "    model,\n",
    "    test_loader,\n",
    "    F.binary_cross_entropy_with_logits,\n",
    "    device,\n",
    "    best_threshold\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicbrainz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
