{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31d33376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear, ReLU, Sequential\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.nn import HeteroConv, GATConv, SAGEConv, Linear\n",
    "from torch_geometric.nn.aggr import Aggregation, MultiAggregation\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.typing import OptPairTensor, Adj, Size\n",
    "import os.path as path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import tqdm\n",
    "from typing import Optional, Union, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e12322ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"ds/\"\n",
    "model_name = \"main_mb\"\n",
    "year = 2023\n",
    "month = 11\n",
    "perc = 0\n",
    "train_hd = f\"train_hdmb_{year}_{month}_{perc}.pt\"\n",
    "# train_hd = f\"train_hd_{year}_{month}_{perc}.pt\"\n",
    "# train_hd = f\"train_hd_nomatch_{year}_{month}_{perc}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4d559d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest epoch: 0\n",
      "Is trained? False\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.read_csv(\"results.csv\", dtype={\n",
    "    \"model\": str,\n",
    "    \"year\": int,\n",
    "    \"month\": int,\n",
    "    \"perc\": float,\n",
    "    \"epoch\": int,\n",
    "    \"train_loss\": float,\n",
    "    \"val_loss\": float,\n",
    "    \"acc\": float,\n",
    "    \"prec\": float,\n",
    "    \"rec\": float,\n",
    "    \"f1\": float,\n",
    "    \"auc\": float,\n",
    "    \"tp\": int,\n",
    "    \"fp\": int,\n",
    "    \"fn\": int,\n",
    "    \"tn\": int,\n",
    "    \"best_threshold\": float,\n",
    "    \"done\": bool\n",
    "})\n",
    "\n",
    "filtered_df = results_df[\n",
    "    (results_df[\"model\"] == model_name) &\n",
    "    (results_df[\"year\"] == year) &\n",
    "    (results_df[\"month\"] == month) &\n",
    "    (results_df[\"perc\"] == perc)\n",
    "]\n",
    "\n",
    "if filtered_df.empty:\n",
    "    latest_epoch = 0\n",
    "    is_trained = False\n",
    "else:\n",
    "    latest_epoch = filtered_df[\"epoch\"].max()\n",
    "    is_trained = filtered_df[\"done\"].any()\n",
    "\n",
    "print(\"Latest epoch:\", latest_epoch)\n",
    "print(\"Is trained?\", is_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdb5b1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44189/2815972150.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(path.join(data_folder, train_hd))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.load(path.join(data_folder, train_hd))\n",
    "\n",
    "data.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd6e6398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artist channels: 15\n",
      "Track channels: 4\n",
      "Tag channels: 24\n"
     ]
    }
   ],
   "source": [
    "artist_channels = data[\"artist\"].x.size(1)\n",
    "track_channels = data[\"track\"].x.size(1)\n",
    "tag_channels = data[\"tag\"].x.size(1)\n",
    "\n",
    "print(f\"Artist channels: {artist_channels}\")\n",
    "print(f\"Track channels: {track_channels}\")\n",
    "print(f\"Tag channels: {tag_channels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82faa4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data[\"artist\", \"collab_with\", \"artist\"].edge_index = data[\"artist\", \"collab_with\", \"artist\"].edge_index.contiguous()\n",
    "\n",
    "print(f\"Device: '{device}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "956bf7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train_loader...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleferu/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    }
   ],
   "source": [
    "compt_tree_size = [25, 20]\n",
    "\n",
    "print(\"Creating train_loader...\")\n",
    "train_loader = LinkNeighborLoader(\n",
    "    data=data,\n",
    "    num_neighbors=compt_tree_size,\n",
    "    neg_sampling_ratio=1,\n",
    "    edge_label_index=(\"artist\", \"collab_with\", \"artist\"),\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e2d1165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14572\n",
      "3643\n"
     ]
    }
   ],
   "source": [
    "train_count = int(0.8 * len(train_loader))\n",
    "val_count = len(train_loader) - train_count\n",
    "\n",
    "print(train_count)\n",
    "print(val_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfd307ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPSAGEConv(MessagePassing):\n",
    "    r\"\"\"The GraphSAGE operator with an MLP instead of the linear transformation.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int or tuple): Size of each input sample, or :obj:`-1` to\n",
    "            derive the size from the first input(s) to the forward method.\n",
    "            A tuple corresponds to the sizes of source and target\n",
    "            dimensionalities.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        hidden_channels (int, optional): Size of the hidden layer in the MLP.\n",
    "            (default: :obj:`64`)\n",
    "        aggr (str or Aggregation, optional): The aggregation scheme to use.\n",
    "            Any aggregation of :obj:`torch_geometric.nn.aggr` can be used,\n",
    "            *e.g.*, :obj:`\"mean\"`, :obj:`\"max\"`, or :obj:`\"lstm\"`.\n",
    "            (default: :obj:`\"mean\"`)\n",
    "        normalize (bool, optional): If set to :obj:`True`, output features\n",
    "            will be :math:`\\ell_2`-normalized, *i.e.*,\n",
    "            :math:`\\frac{\\mathbf{x}^{\\prime}_i}\n",
    "            {\\| \\mathbf{x}^{\\prime}_i \\|_2}`.\n",
    "            (default: :obj:`False`)\n",
    "        root_weight (bool, optional): If set to :obj:`False`, the layer will\n",
    "            not add transformed root node features to the output.\n",
    "            (default: :obj:`True`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "\n",
    "    Shapes:\n",
    "        - **inputs:**\n",
    "          node features :math:`(|\\mathcal{V}|, F_{in})` or\n",
    "          :math:`((|\\mathcal{V_s}|, F_{s}), (|\\mathcal{V_t}|, F_{t}))`\n",
    "          if bipartite,\n",
    "          edge indices :math:`(2, |\\mathcal{E}|)`\n",
    "        - **outputs:** node features :math:`(|\\mathcal{V}|, F_{out})` or\n",
    "          :math:`(|\\mathcal{V_t}|, F_{out})` if bipartite\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: Union[int, Tuple[int, int]],\n",
    "        out_channels: int,\n",
    "        hidden_channels: int = 64,\n",
    "        aggr: Optional[Union[str, List[str], Aggregation]] = \"mean\",\n",
    "        normalize: bool = False,\n",
    "        root_weight: bool = True,\n",
    "        bias: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.normalize = normalize\n",
    "        self.root_weight = root_weight\n",
    "\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "\n",
    "        if aggr == \"lstm\":\n",
    "            kwargs.setdefault(\"aggr_kwargs\", {})\n",
    "            kwargs[\"aggr_kwargs\"].setdefault(\"in_channels\", in_channels[0])\n",
    "            kwargs[\"aggr_kwargs\"].setdefault(\"out_channels\", in_channels[0])\n",
    "\n",
    "        super().__init__(aggr, **kwargs)\n",
    "\n",
    "\n",
    "        self.mlp = Sequential(\n",
    "            Linear(in_channels[0], hidden_channels),\n",
    "            ReLU(),\n",
    "            Linear(hidden_channels, in_channels[0]) # Output size should match input for aggregation\n",
    "        )\n",
    "\n",
    "        if isinstance(self.aggr_module, MultiAggregation):\n",
    "            aggr_out_channels = self.aggr_module.get_out_channels(in_channels[0])\n",
    "        else:\n",
    "            aggr_out_channels = in_channels[0]\n",
    "\n",
    "        self.lin_l = Linear(aggr_out_channels, out_channels, bias=bias)\n",
    "        if self.root_weight:\n",
    "            self.lin_r = Linear(in_channels[1], out_channels, bias=False)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()\n",
    "        for layer in self.mlp:\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()\n",
    "        self.lin_l.reset_parameters()\n",
    "        if self.root_weight:\n",
    "            self.lin_r.reset_parameters()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Union[Tensor, OptPairTensor],\n",
    "        edge_index: Adj,\n",
    "        size: Size = None,\n",
    "    ) -> Tensor:\n",
    "\n",
    "        if isinstance(x, Tensor):\n",
    "            x = (x, x)\n",
    "\n",
    "        # Propagate through MLP\n",
    "        x = (self.mlp(x[0]), x[1])\n",
    "\n",
    "        # propagate_type: (x: OptPairTensor)\n",
    "        out = self.propagate(edge_index, x=x, size=size)\n",
    "        out = self.lin_l(out)\n",
    "\n",
    "        x_r = x[1]\n",
    "        if self.root_weight and x_r is not None:\n",
    "            out = out + self.lin_r(x_r)\n",
    "\n",
    "        if self.normalize:\n",
    "            out = F.normalize(out, p=2.0, dim=-1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j: Tensor) -> Tensor:\n",
    "        return x_j\n",
    "\n",
    "    def message_and_aggregate(self, adj_t: Adj, x: OptPairTensor) -> Tensor:\n",
    "        if isinstance(adj_t, SparseTensor):\n",
    "            adj_t = adj_t.set_value(None, layout=None)\n",
    "        return spmm(adj_t, x[0], reduce=self.aggr)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"{self.__class__.__name__}({self.in_channels}, \"\n",
    "            f\"{self.out_channels}, hidden_channels={self.hidden_channels}, aggr={self.aggr})\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91d598ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, metadata, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.metadata = metadata\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.conv1 = HeteroConv({\n",
    "            (\"artist\", \"collab_with\", \"artist\"): GATConv((artist_channels, artist_channels), hidden_channels),\n",
    "            (\"artist\", \"has_tag_artists\", \"tag\"): SAGEConv((artist_channels, tag_channels), hidden_channels),\n",
    "            # (\"artist\", \"last_fm_match\", \"artist\"): GATConv((artist_channels, artist_channels), hidden_channels),\n",
    "            (\"track\", \"has_tag_tracks\", \"tag\"): SAGEConv((track_channels, tag_channels), hidden_channels),\n",
    "            (\"artist\", \"linked_to\", \"artist\"): GATConv((artist_channels, artist_channels), hidden_channels),\n",
    "            (\"artist\", \"musically_related_to\", \"artist\"): GATConv((artist_channels, artist_channels), hidden_channels),\n",
    "            (\"artist\", \"personally_related_to\", \"artist\"): GATConv((artist_channels, artist_channels), hidden_channels),\n",
    "            (\"tag\", \"tags_artists\", \"artist\"): SAGEConv((tag_channels, artist_channels), hidden_channels),\n",
    "            (\"tag\", \"tags_tracks\", \"track\"): SAGEConv((tag_channels, track_channels), hidden_channels),\n",
    "            (\"track\", \"worked_by\", \"artist\"): SAGEConv((track_channels, artist_channels), hidden_channels),\n",
    "            (\"artist\", \"worked_in\", \"track\"): SAGEConv((artist_channels, track_channels), hidden_channels),\n",
    "        }, aggr=\"mean\")\n",
    "\n",
    "        self.conv2 = HeteroConv({\n",
    "            (\"artist\", \"collab_with\", \"artist\"): GATConv((hidden_channels, hidden_channels), hidden_channels),\n",
    "            (\"artist\", \"has_tag_artists\", \"tag\"): SAGEConv((hidden_channels, hidden_channels), hidden_channels),\n",
    "            # (\"artist\", \"last_fm_match\", \"artist\"): GATConv((hidden_channels, hidden_channels), hidden_channels),\n",
    "            (\"track\", \"has_tag_tracks\", \"tag\"): SAGEConv((hidden_channels, hidden_channels), hidden_channels),\n",
    "            (\"artist\", \"linked_to\", \"artist\"): GATConv((hidden_channels, hidden_channels), hidden_channels),\n",
    "            (\"artist\", \"musically_related_to\", \"artist\"): GATConv((hidden_channels, hidden_channels), hidden_channels),\n",
    "            (\"artist\", \"personally_related_to\", \"artist\"): GATConv((hidden_channels, hidden_channels), hidden_channels),\n",
    "            (\"tag\", \"tags_artists\", \"artist\"): SAGEConv((hidden_channels, hidden_channels), hidden_channels),\n",
    "            (\"tag\", \"tags_tracks\", \"track\"): SAGEConv((hidden_channels, hidden_channels), hidden_channels),\n",
    "            (\"track\", \"worked_by\", \"artist\"): SAGEConv((hidden_channels, hidden_channels), hidden_channels),\n",
    "            (\"artist\", \"worked_in\", \"track\"): SAGEConv((hidden_channels, hidden_channels), hidden_channels),\n",
    "        }, aggr=\"mean\")\n",
    "\n",
    "        self.linear1 = Linear(hidden_channels * 2, hidden_channels * 4)\n",
    "        self.linear2 = Linear(hidden_channels * 4, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict1 = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict2 = self.conv2(x_dict1, edge_index_dict)\n",
    "\n",
    "        x_artist = torch.cat([x_dict1['artist'], x_dict2['artist']], dim=-1)\n",
    "\n",
    "        x_artist = self.linear1(x_artist)\n",
    "        x_artist = self.linear2(x_artist)\n",
    "\n",
    "        # Normalize the artist node features\n",
    "        x_artist = F.normalize(x_artist, p=2, dim=-1)\n",
    "\n",
    "        # Update the dictionary with the new 'artist' features, leaving other nodes unchanged\n",
    "        x_dict['artist'] = x_artist\n",
    "\n",
    "        return x_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9525bb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, device, num_epochs, results_df, patience=5):\n",
    "    best_val_f1 = 0.0\n",
    "    best_threshold = 0\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "    train_losses = list()\n",
    "    val_losses = list()\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        epoch_loss = 0.0\n",
    "        train_loader_iter = iter(train_loader)\n",
    "        \n",
    "        for i in tqdm.tqdm(range(train_count)):\n",
    "            # Move data to device\n",
    "            sampled_data = next(train_loader_iter).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_dict = model(sampled_data.x_dict, sampled_data.edge_index_dict)\n",
    "            \n",
    "            # Get predictions and labels for the 'collab_with' edge type\n",
    "            edge_label_index = sampled_data['artist', 'collab_with', 'artist'].edge_label_index\n",
    "            edge_label = sampled_data['artist', 'collab_with', 'artist'].edge_label\n",
    "\n",
    "            src_emb = pred_dict['artist'][edge_label_index[0]]  # Source node embeddings\n",
    "            dst_emb = pred_dict['artist'][edge_label_index[1]]  # Destination node embeddings\n",
    "            \n",
    "            # Compute the dot product between source and destination embeddings\n",
    "            preds = (src_emb * dst_emb).sum(dim=-1)  # Scalar for each edge\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(preds, edge_label.float())\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Average loss for the epoch\n",
    "        epoch_loss /= train_count\n",
    "        train_losses.append(epoch_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        print(\"Computing validation metrics\")\n",
    "        \n",
    "        # Validation metrics\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient computation for validation\n",
    "            for i in tqdm.tqdm(range(val_count)):\n",
    "                # Move data to device\n",
    "                sampled_data = next(train_loader_iter).to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                pred_dict = model(sampled_data.x_dict, sampled_data.edge_index_dict)\n",
    "                \n",
    "                # Get predictions and labels for the 'collab_with' edge type\n",
    "                edge_label_index = sampled_data['artist', 'collab_with', 'artist'].edge_label_index\n",
    "                edge_label = sampled_data['artist', 'collab_with', 'artist'].edge_label\n",
    "\n",
    "                src_emb = pred_dict['artist'][edge_label_index[0]]  # Source node embeddings\n",
    "                dst_emb = pred_dict['artist'][edge_label_index[1]]  # Destination node embeddings\n",
    "                \n",
    "                # Compute the dot product between source and destination embeddings\n",
    "                preds = (src_emb * dst_emb).sum(dim=-1)  # Scalar for each edge\n",
    "\n",
    "                loss = criterion(preds, edge_label.float())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                probs = torch.sigmoid(preds)  # Convert to probabilities\n",
    "                \n",
    "                # Collect predictions, probabilities, and labels\n",
    "                all_labels.append(edge_label.cpu())\n",
    "                all_probs.append(probs.cpu())\n",
    "        \n",
    "        # Concatenate all predictions and labels\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        all_probs = torch.cat(all_probs)\n",
    "\n",
    "        val_loss /= val_count\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Find threshold for predictions\n",
    "        print(\"Looking for threshold\")\n",
    "        best_threshold_epoch = 0\n",
    "        best_f1_epoch = 0\n",
    "        for threshold in tqdm.tqdm(np.arange(0.2, 0.91, 0.01)):\n",
    "            preds_binary = (all_probs > threshold).long()\n",
    "            cm = confusion_matrix(all_labels, preds_binary)\n",
    "            tp = cm[1, 1]\n",
    "            fp = cm[0, 1]\n",
    "            fn = cm[1, 0]\n",
    "            tn = cm[0, 0]\n",
    "            precision = 0 if tp == 0 else tp / (tp + fp)\n",
    "            recall = 0 if tp == 0 else tp / (tp + fn)\n",
    "            f1 = 0 if precision * recall == 0 else 2 * precision * recall / (precision + recall)\n",
    "            if f1 > best_f1_epoch:\n",
    "                best_threshold_epoch = threshold\n",
    "                best_f1_epoch = f1\n",
    "        print(f\"Best threshold: {best_threshold_epoch}\")\n",
    "        all_preds = (all_probs > best_threshold_epoch).long()\n",
    "        \n",
    "        # Compute metrics\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        tp = cm[1, 1]\n",
    "        fp = cm[0, 1]\n",
    "        fn = cm[1, 0]\n",
    "        tn = cm[0, 0]\n",
    "        accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        roc_auc = roc_auc_score(all_labels, all_probs)\n",
    "        \n",
    "        # Print validation metrics\n",
    "        print(f\"Validation Metrics - Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"Loss:      {val_loss:.4f}\")\n",
    "        print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall:    {recall:.4f}\")\n",
    "        print(f\"F1-score:  {f1:.4f}\")\n",
    "        print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{tp} {fn}\\n{fp} {tn}\")\n",
    "\n",
    "        new_row = pd.DataFrame(\n",
    "            {\n",
    "                \"model\": [model_name],\n",
    "                \"year\": [year],\n",
    "                \"month\": [month],\n",
    "                \"perc\": [perc],\n",
    "                \"epoch\": [latest_epoch + epoch + 1],\n",
    "                \"train_loss\": [epoch_loss],\n",
    "                \"val_loss\": [val_loss],\n",
    "                \"acc\": [accuracy],\n",
    "                \"prec\": [precision],\n",
    "                \"rec\": [recall],\n",
    "                \"f1\": [f1],\n",
    "                \"auc\": [roc_auc],\n",
    "                \"tp\": [tp],\n",
    "                \"fp\": [fp],\n",
    "                \"fn\": [fn],\n",
    "                \"tn\": [tn],\n",
    "                \"best_threshold\": [best_threshold_epoch],\n",
    "                \"done\": [False]\n",
    "            }\n",
    "        )\n",
    "        results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "        results_df.to_csv(\"results.csv\", index=False)\n",
    "\n",
    "        torch.save(model.state_dict(), f\"./model_{model_name}_{year}_{month}_{perc}_{latest_epoch + epoch + 1}.pth\")\n",
    "\n",
    "        if f1 > best_val_f1:\n",
    "            best_val_f1 = f1\n",
    "            best_threshold = best_threshold_epoch\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = latest_epoch + epoch + 1\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print(f\"Early stopping!!!\")\n",
    "                print(f\"Early stopping!!!\")\n",
    "                print(f\"Early stopping!!!\")\n",
    "                print(\"Best epoch:\", best_epoch)\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "\n",
    "    return best_threshold, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c71d770d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14572/14572 [07:13<00:00, 33.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 0.5775\n",
      "Computing validation metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3643/3643 [01:17<00:00, 46.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [02:05<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.7000000000000004\n",
      "Validation Metrics - Epoch 1/100:\n",
      "Loss:      0.5699\n",
      "Accuracy:  0.7848\n",
      "Precision: 0.7131\n",
      "Recall:    0.9531\n",
      "F1-score:  0.8158\n",
      "ROC-AUC:   0.6990\n",
      "Confusion Matrix:\n",
      "444360 21878\n",
      "178818 287420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14572/14572 [07:12<00:00, 33.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, Training Loss: 0.5698\n",
      "Computing validation metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3643/3643 [01:18<00:00, 46.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [02:04<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.6900000000000004\n",
      "Validation Metrics - Epoch 2/100:\n",
      "Loss:      0.5699\n",
      "Accuracy:  0.7822\n",
      "Precision: 0.7087\n",
      "Recall:    0.9582\n",
      "F1-score:  0.8148\n",
      "ROC-AUC:   0.7111\n",
      "Confusion Matrix:\n",
      "446757 19481\n",
      "183617 282621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14572/14572 [07:11<00:00, 33.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, Training Loss: 0.5674\n",
      "Computing validation metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3643/3643 [01:18<00:00, 46.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [02:03<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.7100000000000004\n",
      "Validation Metrics - Epoch 3/100:\n",
      "Loss:      0.5538\n",
      "Accuracy:  0.8663\n",
      "Precision: 0.8276\n",
      "Recall:    0.9255\n",
      "F1-score:  0.8738\n",
      "ROC-AUC:   0.8873\n",
      "Confusion Matrix:\n",
      "431484 34754\n",
      "89890 376348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14572/14572 [07:11<00:00, 33.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, Training Loss: 0.5459\n",
      "Computing validation metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3643/3643 [01:19<00:00, 46.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [02:03<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.7100000000000004\n",
      "Validation Metrics - Epoch 4/100:\n",
      "Loss:      0.5471\n",
      "Accuracy:  0.8747\n",
      "Precision: 0.8374\n",
      "Recall:    0.9299\n",
      "F1-score:  0.8813\n",
      "ROC-AUC:   0.8893\n",
      "Confusion Matrix:\n",
      "433572 32666\n",
      "84177 382061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14572/14572 [07:11<00:00, 33.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Training Loss: 0.5440\n",
      "Computing validation metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3643/3643 [01:19<00:00, 46.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [02:04<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.7000000000000004\n",
      "Validation Metrics - Epoch 5/100:\n",
      "Loss:      0.5426\n",
      "Accuracy:  0.8756\n",
      "Precision: 0.8280\n",
      "Recall:    0.9482\n",
      "F1-score:  0.8840\n",
      "ROC-AUC:   0.8954\n",
      "Confusion Matrix:\n",
      "442089 24149\n",
      "91866 374372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14572/14572 [07:11<00:00, 33.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100, Training Loss: 0.5431\n",
      "Computing validation metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3643/3643 [01:18<00:00, 46.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [02:04<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.7100000000000004\n",
      "Validation Metrics - Epoch 6/100:\n",
      "Loss:      0.5426\n",
      "Accuracy:  0.8765\n",
      "Precision: 0.8352\n",
      "Recall:    0.9381\n",
      "F1-score:  0.8836\n",
      "ROC-AUC:   0.8959\n",
      "Confusion Matrix:\n",
      "437356 28882\n",
      "86309 379929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14572/14572 [07:12<00:00, 33.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100, Training Loss: 0.5410\n",
      "Computing validation metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3643/3643 [01:19<00:00, 46.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [02:03<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.7000000000000004\n",
      "Validation Metrics - Epoch 7/100:\n",
      "Loss:      0.5434\n",
      "Accuracy:  0.8835\n",
      "Precision: 0.8440\n",
      "Recall:    0.9407\n",
      "F1-score:  0.8898\n",
      "ROC-AUC:   0.9016\n",
      "Confusion Matrix:\n",
      "438606 27632\n",
      "81041 385197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14572/14572 [07:12<00:00, 33.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100, Training Loss: 0.5379\n",
      "Computing validation metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3643/3643 [01:18<00:00, 46.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [02:03<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.7000000000000004\n",
      "Validation Metrics - Epoch 8/100:\n",
      "Loss:      0.5383\n",
      "Accuracy:  0.8809\n",
      "Precision: 0.8408\n",
      "Recall:    0.9399\n",
      "F1-score:  0.8876\n",
      "ROC-AUC:   0.9075\n",
      "Confusion Matrix:\n",
      "438209 28029\n",
      "82983 383255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14572/14572 [07:12<00:00, 33.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100, Training Loss: 0.5368\n",
      "Computing validation metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3643/3643 [01:18<00:00, 46.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [02:03<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.7000000000000004\n",
      "Validation Metrics - Epoch 9/100:\n",
      "Loss:      0.5357\n",
      "Accuracy:  0.8849\n",
      "Precision: 0.8511\n",
      "Recall:    0.9331\n",
      "F1-score:  0.8902\n",
      "ROC-AUC:   0.9073\n",
      "Confusion Matrix:\n",
      "435039 31199\n",
      "76101 390137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14572/14572 [07:12<00:00, 33.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Training Loss: 0.5361\n",
      "Computing validation metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3643/3643 [01:18<00:00, 46.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [02:04<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.7000000000000004\n",
      "Validation Metrics - Epoch 10/100:\n",
      "Loss:      0.5360\n",
      "Accuracy:  0.8833\n",
      "Precision: 0.8474\n",
      "Recall:    0.9351\n",
      "F1-score:  0.8891\n",
      "ROC-AUC:   0.9100\n",
      "Confusion Matrix:\n",
      "435968 30270\n",
      "78504 387734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14572/14572 [07:10<00:00, 33.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100, Training Loss: 0.5354\n",
      "Computing validation metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3643/3643 [01:18<00:00, 46.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [02:03<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.7000000000000004\n",
      "Validation Metrics - Epoch 11/100:\n",
      "Loss:      0.5350\n",
      "Accuracy:  0.8858\n",
      "Precision: 0.8498\n",
      "Recall:    0.9373\n",
      "F1-score:  0.8914\n",
      "ROC-AUC:   0.9121\n",
      "Confusion Matrix:\n",
      "436990 29248\n",
      "77215 389023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14572/14572 [07:12<00:00, 33.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100, Training Loss: 0.5353\n",
      "Computing validation metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3643/3643 [01:18<00:00, 46.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [02:03<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.7000000000000004\n",
      "Validation Metrics - Epoch 12/100:\n",
      "Loss:      0.5340\n",
      "Accuracy:  0.8875\n",
      "Precision: 0.8482\n",
      "Recall:    0.9440\n",
      "F1-score:  0.8936\n",
      "ROC-AUC:   0.9146\n",
      "Confusion Matrix:\n",
      "440115 26123\n",
      "78740 387498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14572/14572 [07:13<00:00, 33.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100, Training Loss: 0.5348\n",
      "Computing validation metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3643/3643 [01:18<00:00, 46.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [02:04<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.7000000000000004\n",
      "Validation Metrics - Epoch 13/100:\n",
      "Loss:      0.5336\n",
      "Accuracy:  0.8830\n",
      "Precision: 0.8432\n",
      "Recall:    0.9409\n",
      "F1-score:  0.8894\n",
      "ROC-AUC:   0.9143\n",
      "Confusion Matrix:\n",
      "438675 27563\n",
      "81554 384684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14572/14572 [07:12<00:00, 33.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100, Training Loss: 0.5342\n",
      "Computing validation metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3643/3643 [01:18<00:00, 46.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [02:04<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.7000000000000004\n",
      "Validation Metrics - Epoch 14/100:\n",
      "Loss:      0.5340\n",
      "Accuracy:  0.8828\n",
      "Precision: 0.8431\n",
      "Recall:    0.9406\n",
      "F1-score:  0.8892\n",
      "ROC-AUC:   0.9150\n",
      "Confusion Matrix:\n",
      "438537 27701\n",
      "81593 384645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14572/14572 [07:13<00:00, 33.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100, Training Loss: 0.5341\n",
      "Computing validation metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3643/3643 [01:18<00:00, 46.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [02:03<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.7000000000000004\n",
      "Validation Metrics - Epoch 15/100:\n",
      "Loss:      0.5334\n",
      "Accuracy:  0.8906\n",
      "Precision: 0.8530\n",
      "Recall:    0.9438\n",
      "F1-score:  0.8961\n",
      "ROC-AUC:   0.9198\n",
      "Confusion Matrix:\n",
      "440031 26207\n",
      "75830 390408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14572/14572 [07:13<00:00, 33.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100, Training Loss: 0.5336\n",
      "Computing validation metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3643/3643 [01:25<00:00, 42.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [02:08<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.7000000000000004\n",
      "Validation Metrics - Epoch 16/100:\n",
      "Loss:      0.5332\n",
      "Accuracy:  0.8877\n",
      "Precision: 0.8515\n",
      "Recall:    0.9391\n",
      "F1-score:  0.8932\n",
      "ROC-AUC:   0.9227\n",
      "Confusion Matrix:\n",
      "437852 28386\n",
      "76365 389873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14572/14572 [07:58<00:00, 30.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100, Training Loss: 0.5331\n",
      "Computing validation metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3643/3643 [01:42<00:00, 35.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [02:13<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.7100000000000004\n",
      "Validation Metrics - Epoch 17/100:\n",
      "Loss:      0.5322\n",
      "Accuracy:  0.8912\n",
      "Precision: 0.8694\n",
      "Recall:    0.9206\n",
      "F1-score:  0.8943\n",
      "ROC-AUC:   0.9234\n",
      "Confusion Matrix:\n",
      "429227 37011\n",
      "64471 401767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14572/14572 [08:06<00:00, 29.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100, Training Loss: 0.5329\n",
      "Computing validation metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3643/3643 [01:35<00:00, 38.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [02:12<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.7100000000000004\n",
      "Validation Metrics - Epoch 18/100:\n",
      "Loss:      0.5331\n",
      "Accuracy:  0.8911\n",
      "Precision: 0.8599\n",
      "Recall:    0.9346\n",
      "F1-score:  0.8957\n",
      "ROC-AUC:   0.9254\n",
      "Confusion Matrix:\n",
      "435733 30505\n",
      "71007 395231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14572/14572 [07:42<00:00, 31.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100, Training Loss: 0.5323\n",
      "Computing validation metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3643/3643 [01:26<00:00, 42.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [02:10<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.6600000000000004\n",
      "Validation Metrics - Epoch 19/100:\n",
      "Loss:      0.5317\n",
      "Accuracy:  0.8801\n",
      "Precision: 0.8302\n",
      "Recall:    0.9558\n",
      "F1-score:  0.8886\n",
      "ROC-AUC:   0.9241\n",
      "Confusion Matrix:\n",
      "445635 20603\n",
      "91157 375081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14572/14572 [07:54<00:00, 30.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100, Training Loss: 0.5321\n",
      "Computing validation metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3643/3643 [01:32<00:00, 39.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [02:12<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.6200000000000003\n",
      "Validation Metrics - Epoch 20/100:\n",
      "Loss:      0.5313\n",
      "Accuracy:  0.8791\n",
      "Precision: 0.8238\n",
      "Recall:    0.9643\n",
      "F1-score:  0.8886\n",
      "ROC-AUC:   0.9247\n",
      "Confusion Matrix:\n",
      "449608 16630\n",
      "96141 370097\n",
      "Early stopping!!!\n",
      "Early stopping!!!\n",
      "Early stopping!!!\n",
      "Best epoch: 15\n"
     ]
    }
   ],
   "source": [
    "model = GNN(metadata=data.metadata(), hidden_channels=64, out_channels=64).to(device)\n",
    "\n",
    "# model.load_state_dict(\n",
    "#     torch.load(\"model_main_nomatch_2019_11_0_35.pth\")\n",
    "# )\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "best_threshold, train_losses, val_losses = train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    optimizer,\n",
    "    F.binary_cross_entropy_with_logits,\n",
    "    device,\n",
    "    100,\n",
    "    results_df\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f570fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST THRESHOLD: 0.7000000000000004\n"
     ]
    }
   ],
   "source": [
    "print(\"BEST THRESHOLD:\", best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ebb80aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"./model_{model_name}_{year}_{month}_{perc}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicbrainz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
