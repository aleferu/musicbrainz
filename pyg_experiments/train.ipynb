{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31d33376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "import os.path as path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e12322ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"ds_float32/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdb5b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = HeteroData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1192f48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artist tensor shape: torch.Size([1489250, 16])\n",
      "Track tensor shape: torch.Size([24324100, 4])\n",
      "Tag tensor shape: torch.Size([23, 1])\n",
      "collab_with index tensor shape: torch.Size([2, 2463052])\n",
      "collab_with attr tensor shape: torch.Size([2463052, 1])\n",
      "has_tag_artists index tensor shape: torch.Size([2, 2410207])\n",
      "has_tag_tracks index tensor shape: torch.Size([2, 4030735])\n",
      "last_fm_match index tensor shape: torch.Size([2, 154865250])\n",
      "last_fm_match attr tensor shape: torch.Size([154865250, 1])\n",
      "linked_to index tensor shape: torch.Size([2, 23128])\n",
      "linked_to attr tensor shape: torch.Size([23128, 1])\n",
      "musically_related_to index tensor shape: torch.Size([2, 373262])\n",
      "musically_related_to attr tensor shape: torch.Size([373262, 1])\n",
      "personally_related_to index tensor shape: torch.Size([2, 26720])\n",
      "personally_related_to attr tensor shape: torch.Size([26720, 1])\n",
      "tags_artists index tensor shape: torch.Size([2, 2410207])\n",
      "tags_tracks index tensor shape: torch.Size([2, 4030735])\n",
      "worked_by index tensor shape: torch.Size([2, 27661673])\n",
      "worked_in index tensor shape: torch.Size([2, 27661673])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"artist\"].x = torch.load(path.join(data_folder, \"artists.pt\"), weights_only=True)\n",
    "print(\"Artist tensor shape:\", data[\"artist\"].x.shape)\n",
    "\n",
    "data[\"track\"].x = torch.load(path.join(data_folder, \"tracks.pt\"), weights_only=True)\n",
    "print(\"Track tensor shape:\", data[\"track\"].x.shape)\n",
    "\n",
    "data[\"tag\"].x = torch.load(path.join(data_folder, \"tags.pt\"), weights_only=True)\n",
    "print(\"Tag tensor shape:\", data[\"tag\"].x.shape)\n",
    "\n",
    "\n",
    "data[\"artist\", \"collab_with\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"collab_with.pt\"), weights_only=True).t().long()\n",
    "data[\"artist\", \"collab_with\", \"artist\"].edge_attr = torch.load(path.join(data_folder, \"collab_with_attr.pt\"), weights_only=True)\n",
    "print(\"collab_with index tensor shape:\", data[\"artist\", \"collab_with\", \"artist\"].edge_index.shape)\n",
    "print(\"collab_with attr tensor shape:\", data[\"artist\", \"collab_with\", \"artist\"].edge_attr.shape)\n",
    "\n",
    "data[\"artist\", \"has_tag_artists\", \"tag\"].edge_index = torch.load(path.join(data_folder, \"has_tag_artists.pt\"), weights_only=True).t().long()\n",
    "data[\"track\", \"has_tag_tracks\", \"tag\"].edge_index = torch.load(path.join(data_folder, \"has_tag_tracks.pt\"), weights_only=True).t().long()\n",
    "print(\"has_tag_artists index tensor shape:\", data[\"artist\", \"has_tag_artists\", \"tag\"].edge_index.shape)\n",
    "print(\"has_tag_tracks index tensor shape:\", data[\"track\", \"has_tag_tracks\", \"tag\"].edge_index.shape)\n",
    "\n",
    "data[\"artist\", \"last_fm_match\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"last_fm_match.pt\"), weights_only=True).t().long()\n",
    "data[\"artist\", \"last_fm_match\", \"artist\"].edge_attr = torch.load(path.join(data_folder, \"last_fm_match_attr.pt\"), weights_only=True)\n",
    "print(\"last_fm_match index tensor shape:\", data[\"artist\", \"last_fm_match\", \"artist\"].edge_index.shape)\n",
    "print(\"last_fm_match attr tensor shape:\", data[\"artist\", \"last_fm_match\", \"artist\"].edge_attr.shape)\n",
    "\n",
    "data[\"artist\", \"linked_to\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"linked_to.pt\"), weights_only=True).t().long()\n",
    "data[\"artist\", \"linked_to\", \"artist\"].edge_attr = torch.load(path.join(data_folder, \"linked_to_attr.pt\"), weights_only=True)\n",
    "print(\"linked_to index tensor shape:\", data[\"artist\", \"linked_to\", \"artist\"].edge_index.shape)\n",
    "print(\"linked_to attr tensor shape:\", data[\"artist\", \"linked_to\", \"artist\"].edge_attr.shape)\n",
    "\n",
    "data[\"artist\", \"musically_related_to\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"musically_related_to.pt\"), weights_only=True).t().long()\n",
    "data[\"artist\", \"musically_related_to\", \"artist\"].edge_attr = torch.load(path.join(data_folder, \"musically_related_to_attr.pt\"), weights_only=True)\n",
    "print(\"musically_related_to index tensor shape:\", data[\"artist\", \"musically_related_to\", \"artist\"].edge_index.shape)\n",
    "print(\"musically_related_to attr tensor shape:\", data[\"artist\", \"musically_related_to\", \"artist\"].edge_attr.shape)\n",
    "\n",
    "data[\"artist\", \"personally_related_to\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"personally_related_to.pt\"), weights_only=True).t().long()\n",
    "data[\"artist\", \"personally_related_to\", \"artist\"].edge_attr = torch.load(path.join(data_folder, \"personally_related_to_attr.pt\"), weights_only=True)\n",
    "print(\"personally_related_to index tensor shape:\", data[\"artist\", \"personally_related_to\", \"artist\"].edge_index.shape)\n",
    "print(\"personally_related_to attr tensor shape:\", data[\"artist\", \"personally_related_to\", \"artist\"].edge_attr.shape)\n",
    "\n",
    "data[\"tag\", \"tags_artists\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"tags_artists.pt\"), weights_only=True).t().long()\n",
    "data[\"tag\", \"tags_track\", \"track\"].edge_index = torch.load(path.join(data_folder, \"tags_tracks.pt\"), weights_only=True).t().long()\n",
    "print(\"tags_artists index tensor shape:\", data[\"tag\", \"tags_artists\", \"artist\"].edge_index.shape)\n",
    "print(\"tags_tracks index tensor shape:\", data[\"tag\", \"tags_track\", \"track\"].edge_index.shape)\n",
    "\n",
    "data[\"track\", \"worked_by\", \"artist\"].edge_index = torch.load(path.join(data_folder, \"worked_by.pt\"), weights_only=True).t().long()\n",
    "data[\"artist\", \"worked_in\", \"track\"].edge_index = torch.load(path.join(data_folder, \"worked_in.pt\"), weights_only=True).t().long()\n",
    "print(\"worked_by index tensor shape:\", data[\"track\", \"worked_by\", \"artist\"].edge_index.shape)\n",
    "print(\"worked_in index tensor shape:\", data[\"artist\", \"worked_in\", \"track\"].edge_index.shape)\n",
    "\n",
    "print()\n",
    "\n",
    "data.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a33bd03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_type: ('artist', 'collab_with', 'artist')\n",
      "edge_type: ('artist', 'has_tag_artists', 'tag')\n",
      "No edge_attr for ('artist', 'has_tag_artists', 'tag')\n",
      "edge_type: ('track', 'has_tag_tracks', 'tag')\n",
      "No edge_attr for ('track', 'has_tag_tracks', 'tag')\n",
      "edge_type: ('artist', 'last_fm_match', 'artist')\n",
      "edge_type: ('artist', 'linked_to', 'artist')\n",
      "edge_type: ('artist', 'musically_related_to', 'artist')\n",
      "edge_type: ('artist', 'personally_related_to', 'artist')\n",
      "edge_type: ('tag', 'tags_artists', 'artist')\n",
      "No edge_attr for ('tag', 'tags_artists', 'artist')\n",
      "edge_type: ('tag', 'tags_track', 'track')\n",
      "No edge_attr for ('tag', 'tags_track', 'track')\n",
      "edge_type: ('track', 'worked_by', 'artist')\n",
      "No edge_attr for ('track', 'worked_by', 'artist')\n",
      "edge_type: ('artist', 'worked_in', 'track')\n",
      "No edge_attr for ('artist', 'worked_in', 'track')\n",
      "Edge type: ('artist', 'collab_with', 'artist'), edge_index shape: torch.Size([2, 629340])\n",
      "Edge type: ('artist', 'has_tag_artists', 'tag'), edge_index shape: torch.Size([2, 1042766])\n",
      "Edge type: ('track', 'has_tag_tracks', 'tag'), edge_index shape: torch.Size([2, 4030735])\n",
      "Edge type: ('artist', 'last_fm_match', 'artist'), edge_index shape: torch.Size([2, 28357816])\n",
      "Edge type: ('artist', 'linked_to', 'artist'), edge_index shape: torch.Size([2, 1438])\n",
      "Edge type: ('artist', 'musically_related_to', 'artist'), edge_index shape: torch.Size([2, 41760])\n",
      "Edge type: ('artist', 'personally_related_to', 'artist'), edge_index shape: torch.Size([2, 3334])\n",
      "Edge type: ('tag', 'tags_artists', 'artist'), edge_index shape: torch.Size([2, 1042766])\n",
      "Edge type: ('tag', 'tags_track', 'track'), edge_index shape: torch.Size([2, 4030735])\n",
      "Edge type: ('track', 'worked_by', 'artist'), edge_index shape: torch.Size([2, 12509457])\n",
      "Edge type: ('artist', 'worked_in', 'track'), edge_index shape: torch.Size([2, 12509457])\n",
      "Subgraph artist tensor shape: torch.Size([223388, 16])\n",
      "Subgraph track tensor shape: torch.Size([24324100, 4])\n",
      "Subgraph tag tensor shape: torch.Size([23, 1])\n",
      "\n",
      "\n",
      "Validation successful.\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL SUBGRAPH\n",
    "\n",
    "if True:\n",
    "\n",
    "    # Data\n",
    "    percentile = 0.85\n",
    "    artist_popularity = data[\"artist\"].x[:, 8]\n",
    "    edge_types = [\n",
    "        (\"artist\", \"collab_with\", \"artist\"),\n",
    "        (\"artist\", \"has_tag_artists\", \"tag\"),\n",
    "        (\"track\", \"has_tag_tracks\", \"tag\"),\n",
    "        (\"artist\", \"last_fm_match\", \"artist\"),\n",
    "        (\"artist\", \"linked_to\", \"artist\"),\n",
    "        (\"artist\", \"musically_related_to\", \"artist\"),\n",
    "        (\"artist\", \"personally_related_to\", \"artist\"),\n",
    "        (\"tag\", \"tags_artists\", \"artist\"),\n",
    "        (\"tag\", \"tags_track\", \"track\"),\n",
    "        (\"track\", \"worked_by\", \"artist\"),\n",
    "        (\"artist\", \"worked_in\", \"track\")\n",
    "    ]\n",
    "\n",
    "    # Threshold obtention\n",
    "    threshold = torch.quantile(artist_popularity, percentile)\n",
    "    selected_artists = artist_popularity >= threshold\n",
    "    selected_artist_ids = torch.nonzero(selected_artists).squeeze()\n",
    "\n",
    "    # Mapping\n",
    "    old_to_new_artist_idx = torch.zeros(\n",
    "        data[\"artist\"].x.shape[0],\n",
    "        dtype=torch.long\n",
    "    )\n",
    "\n",
    "    for i, selected_artist_id in enumerate(selected_artist_ids):\n",
    "        old_to_new_artist_idx[selected_artist_id] = i\n",
    "\n",
    "    # Subgraph\n",
    "    for edge_type in edge_types:\n",
    "        print(f\"edge_type: {edge_type}\")\n",
    "        # Filter edge indices\n",
    "        edge_index = data[edge_type].edge_index\n",
    "        mask = torch.ones(edge_index.shape[1], dtype=torch.bool)\n",
    "        if edge_type[0] == \"artist\":\n",
    "            mask &= torch.isin(edge_index[0], selected_artist_ids)\n",
    "        if edge_type[2] == \"artist\":\n",
    "            mask &= torch.isin(edge_index[1], selected_artist_ids)\n",
    "\n",
    "        filtered_edge_index = edge_index[:, mask]\n",
    "\n",
    "        # Map the old indices to new ones for 'artist' nodes\n",
    "        # if edge_type[0] == \"artist\":  # Reindex source node\n",
    "        #     filtered_edge_index[0] = torch.tensor(\n",
    "        #         [old_to_new_artist_idx[idx.item()] for idx in filtered_edge_index[0]],\n",
    "        #         dtype=torch.long,\n",
    "        #     )\n",
    "        # if edge_type[2] == \"artist\":  # Reindex destination node\n",
    "        #     filtered_edge_index[1] = torch.tensor(\n",
    "        #         [old_to_new_artist_idx[idx.item()] for idx in filtered_edge_index[1]],\n",
    "        #         dtype=torch.long,\n",
    "        #     )\n",
    "        if edge_type[0] == \"artist\":  # Reindex source node\n",
    "            filtered_edge_index[0] = old_to_new_artist_idx[filtered_edge_index[0]]\n",
    "        if edge_type[2] == \"artist\":  # Reindex destination node\n",
    "            filtered_edge_index[1] = old_to_new_artist_idx[filtered_edge_index[1]]\n",
    "\n",
    "        # Assign filtered edges to subgraph\n",
    "        data[edge_type].edge_index = filtered_edge_index\n",
    "\n",
    "        # Handle edge attributes if they exist\n",
    "        if hasattr(data[edge_type], \"edge_attr\"):\n",
    "            try:\n",
    "                data[edge_type].edge_attr = data[edge_type].edge_attr[mask]\n",
    "            except IndexError as e:\n",
    "                print(f\"IndexError for {edge_type}: {e}\")\n",
    "        else:\n",
    "            print(f\"No edge_attr for {edge_type}\")\n",
    "\n",
    "    # Nodes filtering\n",
    "    data[\"artist\"].x = data[\"artist\"].x[selected_artist_ids]\n",
    "    # data[\"track\"].x = data[\"track\"].x\n",
    "    # data[\"tag\"].x = data[\"tag\"].x\n",
    "\n",
    "    # Check the shape of the filtered nodes and edges\n",
    "    for edge_type in edge_types:\n",
    "        print(f\"Edge type: {edge_type}, edge_index shape: {data[edge_type].edge_index.shape}\")\n",
    "\n",
    "    # Check the artist features (should only have the selected artists)\n",
    "    print(\"Subgraph artist tensor shape:\", data[\"artist\"].x.shape)\n",
    "    print(\"Subgraph track tensor shape:\", data[\"track\"].x.shape)\n",
    "    print(\"Subgraph tag tensor shape:\", data[\"tag\"].x.shape)\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Validate the subgraph\n",
    "    try:\n",
    "        data.validate()\n",
    "        print(\"Validation successful.\")\n",
    "    except ValueError as e:\n",
    "        print(\"Validation failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b839629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "==============\n",
      "HeteroData(\n",
      "  artist={ x=[223388, 16] },\n",
      "  track={ x=[24324100, 4] },\n",
      "  tag={ x=[23, 1] },\n",
      "  (artist, collab_with, artist)={\n",
      "    edge_index=[2, 220269],\n",
      "    edge_attr=[220269, 1],\n",
      "    edge_label=[94401],\n",
      "    edge_label_index=[2, 94401],\n",
      "  },\n",
      "  (artist, has_tag_artists, tag)={ edge_index=[2, 1042766] },\n",
      "  (track, has_tag_tracks, tag)={ edge_index=[2, 4030735] },\n",
      "  (artist, last_fm_match, artist)={\n",
      "    edge_index=[2, 28357816],\n",
      "    edge_attr=[28357816, 1],\n",
      "  },\n",
      "  (artist, linked_to, artist)={\n",
      "    edge_index=[2, 1438],\n",
      "    edge_attr=[1438, 1],\n",
      "  },\n",
      "  (artist, musically_related_to, artist)={\n",
      "    edge_index=[2, 41760],\n",
      "    edge_attr=[41760, 1],\n",
      "  },\n",
      "  (artist, personally_related_to, artist)={\n",
      "    edge_index=[2, 3334],\n",
      "    edge_attr=[3334, 1],\n",
      "  },\n",
      "  (tag, tags_artists, artist)={ edge_index=[2, 1042766] },\n",
      "  (tag, tags_track, track)={ edge_index=[2, 4030735] },\n",
      "  (track, worked_by, artist)={ edge_index=[2, 12509457] },\n",
      "  (artist, worked_in, track)={ edge_index=[2, 12509457] }\n",
      ")\n",
      "\n",
      "Validation data:\n",
      "================\n",
      "HeteroData(\n",
      "  artist={ x=[223388, 16] },\n",
      "  track={ x=[24324100, 4] },\n",
      "  tag={ x=[23, 1] },\n",
      "  (artist, collab_with, artist)={\n",
      "    edge_index=[2, 314670],\n",
      "    edge_attr=[314670, 1],\n",
      "    edge_label=[314670],\n",
      "    edge_label_index=[2, 314670],\n",
      "  },\n",
      "  (artist, has_tag_artists, tag)={ edge_index=[2, 1042766] },\n",
      "  (track, has_tag_tracks, tag)={ edge_index=[2, 4030735] },\n",
      "  (artist, last_fm_match, artist)={\n",
      "    edge_index=[2, 28357816],\n",
      "    edge_attr=[28357816, 1],\n",
      "  },\n",
      "  (artist, linked_to, artist)={\n",
      "    edge_index=[2, 1438],\n",
      "    edge_attr=[1438, 1],\n",
      "  },\n",
      "  (artist, musically_related_to, artist)={\n",
      "    edge_index=[2, 41760],\n",
      "    edge_attr=[41760, 1],\n",
      "  },\n",
      "  (artist, personally_related_to, artist)={\n",
      "    edge_index=[2, 3334],\n",
      "    edge_attr=[3334, 1],\n",
      "  },\n",
      "  (tag, tags_artists, artist)={ edge_index=[2, 1042766] },\n",
      "  (tag, tags_track, track)={ edge_index=[2, 4030735] },\n",
      "  (track, worked_by, artist)={ edge_index=[2, 12509457] },\n",
      "  (artist, worked_in, track)={ edge_index=[2, 12509457] }\n",
      ")\n",
      "\n",
      "Test data:\n",
      "================\n",
      "HeteroData(\n",
      "  artist={ x=[223388, 16] },\n",
      "  track={ x=[24324100, 4] },\n",
      "  tag={ x=[23, 1] },\n",
      "  (artist, collab_with, artist)={\n",
      "    edge_index=[2, 472005],\n",
      "    edge_attr=[472005, 1],\n",
      "    edge_label=[314670],\n",
      "    edge_label_index=[2, 314670],\n",
      "  },\n",
      "  (artist, has_tag_artists, tag)={ edge_index=[2, 1042766] },\n",
      "  (track, has_tag_tracks, tag)={ edge_index=[2, 4030735] },\n",
      "  (artist, last_fm_match, artist)={\n",
      "    edge_index=[2, 28357816],\n",
      "    edge_attr=[28357816, 1],\n",
      "  },\n",
      "  (artist, linked_to, artist)={\n",
      "    edge_index=[2, 1438],\n",
      "    edge_attr=[1438, 1],\n",
      "  },\n",
      "  (artist, musically_related_to, artist)={\n",
      "    edge_index=[2, 41760],\n",
      "    edge_attr=[41760, 1],\n",
      "  },\n",
      "  (artist, personally_related_to, artist)={\n",
      "    edge_index=[2, 3334],\n",
      "    edge_attr=[3334, 1],\n",
      "  },\n",
      "  (tag, tags_artists, artist)={ edge_index=[2, 1042766] },\n",
      "  (tag, tags_track, track)={ edge_index=[2, 4030735] },\n",
      "  (track, worked_by, artist)={ edge_index=[2, 12509457] },\n",
      "  (artist, worked_in, track)={ edge_index=[2, 12509457] }\n",
      ")\n",
      "Training edges: 220269\n",
      "Validation edges: 314670\n",
      "Test edges: 472005\n"
     ]
    }
   ],
   "source": [
    "import torch_geometric.transforms as T\n",
    "\n",
    "transform = T.RandomLinkSplit(\n",
    "    num_val=0.25,\n",
    "    num_test=0.25,\n",
    "    disjoint_train_ratio=0.3,\n",
    "    neg_sampling_ratio=1,\n",
    "    add_negative_train_samples=False,\n",
    "    edge_types=(\"artist\", \"collab_with\", \"artist\"),\n",
    ")\n",
    "\n",
    "train_data, val_data, test_data = transform(data)\n",
    "\n",
    "print(\"Training data:\")\n",
    "print(\"==============\")\n",
    "print(train_data)\n",
    "print()\n",
    "print(\"Validation data:\")\n",
    "print(\"================\")\n",
    "print(val_data)\n",
    "print()\n",
    "print(\"Test data:\")\n",
    "print(\"================\")\n",
    "print(test_data)\n",
    "\n",
    "print(f\"Training edges: {train_data['artist', 'collab_with', 'artist'].edge_index.shape[1]}\")\n",
    "print(f\"Validation edges: {val_data['artist', 'collab_with', 'artist'].edge_index.shape[1]}\")\n",
    "print(f\"Test edges: {test_data['artist', 'collab_with', 'artist'].edge_index.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82faa4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: '{device}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "956bf7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train_loader...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleferu/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating val_loader...\n",
      "Creating test_loader...\n",
      "Sampling mini-batch...\n",
      "Sampled mini-batch:\n",
      "===================\n",
      "HeteroData(\n",
      "  artist={\n",
      "    x=[181857, 16],\n",
      "    n_id=[181857],\n",
      "  },\n",
      "  track={\n",
      "    x=[140745, 4],\n",
      "    n_id=[140745],\n",
      "  },\n",
      "  tag={\n",
      "    x=[23, 1],\n",
      "    n_id=[23],\n",
      "  },\n",
      "  (artist, collab_with, artist)={\n",
      "    edge_index=[2, 24823],\n",
      "    edge_attr=[24823, 1],\n",
      "    edge_label=[256],\n",
      "    edge_label_index=[2, 256],\n",
      "    e_id=[24823],\n",
      "    input_id=[128],\n",
      "  },\n",
      "  (artist, has_tag_artists, tag)={\n",
      "    edge_index=[2, 460],\n",
      "    e_id=[460],\n",
      "  },\n",
      "  (track, has_tag_tracks, tag)={\n",
      "    edge_index=[2, 460],\n",
      "    e_id=[460],\n",
      "  },\n",
      "  (artist, last_fm_match, artist)={\n",
      "    edge_index=[2, 228066],\n",
      "    edge_attr=[228066, 1],\n",
      "    e_id=[228066],\n",
      "  },\n",
      "  (artist, linked_to, artist)={\n",
      "    edge_index=[2, 697],\n",
      "    edge_attr=[697, 1],\n",
      "    e_id=[697],\n",
      "  },\n",
      "  (artist, musically_related_to, artist)={\n",
      "    edge_index=[2, 8075],\n",
      "    edge_attr=[8075, 1],\n",
      "    e_id=[8075],\n",
      "  },\n",
      "  (artist, personally_related_to, artist)={\n",
      "    edge_index=[2, 836],\n",
      "    edge_attr=[836, 1],\n",
      "    e_id=[836],\n",
      "  },\n",
      "  (tag, tags_artists, artist)={\n",
      "    edge_index=[2, 31598],\n",
      "    e_id=[31598],\n",
      "  },\n",
      "  (tag, tags_track, track)={\n",
      "    edge_index=[2, 988],\n",
      "    e_id=[988],\n",
      "  },\n",
      "  (track, worked_by, artist)={\n",
      "    edge_index=[2, 141590],\n",
      "    e_id=[141590],\n",
      "  },\n",
      "  (artist, worked_in, track)={\n",
      "    edge_index=[2, 10527],\n",
      "    e_id=[10527],\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "edge_label_index = train_data[\"artist\", \"collab_with\", \"artist\"].edge_label_index\n",
    "edge_label = train_data[\"artist\", \"collab_with\", \"artist\"].edge_label\n",
    "\n",
    "print(\"Creating train_loader...\")\n",
    "train_loader = LinkNeighborLoader(\n",
    "    data=train_data,\n",
    "    num_neighbors=[25, 20],\n",
    "    neg_sampling_ratio=1,\n",
    "    edge_label_index=((\"artist\", \"collab_with\", \"artist\"), edge_label_index),\n",
    "    edge_label=edge_label,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "edge_label_index = val_data[\"artist\", \"collab_with\", \"artist\"].edge_label_index\n",
    "edge_label = val_data[\"artist\", \"collab_with\", \"artist\"].edge_label\n",
    "\n",
    "print(\"Creating val_loader...\")\n",
    "val_loader = LinkNeighborLoader(\n",
    "    data=val_data,\n",
    "    num_neighbors=[25, 20],\n",
    "    edge_label_index=((\"artist\", \"collab_with\", \"artist\"), edge_label_index),\n",
    "    edge_label=edge_label,\n",
    "    batch_size=512,\n",
    "    shuffle=False,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "edge_label_index = test_data[\"artist\", \"collab_with\", \"artist\"].edge_label_index\n",
    "edge_label = test_data[\"artist\", \"collab_with\", \"artist\"].edge_label\n",
    "\n",
    "print(\"Creating test_loader...\")\n",
    "test_loader = LinkNeighborLoader(\n",
    "    data=test_data,\n",
    "    num_neighbors=[25, 20],\n",
    "    edge_label_index=((\"artist\", \"collab_with\", \"artist\"), edge_label_index),\n",
    "    edge_label=edge_label,\n",
    "    batch_size=512,\n",
    "    shuffle=False,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(\"Sampling mini-batch...\")\n",
    "\n",
    "sampled_data = next(iter(train_loader))\n",
    "\n",
    "print(\"Sampled mini-batch:\")\n",
    "print(\"===================\")\n",
    "print(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13d3164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "if debug:\n",
    "    print(torch.unique(train_data['artist', 'collab_with', 'artist'].edge_label))\n",
    "    print(torch.unique(next(iter(train_loader))[\"artist\", \"collab_with\", \"artist\"].edge_label))\n",
    "    print(torch.unique(val_data['artist', 'collab_with', 'artist'].edge_label))\n",
    "    print(torch.unique(next(iter(val_loader))[\"artist\", \"collab_with\", \"artist\"].edge_label))\n",
    "    print(torch.unique(test_data['artist', 'collab_with', 'artist'].edge_label))\n",
    "    print(torch.unique(next(iter(test_loader))[\"artist\", \"collab_with\", \"artist\"].edge_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91d598ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import HeteroConv, GATConv, SAGEConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, metadata, out_channels):\n",
    "        super().__init__()\n",
    "        self.metadata = metadata\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.conv1 = HeteroConv({\n",
    "            (\"artist\", \"collab_with\", \"artist\"): GATConv((-1, -1), out_channels),\n",
    "            (\"artist\", \"has_tag_artists\", \"tag\"): SAGEConv((-1, -1), out_channels),\n",
    "            (\"artist\", \"last_fm_match\", \"artist\"): GATConv((-1, -1), out_channels),\n",
    "            (\"track\", \"has_tag_tracks\", \"tag\"): SAGEConv((-1, -1), out_channels),\n",
    "            (\"artist\", \"linked_to\", \"artist\"): GATConv((-1, -1), out_channels),\n",
    "            (\"artist\", \"musically_related_to\", \"artist\"): GATConv((-1, -1), out_channels),\n",
    "            (\"artist\", \"personally_related_to\", \"artist\"): GATConv((-1, -1), out_channels),\n",
    "            (\"tag\", \"tags_artists\", \"artist\"): SAGEConv((-1, -1), out_channels),\n",
    "            (\"tag\", \"tags_tracks\", \"track\"): SAGEConv((-1, -1), out_channels),\n",
    "            (\"track\", \"worked_by\", \"artist\"): SAGEConv((-1, -1), out_channels),\n",
    "            (\"artist\", \"worked_in\", \"track\"): SAGEConv((-1, -1), out_channels),\n",
    "        }, aggr=\"mean\")\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        norm_x_dict = {\n",
    "            key: F.normalize(\n",
    "                x,\n",
    "                p=2,\n",
    "                dim=-1\n",
    "            )\n",
    "            for key, x in x_dict.items()\n",
    "        }\n",
    "        return norm_x_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9525bb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for sampled_data in tqdm.tqdm(train_loader):\n",
    "            # Move data to device\n",
    "            sampled_data = sampled_data.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_dict = model(sampled_data.x_dict, sampled_data.edge_index_dict)\n",
    "            \n",
    "            # Get predictions and labels for the 'collab_with' edge type\n",
    "            edge_label_index = sampled_data['artist', 'collab_with', 'artist'].edge_label_index\n",
    "            edge_label = sampled_data['artist', 'collab_with', 'artist'].edge_label\n",
    "\n",
    "            src_emb = pred_dict['artist'][edge_label_index[0]]  # Source node embeddings\n",
    "            dst_emb = pred_dict['artist'][edge_label_index[1]]  # Destination node embeddings\n",
    "            \n",
    "            # Compute the dot product between source and destination embeddings\n",
    "            preds = (src_emb * dst_emb).sum(dim=-1)  # Scalar for each edge\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(preds, edge_label.float())\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Average loss for the epoch\n",
    "        epoch_loss /= len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # Validation metrics\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient computation for validation\n",
    "            for sampled_data in tqdm.tqdm(val_loader):\n",
    "                # Move data to device\n",
    "                sampled_data = sampled_data.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                pred_dict = model(sampled_data.x_dict, sampled_data.edge_index_dict)\n",
    "                \n",
    "                # Get predictions and labels for the 'collab_with' edge type\n",
    "                edge_label_index = sampled_data['artist', 'collab_with', 'artist'].edge_label_index\n",
    "                edge_label = sampled_data['artist', 'collab_with', 'artist'].edge_label\n",
    "\n",
    "                src_emb = pred_dict['artist'][edge_label_index[0]]  # Source node embeddings\n",
    "                dst_emb = pred_dict['artist'][edge_label_index[1]]  # Destination node embeddings\n",
    "                \n",
    "                # Compute the dot product between source and destination embeddings\n",
    "                preds = (src_emb * dst_emb).sum(dim=-1)  # Scalar for each edge\n",
    "\n",
    "                probs = torch.sigmoid(preds)  # Convert to probabilities\n",
    "\n",
    "                loss = criterion(preds, edge_label.float())\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Collect predictions, probabilities, and labels\n",
    "                all_labels.append(edge_label.cpu())\n",
    "                all_probs.append(probs.cpu())\n",
    "        \n",
    "        # Concatenate all predictions and labels\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        all_probs = torch.cat(all_probs)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Find threshold for predictions\n",
    "        best_threshold = 0\n",
    "        best_f1 = 0\n",
    "        for threshold in np.arange(0.2, 0.81, 0.01):\n",
    "            preds_binary = (all_probs > threshold).long()\n",
    "            cm = confusion_matrix(all_labels, preds_binary)\n",
    "            tp = cm[1, 1]\n",
    "            fp = cm[0, 1]\n",
    "            fn = cm[1, 0]\n",
    "            tn = cm[0, 0]\n",
    "            precision = 0 if tp == 0 else tp / (tp + fp)\n",
    "            recall = 0 if tp == 0 else tp / (tp + fn)\n",
    "            f1 = 0 if precision * recall == 0 else 2 * precision * recall / (precision + recall)\n",
    "            if f1 > best_f1:\n",
    "                best_threshold = threshold\n",
    "                best_f1 = f1\n",
    "        print(f\"Best threshold: {best_threshold}\")\n",
    "        all_preds = (all_probs > best_threshold).long()\n",
    "        \n",
    "        # Compute metrics\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        tp = cm[1, 1]\n",
    "        fp = cm[0, 1]\n",
    "        fn = cm[1, 0]\n",
    "        tn = cm[0, 0]\n",
    "        accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        roc_auc = roc_auc_score(all_labels, all_probs)\n",
    "        \n",
    "        # Print validation metrics\n",
    "        print(f\"Validation Metrics - Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"Loss:      {val_loss:.4f}\")\n",
    "        print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall:    {recall:.4f}\")\n",
    "        print(f\"F1-score:  {f1:.4f}\")\n",
    "        print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{tp} {fn}\\n{fp} {tn}\")\n",
    "\n",
    "    return best_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13d6278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion, device, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for sampled_data in tqdm.tqdm(test_loader):\n",
    "            # Move data to the device\n",
    "            sampled_data = sampled_data.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            pred_dict = model(sampled_data.x_dict, sampled_data.edge_index_dict)\n",
    "\n",
    "            # Get predictions and labels for the 'collab_with' edge type\n",
    "            edge_label_index = sampled_data['artist', 'collab_with', 'artist'].edge_label_index\n",
    "            edge_label = sampled_data['artist', 'collab_with', 'artist'].edge_label\n",
    "\n",
    "            src_emb = pred_dict['artist'][edge_label_index[0]]  # Source node embeddings\n",
    "            dst_emb = pred_dict['artist'][edge_label_index[1]]  # Destination node embeddings\n",
    "            \n",
    "            # Compute the dot product between source and destination embeddings\n",
    "            preds = (src_emb * dst_emb).sum(dim=-1)  # Scalar for each edge\n",
    "            probs = torch.sigmoid(preds)  # Convert logits to probabilities\n",
    "            preds_binary = (probs > threshold).long()  # Convert probabilities to binary predictions\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(preds, edge_label.float())\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Collect predictions and labels\n",
    "            all_preds.append(preds_binary.cpu())\n",
    "            all_labels.append(edge_label.cpu())\n",
    "            all_probs.append(probs.cpu())\n",
    "\n",
    "    # Concatenate all predictions and labels\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    all_probs = torch.cat(all_probs)\n",
    "\n",
    "    # Compute metrics\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    tp = cm[1, 1]\n",
    "    fp = cm[0, 1]\n",
    "    fn = cm[1, 0]\n",
    "    tn = cm[0, 0]\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    roc_auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    # Average test loss\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    print(\"Test Results:\")\n",
    "    print(f\"Loss:      {test_loss:.4f}\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-score:  {f1:.4f}\")\n",
    "    print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{tp} {fn}\\n{fp} {tn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c71d770d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4811/4811 [07:32<00:00, 10.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 0.6891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2406/2406 [07:42<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.5300000000000002\n",
      "Validation Metrics - Epoch 1/20:\n",
      "Loss:      0.6114\n",
      "Accuracy:  0.6961\n",
      "Precision: 0.6455\n",
      "Recall:    0.8696\n",
      "F1-score:  0.7410\n",
      "ROC-AUC:   0.7919\n",
      "Confusion Matrix:\n",
      "535493 80270\n",
      "294040 321723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 3188/4811 [05:02<02:34, 10.54it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m GNN(metadata\u001b[38;5;241m=\u001b[39mtrain_data\u001b[38;5;241m.\u001b[39mmetadata(), out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m best_threshold \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)\u001b[0m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set model to training mode\u001b[39;00m\n\u001b[1;32m      8\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msampled_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Move data to device\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampled_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msampled_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Forward pass\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1283\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1283\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1284\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1285\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/musicbrainz/lib/python3.12/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/miniforge3/envs/musicbrainz/lib/python3.12/threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = GNN(metadata=train_data.metadata(), out_channels=64).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "best_threshold = train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    F.binary_cross_entropy_with_logits,\n",
    "    device,\n",
    "    20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab218838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_threshold = train(\n",
    "#     model,\n",
    "#     train_loader,\n",
    "#     val_loader,\n",
    "#     optimizer,\n",
    "#     F.binary_cross_entropy_with_logits,\n",
    "#     device,\n",
    "#     20\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4561c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:28<00:00, 21.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Loss:      0.5740\n",
      "Accuracy:  0.8027\n",
      "Precision: 0.7512\n",
      "Recall:    0.9052\n",
      "F1-score:  0.8210\n",
      "ROC-AUC:   0.8481\n",
      "Confusion Matrix:\n",
      "142413 14922\n",
      "47159 110176\n"
     ]
    }
   ],
   "source": [
    "test_model(\n",
    "    model,\n",
    "    test_loader,\n",
    "    F.binary_cross_entropy_with_logits,\n",
    "    device,\n",
    "    best_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ebb80aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./normal-nolfm16.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d13ec911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108036/3795534198.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test.load_state_dict(torch.load(\"./normal-nolfm.pth\"))\n",
      "100%|██████████| 615/615 [00:29<00:00, 21.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Loss:      0.5740\n",
      "Accuracy:  0.8026\n",
      "Precision: 0.7512\n",
      "Recall:    0.9051\n",
      "F1-score:  0.8210\n",
      "ROC-AUC:   0.8483\n",
      "Confusion Matrix:\n",
      "142398 14937\n",
      "47169 110166\n"
     ]
    }
   ],
   "source": [
    "test = GNN(metadata=train_data.metadata(), out_channels=64).to(device)\n",
    "test.load_state_dict(torch.load(\"./normal-nolfm16.pth\"))\n",
    "test_model(\n",
    "    test,\n",
    "    test_loader,\n",
    "    F.binary_cross_entropy_with_logits,\n",
    "    device,\n",
    "    best_threshold\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicbrainz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
