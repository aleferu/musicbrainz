{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear, ReLU, Sequential\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.nn import HeteroConv, GATConv, SAGEConv, Linear\n",
    "from torch_geometric.nn.aggr import Aggregation, MultiAggregation\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.typing import OptPairTensor, Adj, Size\n",
    "import os.path as path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import tqdm\n",
    "from typing import Optional, Union, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"ds/\"\n",
    "perc = 0\n",
    "year = 2023\n",
    "month = 11\n",
    "# test_hd = f\"full_hd_nomatch_{perc}.pt\"\n",
    "# test_hd = f\"full_hd_{perc}.pt\"\n",
    "test_hd = f\"full_hdmb_{perc}.pt\"\n",
    "# train_collab_with_filename = f\"collab_with_nomatch_{year}_{month}_{perc}.pt\"\n",
    "# train_collab_with_filename = f\"collab_with_{year}_{month}_{perc}.pt\"\n",
    "train_collab_with_filename = f\"collab_withmb_{year}_{month}_{perc}.pt\"\n",
    "best_threshold = 0.69\n",
    "# model_path = f\"model_main_nomatch_{year}_{month}_{perc}.pth\"\n",
    "# model_path = f\"model_main_{year}_{month}_{perc}.pth\"\n",
    "model_path = f\"model_main_mb_{year}_{month}_{perc}.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_83469/624821894.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(path.join(data_folder, test_hd))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.load(path.join(data_folder, test_hd))\n",
    "\n",
    "data.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artist channels: 15\n",
      "Track channels: 4\n",
      "Tag channels: 24\n"
     ]
    }
   ],
   "source": [
    "artist_channels = data[\"artist\"].x.size(1)\n",
    "track_channels = data[\"track\"].x.size(1)\n",
    "tag_channels = data[\"tag\"].x.size(1)\n",
    "\n",
    "print(f\"Artist channels: {artist_channels}\")\n",
    "print(f\"Track channels: {track_channels}\")\n",
    "print(f\"Tag channels: {tag_channels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_83469/3191979287.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_collab_with = torch.load(path.join(data_folder, train_collab_with_filename))\n"
     ]
    }
   ],
   "source": [
    "train_collab_with = torch.load(path.join(data_folder, train_collab_with_filename))\n",
    "train_edges_set = set(map(tuple, train_collab_with.t().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: '{device}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test_loader...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleferu/miniforge3/envs/musicbrainz/lib/python3.12/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    }
   ],
   "source": [
    "compt_tree_size = [25, 20]\n",
    "\n",
    "print(\"Creating test_loader...\")\n",
    "test_loader = LinkNeighborLoader(\n",
    "    data=data,\n",
    "    num_neighbors=compt_tree_size,\n",
    "    neg_sampling_ratio=1,\n",
    "    edge_label_index=(\"artist\", \"collab_with\", \"artist\"),\n",
    "    batch_size=128,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPSAGEConv(MessagePassing):\n",
    "    r\"\"\"The GraphSAGE operator with an MLP instead of the linear transformation.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int or tuple): Size of each input sample, or :obj:`-1` to\n",
    "            derive the size from the first input(s) to the forward method.\n",
    "            A tuple corresponds to the sizes of source and target\n",
    "            dimensionalities.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        hidden_channels (int, optional): Size of the hidden layer in the MLP.\n",
    "            (default: :obj:`64`)\n",
    "        aggr (str or Aggregation, optional): The aggregation scheme to use.\n",
    "            Any aggregation of :obj:`torch_geometric.nn.aggr` can be used,\n",
    "            *e.g.*, :obj:`\"mean\"`, :obj:`\"max\"`, or :obj:`\"lstm\"`.\n",
    "            (default: :obj:`\"mean\"`)\n",
    "        normalize (bool, optional): If set to :obj:`True`, output features\n",
    "            will be :math:`\\ell_2`-normalized, *i.e.*,\n",
    "            :math:`\\frac{\\mathbf{x}^{\\prime}_i}\n",
    "            {\\| \\mathbf{x}^{\\prime}_i \\|_2}`.\n",
    "            (default: :obj:`False`)\n",
    "        root_weight (bool, optional): If set to :obj:`False`, the layer will\n",
    "            not add transformed root node features to the output.\n",
    "            (default: :obj:`True`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "\n",
    "    Shapes:\n",
    "        - **inputs:**\n",
    "          node features :math:`(|\\mathcal{V}|, F_{in})` or\n",
    "          :math:`((|\\mathcal{V_s}|, F_{s}), (|\\mathcal{V_t}|, F_{t}))`\n",
    "          if bipartite,\n",
    "          edge indices :math:`(2, |\\mathcal{E}|)`\n",
    "        - **outputs:** node features :math:`(|\\mathcal{V}|, F_{out})` or\n",
    "          :math:`(|\\mathcal{V_t}|, F_{out})` if bipartite\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: Union[int, Tuple[int, int]],\n",
    "        out_channels: int,\n",
    "        hidden_channels: int = 64,\n",
    "        aggr: Optional[Union[str, List[str], Aggregation]] = \"mean\",\n",
    "        normalize: bool = False,\n",
    "        root_weight: bool = True,\n",
    "        bias: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.normalize = normalize\n",
    "        self.root_weight = root_weight\n",
    "\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "\n",
    "        if aggr == \"lstm\":\n",
    "            kwargs.setdefault(\"aggr_kwargs\", {})\n",
    "            kwargs[\"aggr_kwargs\"].setdefault(\"in_channels\", in_channels[0])\n",
    "            kwargs[\"aggr_kwargs\"].setdefault(\"out_channels\", in_channels[0])\n",
    "\n",
    "        super().__init__(aggr, **kwargs)\n",
    "\n",
    "\n",
    "        self.mlp = Sequential(\n",
    "            Linear(in_channels[0], hidden_channels),\n",
    "            ReLU(),\n",
    "            Linear(hidden_channels, in_channels[0]) # Output size should match input for aggregation\n",
    "        )\n",
    "\n",
    "        if isinstance(self.aggr_module, MultiAggregation):\n",
    "            aggr_out_channels = self.aggr_module.get_out_channels(in_channels[0])\n",
    "        else:\n",
    "            aggr_out_channels = in_channels[0]\n",
    "\n",
    "        self.lin_l = Linear(aggr_out_channels, out_channels, bias=bias)\n",
    "        if self.root_weight:\n",
    "            self.lin_r = Linear(in_channels[1], out_channels, bias=False)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()\n",
    "        for layer in self.mlp:\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()\n",
    "        self.lin_l.reset_parameters()\n",
    "        if self.root_weight:\n",
    "            self.lin_r.reset_parameters()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Union[Tensor, OptPairTensor],\n",
    "        edge_index: Adj,\n",
    "        size: Size = None,\n",
    "    ) -> Tensor:\n",
    "\n",
    "        if isinstance(x, Tensor):\n",
    "            x = (x, x)\n",
    "\n",
    "        # Propagate through MLP\n",
    "        x = (self.mlp(x[0]), x[1])\n",
    "\n",
    "        # propagate_type: (x: OptPairTensor)\n",
    "        out = self.propagate(edge_index, x=x, size=size)\n",
    "        out = self.lin_l(out)\n",
    "\n",
    "        x_r = x[1]\n",
    "        if self.root_weight and x_r is not None:\n",
    "            out = out + self.lin_r(x_r)\n",
    "\n",
    "        if self.normalize:\n",
    "            out = F.normalize(out, p=2.0, dim=-1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j: Tensor) -> Tensor:\n",
    "        return x_j\n",
    "\n",
    "    def message_and_aggregate(self, adj_t: Adj, x: OptPairTensor) -> Tensor:\n",
    "        if isinstance(adj_t, SparseTensor):\n",
    "            adj_t = adj_t.set_value(None, layout=None)\n",
    "        return spmm(adj_t, x[0], reduce=self.aggr)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"{self.__class__.__name__}({self.in_channels}, \"\n",
    "            f\"{self.out_channels}, hidden_channels={self.hidden_channels}, aggr={self.aggr})\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, metadata, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.metadata = metadata\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.conv1 = HeteroConv({\n",
    "            (\"artist\", \"collab_with\", \"artist\"): GATConv((artist_channels, artist_channels), hidden_channels),\n",
    "            (\"artist\", \"has_tag_artists\", \"tag\"): SAGEConv((artist_channels, tag_channels), hidden_channels),\n",
    "            # (\"artist\", \"last_fm_match\", \"artist\"): GATConv((artist_channels, artist_channels), hidden_channels),\n",
    "            (\"track\", \"has_tag_tracks\", \"tag\"): SAGEConv((track_channels, tag_channels), hidden_channels),\n",
    "            (\"artist\", \"linked_to\", \"artist\"): GATConv((artist_channels, artist_channels), hidden_channels),\n",
    "            (\"artist\", \"musically_related_to\", \"artist\"): GATConv((artist_channels, artist_channels), hidden_channels),\n",
    "            (\"artist\", \"personally_related_to\", \"artist\"): GATConv((artist_channels, artist_channels), hidden_channels),\n",
    "            (\"tag\", \"tags_artists\", \"artist\"): SAGEConv((tag_channels, artist_channels), hidden_channels),\n",
    "            (\"tag\", \"tags_tracks\", \"track\"): SAGEConv((tag_channels, track_channels), hidden_channels),\n",
    "            (\"track\", \"worked_by\", \"artist\"): SAGEConv((track_channels, artist_channels), hidden_channels),\n",
    "            (\"artist\", \"worked_in\", \"track\"): SAGEConv((artist_channels, track_channels), hidden_channels),\n",
    "        }, aggr=\"mean\")\n",
    "\n",
    "        self.conv2 = HeteroConv({\n",
    "            (\"artist\", \"collab_with\", \"artist\"): GATConv((hidden_channels, hidden_channels), hidden_channels),\n",
    "            (\"artist\", \"has_tag_artists\", \"tag\"): SAGEConv((hidden_channels, hidden_channels), hidden_channels),\n",
    "            # (\"artist\", \"last_fm_match\", \"artist\"): GATConv((hidden_channels, hidden_channels), hidden_channels),\n",
    "            (\"track\", \"has_tag_tracks\", \"tag\"): SAGEConv((hidden_channels, hidden_channels), hidden_channels),\n",
    "            (\"artist\", \"linked_to\", \"artist\"): GATConv((hidden_channels, hidden_channels), hidden_channels),\n",
    "            (\"artist\", \"musically_related_to\", \"artist\"): GATConv((hidden_channels, hidden_channels), hidden_channels),\n",
    "            (\"artist\", \"personally_related_to\", \"artist\"): GATConv((hidden_channels, hidden_channels), hidden_channels),\n",
    "            (\"tag\", \"tags_artists\", \"artist\"): SAGEConv((hidden_channels, hidden_channels), hidden_channels),\n",
    "            (\"tag\", \"tags_tracks\", \"track\"): SAGEConv((hidden_channels, hidden_channels), hidden_channels),\n",
    "            (\"track\", \"worked_by\", \"artist\"): SAGEConv((hidden_channels, hidden_channels), hidden_channels),\n",
    "            (\"artist\", \"worked_in\", \"track\"): SAGEConv((hidden_channels, hidden_channels), hidden_channels),\n",
    "        }, aggr=\"mean\")\n",
    "\n",
    "        self.linear1 = Linear(hidden_channels * 2, hidden_channels * 4)\n",
    "        self.linear2 = Linear(hidden_channels * 4, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict1 = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict2 = self.conv2(x_dict1, edge_index_dict)\n",
    "\n",
    "        x_artist = torch.cat([x_dict1['artist'], x_dict2['artist']], dim=-1)\n",
    "\n",
    "        x_artist = self.linear1(x_artist)\n",
    "        x_artist = self.linear2(x_artist)\n",
    "\n",
    "        # Normalize the artist node features\n",
    "        x_artist = F.normalize(x_artist, p=2, dim=-1)\n",
    "\n",
    "        # Update the dictionary with the new 'artist' features, leaving other nodes unchanged\n",
    "        x_dict['artist'] = x_artist\n",
    "\n",
    "        return x_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_83469/1260903973.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(model_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GNN(\n",
       "  (conv1): HeteroConv(num_relations=10)\n",
       "  (conv2): HeteroConv(num_relations=10)\n",
       "  (linear1): Linear(128, 256, bias=True)\n",
       "  (linear2): Linear(256, 64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GNN(metadata=data.metadata(), hidden_channels=64, out_channels=64).to(device)\n",
    "model.load_state_dict(\n",
    "    torch.load(model_path)\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion, device, threshold, train_edges_set):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    total_loss = 0.0\n",
    "    valid_batches = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for sampled_data in tqdm.tqdm(test_loader):\n",
    "            # Move data to the device\n",
    "            sampled_data = sampled_data.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            pred_dict = model(sampled_data.x_dict, sampled_data.edge_index_dict)\n",
    "\n",
    "            # Get predictions and labels for the 'collab_with' edge type\n",
    "            edge_label_index = sampled_data['artist', 'collab_with', 'artist'].edge_label_index\n",
    "            edge_label = sampled_data['artist', 'collab_with', 'artist'].edge_label\n",
    "\n",
    "            # Filter edge list\n",
    "            filtered_edges = []\n",
    "            filtered_edge_label = []\n",
    "            positive_count = 0\n",
    "            for src, dst, label in zip(edge_label_index[0, :], edge_label_index[1, :], edge_label):\n",
    "                lookup_edge = (\n",
    "                    sampled_data['artist'].n_id[src].item(),\n",
    "                    sampled_data['artist'].n_id[dst].item()\n",
    "                )\n",
    "                if lookup_edge in train_edges_set:\n",
    "                    continue\n",
    "                filtered_edges.append([src.item(), dst.item()])\n",
    "                label_item = label.item()\n",
    "                filtered_edge_label.append(label_item)\n",
    "\n",
    "                # Balancing\n",
    "                if np.isclose(label_item, 1):\n",
    "                    positive_count += 1\n",
    "                else:\n",
    "                    positive_count -= 1\n",
    "                    if positive_count == 0:\n",
    "                        break\n",
    "\n",
    "            if len(filtered_edges) == 0:\n",
    "                continue  # Skip if no valid edges left\n",
    "\n",
    "            valid_batches += 1\n",
    "\n",
    "            # Normal evaluation with the rests\n",
    "            filtered_edges = torch.tensor(filtered_edges, dtype=torch.long).t().to(device)\n",
    "            filtered_labels = torch.tensor(filtered_edge_label).to(device)\n",
    "\n",
    "            src_emb = pred_dict['artist'][filtered_edges[0]]  # Source node embeddings\n",
    "            dst_emb = pred_dict['artist'][filtered_edges[1]]  # Destination node embeddings\n",
    "\n",
    "            # Compute the dot product between source and destination embeddings\n",
    "            preds = (src_emb * dst_emb).sum(dim=-1)  # Scalar for each edge\n",
    "            probs = torch.sigmoid(preds)  # Convert logits to probabilities\n",
    "            preds_binary = (probs > threshold).long()  # Convert probabilities to binary predictions\n",
    "\n",
    "            loss = criterion(preds, filtered_labels.float())\n",
    "            total_loss += loss\n",
    "\n",
    "            # Collect predictions and labels\n",
    "            all_preds.append(preds_binary.cpu())\n",
    "            all_labels.append(filtered_labels.cpu())\n",
    "            all_probs.append(probs.cpu())\n",
    "\n",
    "    # Concatenate all predictions and labels\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    all_probs = torch.cat(all_probs)\n",
    "        \n",
    "    # Average loss\n",
    "    total_loss /= valid_batches\n",
    "\n",
    "    # Compute metrics\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    tp = cm[1, 1]\n",
    "    fp = cm[0, 1]\n",
    "    fn = cm[1, 0]\n",
    "    tn = cm[0, 0]\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    roc_auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    print(\"Test Results:\")\n",
    "    print(f\"Loss:      {total_loss:.4f}\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-score:  {f1:.4f}\")\n",
    "    print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{tp} {fn}\\n{fp} {tn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19243/19243 [09:38<00:00, 33.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Loss:      0.5565\n",
      "Accuracy:  0.8379\n",
      "Precision: 0.6352\n",
      "Recall:    0.9338\n",
      "F1-score:  0.7561\n",
      "ROC-AUC:   0.9124\n",
      "Confusion Matrix:\n",
      "122892 8706\n",
      "70570 286948\n"
     ]
    }
   ],
   "source": [
    "test_model(\n",
    "    model,\n",
    "    test_loader, \n",
    "    F.binary_cross_entropy_with_logits,\n",
    "    device,\n",
    "    best_threshold,\n",
    "    train_edges_set\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicbrainz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
